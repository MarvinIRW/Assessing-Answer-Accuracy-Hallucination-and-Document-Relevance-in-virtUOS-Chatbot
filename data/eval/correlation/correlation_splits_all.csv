metric_file,metric_column,human_column,language,spearman_corr,p_value
rouge_evaluation_de.csv,ROUGE-1_f,avg_hallucination,de,0.1904513364711566,0.2884038319483262
rouge_evaluation_de.csv,ROUGE-1_f,avg_answer_acc,de,0.10236346194051754,0.5708146079250376
rouge_evaluation_de.csv,ROUGE-1_f,avg_user_sat,de,0.30949845566548945,0.07965304376923314
rouge_evaluation_de.csv,ROUGE-1_f,avg_coherence,de,0.22049532830561755,0.21755565688712533
rouge_evaluation_de.csv,ROUGE-1_f,avg_context_qual,de,0.2548215202044106,0.1523885717032332
rouge_evaluation_de.csv,ROUGE-1_f,avg_overall,de,0.19367509432315297,0.28016636969464986
rouge_evaluation_de.csv,ROUGE-1_f,overall_mean,de,0.2364746885892119,0.1851885638812799
rouge_evaluation_de.csv,ROUGE-2_f,avg_hallucination,de,0.1950466770294419,0.27670821133246953
rouge_evaluation_de.csv,ROUGE-2_f,avg_answer_acc,de,0.19318344890266825,0.2814127159442913
rouge_evaluation_de.csv,ROUGE-2_f,avg_user_sat,de,0.39421395443543256,0.02320281311479199
rouge_evaluation_de.csv,ROUGE-2_f,avg_coherence,de,0.22187020174789732,0.21462801831911252
rouge_evaluation_de.csv,ROUGE-2_f,avg_context_qual,de,0.25920917551918854,0.14521050291383258
rouge_evaluation_de.csv,ROUGE-2_f,avg_overall,de,0.28189359278448445,0.11198382563345761
rouge_evaluation_de.csv,ROUGE-2_f,overall_mean,de,0.28764955047626906,0.10454185677450732
rouge_evaluation_de.csv,ROUGE-3_f,avg_hallucination,de,0.20943326073979854,0.2421052102725407
rouge_evaluation_de.csv,ROUGE-3_f,avg_answer_acc,de,0.22086870702200068,0.21675790329092226
rouge_evaluation_de.csv,ROUGE-3_f,avg_user_sat,de,0.4142202765972315,0.016551140527268547
rouge_evaluation_de.csv,ROUGE-3_f,avg_coherence,de,0.2140576830088554,0.23162647131131034
rouge_evaluation_de.csv,ROUGE-3_f,avg_context_qual,de,0.2617905927525826,0.1411041498566994
rouge_evaluation_de.csv,ROUGE-3_f,avg_overall,de,0.30081965672289246,0.08892378003365865
rouge_evaluation_de.csv,ROUGE-3_f,overall_mean,de,0.30027617478993285,0.08953069418764442
rouge_evaluation_de.csv,ROUGE-4_f,avg_hallucination,de,0.2243517534025597,0.20941241950465866
rouge_evaluation_de.csv,ROUGE-4_f,avg_answer_acc,de,0.2058622040632264,0.25041072860398855
rouge_evaluation_de.csv,ROUGE-4_f,avg_user_sat,de,0.44185040896926603,0.0100417089505154
rouge_evaluation_de.csv,ROUGE-4_f,avg_coherence,de,0.18475585430130106,0.30333253462916326
rouge_evaluation_de.csv,ROUGE-4_f,avg_context_qual,de,0.24601878104927283,0.16755708523633314
rouge_evaluation_de.csv,ROUGE-4_f,avg_overall,de,0.292730594639688,0.09828822023548534
rouge_evaluation_de.csv,ROUGE-4_f,overall_mean,de,0.30496751227206,0.08439505139327527
rouge_evaluation_de.csv,ROUGE-L_f,avg_hallucination,de,0.13632843656246332,0.4493538008825724
rouge_evaluation_de.csv,ROUGE-L_f,avg_answer_acc,de,0.05058758152284283,0.7798018262553065
rouge_evaluation_de.csv,ROUGE-L_f,avg_user_sat,de,0.2953229538792838,0.0952090976857914
rouge_evaluation_de.csv,ROUGE-L_f,avg_coherence,de,0.2089807632265245,0.2431473012368791
rouge_evaluation_de.csv,ROUGE-L_f,avg_context_qual,de,0.17854382011673273,0.32016148120797583
rouge_evaluation_de.csv,ROUGE-L_f,avg_overall,de,0.1502418489121143,0.40397267959719263
rouge_evaluation_de.csv,ROUGE-L_f,overall_mean,de,0.18262401693028243,0.30904367858908804
rouge_evaluation_de.csv,ROUGE-SU4_f,avg_hallucination,de,0.19947181978927217,0.26574044131996016
rouge_evaluation_de.csv,ROUGE-SU4_f,avg_answer_acc,de,0.1300338504916027,0.4707519872090816
rouge_evaluation_de.csv,ROUGE-SU4_f,avg_user_sat,de,0.33649941144873824,0.0555222816632451
rouge_evaluation_de.csv,ROUGE-SU4_f,avg_coherence,de,0.21774558142105802,0.22349253328527513
rouge_evaluation_de.csv,ROUGE-SU4_f,avg_context_qual,de,0.22663927260564468,0.20468251576058574
rouge_evaluation_de.csv,ROUGE-SU4_f,avg_overall,de,0.22443326811618428,0.20924258978356242
rouge_evaluation_de.csv,ROUGE-SU4_f,overall_mean,de,0.24868307067337908,0.1628567289705619
rouge_evaluation_de.csv,ROUGE-W-1.2_f,avg_hallucination,de,0.06841951497891417,0.7051861524813752
rouge_evaluation_de.csv,ROUGE-W-1.2_f,avg_answer_acc,de,0.08284140866827953,0.6467172252529126
rouge_evaluation_de.csv,ROUGE-W-1.2_f,avg_user_sat,de,0.30241070477238663,0.0871651391676933
rouge_evaluation_de.csv,ROUGE-W-1.2_f,avg_coherence,de,0.18234259028235403,0.3098026287507315
rouge_evaluation_de.csv,ROUGE-W-1.2_f,avg_context_qual,de,0.228495588315743,0.20089889203698091
rouge_evaluation_de.csv,ROUGE-W-1.2_f,avg_overall,de,0.19046407617992442,0.2883709767125276
rouge_evaluation_de.csv,ROUGE-W-1.2_f,overall_mean,de,0.18864458891699504,0.29308766197585845
rouge_evaluation_en.csv,ROUGE-1_f,avg_hallucination,en,-0.11814799583024188,0.5125718726440468
rouge_evaluation_en.csv,ROUGE-1_f,avg_answer_acc,en,0.13341238487847107,0.45920093721229405
rouge_evaluation_en.csv,ROUGE-1_f,avg_user_sat,en,0.15882768165419356,0.3773238423884565
rouge_evaluation_en.csv,ROUGE-1_f,avg_coherence,en,-0.24738426171523306,0.16513614324454823
rouge_evaluation_en.csv,ROUGE-1_f,avg_context_qual,en,0.09791601618662582,0.5877484148907193
rouge_evaluation_en.csv,ROUGE-1_f,avg_overall,en,0.21269685822875975,0.23467770637468136
rouge_evaluation_en.csv,ROUGE-1_f,overall_mean,en,0.03765694648503206,0.8351858372422747
rouge_evaluation_en.csv,ROUGE-2_f,avg_hallucination,en,-0.09176047514769507,0.6115427996355614
rouge_evaluation_en.csv,ROUGE-2_f,avg_answer_acc,en,0.10463052519656743,0.562268606407532
rouge_evaluation_en.csv,ROUGE-2_f,avg_user_sat,en,0.10723828558973239,0.552511608253065
rouge_evaluation_en.csv,ROUGE-2_f,avg_coherence,en,-0.037754113808805605,0.8347663873050517
rouge_evaluation_en.csv,ROUGE-2_f,avg_context_qual,en,-0.050909540478314995,0.7784351029928188
rouge_evaluation_en.csv,ROUGE-2_f,avg_overall,en,0.13863428687739082,0.4416485964613519
rouge_evaluation_en.csv,ROUGE-2_f,overall_mean,en,-0.0021757346858018523,0.9904122844918013
rouge_evaluation_en.csv,ROUGE-3_f,avg_hallucination,en,-0.11807645515377292,0.5128290327490732
rouge_evaluation_en.csv,ROUGE-3_f,avg_answer_acc,en,0.032533766263744414,0.8573618986206809
rouge_evaluation_en.csv,ROUGE-3_f,avg_user_sat,en,0.04359142046551612,0.8096539709040265
rouge_evaluation_en.csv,ROUGE-3_f,avg_coherence,en,-0.017426281405687624,0.9233191819288791
rouge_evaluation_en.csv,ROUGE-3_f,avg_context_qual,en,-0.15115799136052377,0.40107920900587046
rouge_evaluation_en.csv,ROUGE-3_f,avg_overall,en,0.042574960518073655,0.8140141997551367
rouge_evaluation_en.csv,ROUGE-3_f,overall_mean,en,-0.08015077175958647,0.6574820649117947
rouge_evaluation_en.csv,ROUGE-4_f,avg_hallucination,en,-0.14248430800627154,0.4289452382363539
rouge_evaluation_en.csv,ROUGE-4_f,avg_answer_acc,en,-0.04652584837136791,0.7970982953718438
rouge_evaluation_en.csv,ROUGE-4_f,avg_user_sat,en,-0.012723180130249185,0.9439758718672673
rouge_evaluation_en.csv,ROUGE-4_f,avg_coherence,en,0.0065701845767034,0.9710528304493823
rouge_evaluation_en.csv,ROUGE-4_f,avg_context_qual,en,-0.2079806523910928,0.2454611511206933
rouge_evaluation_en.csv,ROUGE-4_f,avg_overall,en,-0.04172944931334325,0.8176453076641501
rouge_evaluation_en.csv,ROUGE-4_f,overall_mean,en,-0.13420072408976436,0.4565275429113278
rouge_evaluation_en.csv,ROUGE-L_f,avg_hallucination,en,-0.13636389720464517,0.44923476006496976
rouge_evaluation_en.csv,ROUGE-L_f,avg_answer_acc,en,0.11614326906932888,0.519802175501924
rouge_evaluation_en.csv,ROUGE-L_f,avg_user_sat,en,0.24238558872253393,0.17412176213095557
rouge_evaluation_en.csv,ROUGE-L_f,avg_coherence,en,-0.23859220781455234,0.18116857083709378
rouge_evaluation_en.csv,ROUGE-L_f,avg_context_qual,en,0.14441339649015353,0.4226568974787176
rouge_evaluation_en.csv,ROUGE-L_f,avg_overall,en,0.22405198930551423,0.21003777129098578
rouge_evaluation_en.csv,ROUGE-L_f,overall_mean,en,0.06761513946645756,0.708500336857581
rouge_evaluation_en.csv,ROUGE-SU4_f,avg_hallucination,en,-0.10640129307478556,0.5556346124709148
rouge_evaluation_en.csv,ROUGE-SU4_f,avg_answer_acc,en,0.12223825111961437,0.4979760740628124
rouge_evaluation_en.csv,ROUGE-SU4_f,avg_user_sat,en,0.13041122742852315,0.4694542400237781
rouge_evaluation_en.csv,ROUGE-SU4_f,avg_coherence,en,-0.1591189362809478,0.37643832202787975
rouge_evaluation_en.csv,ROUGE-SU4_f,avg_context_qual,en,0.011709194310012447,0.9484345963806227
rouge_evaluation_en.csv,ROUGE-SU4_f,avg_overall,en,0.1755808327689204,0.32838885442167176
rouge_evaluation_en.csv,ROUGE-SU4_f,overall_mean,en,0.0041841051650035625,0.9815632237632232
rouge_evaluation_en.csv,ROUGE-W-1.2_f,avg_hallucination,en,-0.18709603374270292,0.29714055207970114
rouge_evaluation_en.csv,ROUGE-W-1.2_f,avg_answer_acc,en,0.1855583424198024,0.30120007920350433
rouge_evaluation_en.csv,ROUGE-W-1.2_f,avg_user_sat,en,0.2521960312528249,0.1568046612892737
rouge_evaluation_en.csv,ROUGE-W-1.2_f,avg_coherence,en,-0.10291874860208651,0.5687159514947654
rouge_evaluation_en.csv,ROUGE-W-1.2_f,avg_context_qual,en,0.07755219999529983,0.6679431089681357
rouge_evaluation_en.csv,ROUGE-W-1.2_f,avg_overall,en,0.23489868167733943,0.18822095390812385
rouge_evaluation_en.csv,ROUGE-W-1.2_f,overall_mean,en,0.08635993060567353,0.6327457976846588
bleu_evaluation_de.csv,BLEU,avg_hallucination,de,0.10399085485601134,0.5646739707950498
bleu_evaluation_de.csv,BLEU,avg_answer_acc,de,0.1184903755132359,0.5113420400576034
bleu_evaluation_de.csv,BLEU,avg_user_sat,de,0.36029400373272624,0.039429432147494684
bleu_evaluation_de.csv,BLEU,avg_coherence,de,0.18509233716691356,0.3024372387581856
bleu_evaluation_de.csv,BLEU,avg_context_qual,de,0.23929597062904254,0.1798462538849029
bleu_evaluation_de.csv,BLEU,avg_overall,de,0.20635016594116035,0.24926481856453256
bleu_evaluation_de.csv,BLEU,overall_mean,de,0.21807849640759003,0.2227679478378094
bleu_evaluation_en.csv,BLEU,avg_hallucination,en,-0.23459357097128716,0.18881201605570277
bleu_evaluation_en.csv,BLEU,avg_answer_acc,en,0.07754171608418751,0.6679854400455842
bleu_evaluation_en.csv,BLEU,avg_user_sat,en,0.1562904982411873,0.3850893148877297
bleu_evaluation_en.csv,BLEU,avg_coherence,en,-0.1239507206782248,0.4919283359248251
bleu_evaluation_en.csv,BLEU,avg_context_qual,en,-0.04344280787482879,0.8102911155544108
bleu_evaluation_en.csv,BLEU,avg_overall,en,0.09016313034079695,0.6177829649154227
bleu_evaluation_en.csv,BLEU,overall_mean,en,-0.012552315495010687,0.9447270889834752
bertscore_evaluation_de.csv,BERTScore_F1,avg_hallucination,de,0.08918364639042543,0.6216224503885286
bertscore_evaluation_de.csv,BERTScore_F1,avg_answer_acc,de,0.17943313311961367,0.3177173386868078
bertscore_evaluation_de.csv,BERTScore_F1,avg_user_sat,de,0.2973480255630275,0.09285530795862716
bertscore_evaluation_de.csv,BERTScore_F1,avg_coherence,de,0.21585513043792334,0.22763745614052103
bertscore_evaluation_de.csv,BERTScore_F1,avg_context_qual,de,0.24013975049726907,0.17826986114341742
bertscore_evaluation_de.csv,BERTScore_F1,avg_overall,de,0.2832456004237386,0.11020111810883716
bertscore_evaluation_de.csv,BERTScore_F1,overall_mean,de,0.22593868761246483,0.20612319869429474
bertscore_evaluation_en.csv,BERTScore_F1,avg_hallucination,en,-0.35648689231776154,0.04171516043445946
bertscore_evaluation_en.csv,BERTScore_F1,avg_answer_acc,en,-0.008126642733713975,0.9641992855244126
bertscore_evaluation_en.csv,BERTScore_F1,avg_user_sat,en,-0.11840189260696007,0.5116597314574429
bertscore_evaluation_en.csv,BERTScore_F1,avg_coherence,en,-0.08947207793045713,0.6204908021693722
bertscore_evaluation_en.csv,BERTScore_F1,avg_context_qual,en,-0.2723660415589852,0.1251647957306242
bertscore_evaluation_en.csv,BERTScore_F1,avg_overall,en,-0.09050208947741649,0.6164565785842968
bertscore_evaluation_en.csv,BERTScore_F1,overall_mean,en,-0.2205860242989878,0.21736169335043118
bartscore_cnn_de.csv,BARTScore_paper_avg,avg_hallucination,de,0.21189625907648793,0.23648539149272535
bartscore_cnn_de.csv,BARTScore_paper_avg,avg_answer_acc,de,0.22730460288284077,0.20332078908566076
bartscore_cnn_de.csv,BARTScore_paper_avg,avg_user_sat,de,0.21094496705663132,0.23864546550593635
bartscore_cnn_de.csv,BARTScore_paper_avg,avg_coherence,de,0.48086198643734834,0.004615463996888606
bartscore_cnn_de.csv,BARTScore_paper_avg,avg_context_qual,de,0.2315331958413585,0.1948126529417703
bartscore_cnn_de.csv,BARTScore_paper_avg,avg_overall,de,0.2949066663123054,0.09569853744187792
bartscore_cnn_de.csv,BARTScore_paper_avg,overall_mean,de,0.2617276433112564,0.14120326543138714
bartscore_cnn_de.csv,BARTScore_paper_harm,avg_hallucination,de,0.21053467668884784,0.2395811732977623
bartscore_cnn_de.csv,BARTScore_paper_harm,avg_answer_acc,de,0.24461981535039098,0.17006356651218946
bartscore_cnn_de.csv,BARTScore_paper_harm,avg_user_sat,de,0.20605104382091743,0.2499668508919159
bartscore_cnn_de.csv,BARTScore_paper_harm,avg_coherence,de,0.47055043562025006,0.00571769401986241
bartscore_cnn_de.csv,BARTScore_paper_harm,avg_context_qual,de,0.22258912923815735,0.21310794277430042
bartscore_cnn_de.csv,BARTScore_paper_harm,avg_overall,de,0.3099477512990075,0.07919427638010573
bartscore_cnn_de.csv,BARTScore_paper_harm,overall_mean,de,0.26373450064016063,0.1380683813278241
bartscore_cnn_en.csv,BARTScore_paper_avg,avg_hallucination,en,-0.020429048270358827,0.9101554319409451
bartscore_cnn_en.csv,BARTScore_paper_avg,avg_answer_acc,en,-0.006602897221142605,0.9709087644170795
bartscore_cnn_en.csv,BARTScore_paper_avg,avg_user_sat,en,-0.061568984155619226,0.7335760525527687
bartscore_cnn_en.csv,BARTScore_paper_avg,avg_coherence,en,0.026720948129519948,0.8826524619368284
bartscore_cnn_en.csv,BARTScore_paper_avg,avg_context_qual,en,-0.2163655470328387,0.22651324270340603
bartscore_cnn_en.csv,BARTScore_paper_avg,avg_overall,en,-0.04050561682603473,0.8229077199186294
bartscore_cnn_en.csv,BARTScore_paper_avg,overall_mean,en,-0.1138076604880969,0.5282884360474123
bartscore_cnn_en.csv,BARTScore_paper_harm,avg_hallucination,en,0.01481105999601015,0.934800393560316
bartscore_cnn_en.csv,BARTScore_paper_harm,avg_answer_acc,en,-0.016930505695237446,0.9254946147511638
bartscore_cnn_en.csv,BARTScore_paper_harm,avg_user_sat,en,-0.09421407740296679,0.602009704899332
bartscore_cnn_en.csv,BARTScore_paper_harm,avg_coherence,en,0.08947207793045713,0.6204908021693722
bartscore_cnn_en.csv,BARTScore_paper_harm,avg_context_qual,en,-0.23418388620024896,0.1896077041701092
bartscore_cnn_en.csv,BARTScore_paper_harm,avg_overall,en,-0.05338606401757714,0.7679442565458741
bartscore_cnn_en.csv,BARTScore_paper_harm,overall_mean,en,-0.10443526491848892,0.5630023481909711
bartscore_multi_de.csv,BARTScore_multilang_avg,avg_hallucination,de,0.2649979721944512,0.1361210653735862
bartscore_multi_de.csv,BARTScore_multilang_avg,avg_answer_acc,de,0.29011468732395435,0.10147135173034426
bartscore_multi_de.csv,BARTScore_multilang_avg,avg_user_sat,de,0.49496127070167967,0.0034070855692888437
bartscore_multi_de.csv,BARTScore_multilang_avg,avg_coherence,de,0.5037192574152495,0.0028033147343579554
bartscore_multi_de.csv,BARTScore_multilang_avg,avg_context_qual,de,0.38830749535784687,0.025542770817808143
bartscore_multi_de.csv,BARTScore_multilang_avg,avg_overall,de,0.41624935193536283,0.015976131628094026
bartscore_multi_de.csv,BARTScore_multilang_avg,overall_mean,de,0.3973577511230321,0.022031322884940006
bartscore_multi_de.csv,BARTScore_multilang_harm,avg_hallucination,de,0.2666999501790013,0.13352986644445008
bartscore_multi_de.csv,BARTScore_multilang_harm,avg_answer_acc,de,0.29639569576806574,0.09395663110904771
bartscore_multi_de.csv,BARTScore_multilang_harm,avg_user_sat,de,0.5027240454893637,0.0028668818894317357
bartscore_multi_de.csv,BARTScore_multilang_harm,avg_coherence,de,0.5047504124969593,0.0027387451211029062
bartscore_multi_de.csv,BARTScore_multilang_harm,avg_context_qual,de,0.38982629912065464,0.02492330103227671
bartscore_multi_de.csv,BARTScore_multilang_harm,avg_overall,de,0.42723441400430257,0.01314499666295816
bartscore_multi_de.csv,BARTScore_multilang_harm,overall_mean,de,0.40220765633455063,0.020320261756421494
bartscore_multi_en.csv,BARTScore_multilang_avg,avg_hallucination,en,0.11389194410725045,0.5279810353201994
bartscore_multi_en.csv,BARTScore_multilang_avg,avg_answer_acc,en,0.11715909941104315,0.5161322138591316
bartscore_multi_en.csv,BARTScore_multilang_avg,avg_user_sat,en,0.24695251886594527,0.1658988841103181
bartscore_multi_en.csv,BARTScore_multilang_avg,avg_coherence,en,0.007240514977031212,0.9681008903873076
bartscore_multi_en.csv,BARTScore_multilang_avg,avg_context_qual,en,0.054982303716580194,0.7612036263317781
bartscore_multi_en.csv,BARTScore_multilang_avg,avg_overall,en,0.14337971479006434,0.42602003149516965
bartscore_multi_en.csv,BARTScore_multilang_avg,overall_mean,en,0.11096246897589447,0.538716032350941
bartscore_multi_en.csv,BARTScore_multilang_harm,avg_hallucination,en,0.1121895234180539,0.5342068854883486
bartscore_multi_en.csv,BARTScore_multilang_harm,avg_answer_acc,en,0.11851353986666215,0.5112588863184874
bartscore_multi_en.csv,BARTScore_multilang_harm,avg_user_sat,en,0.23545062072698345,0.18715503370411243
bartscore_multi_en.csv,BARTScore_multilang_harm,avg_coherence,en,0.0193080399387499,0.9150672348595644
bartscore_multi_en.csv,BARTScore_multilang_harm,avg_context_qual,en,0.0386912507635194,0.830723283520718
bartscore_multi_en.csv,BARTScore_multilang_harm,avg_overall,en,0.1396511642872494,0.4382736082776636
bartscore_multi_en.csv,BARTScore_multilang_harm,overall_mean,en,0.1035984438854882,0.5661518861753698
bleurt_evaluation_de.csv,BLEURT,avg_hallucination,de,0.1838136223314112,0.3058484753159799
bleurt_evaluation_de.csv,BLEURT,avg_answer_acc,de,0.2829848939549631,0.11054320673510695
bleurt_evaluation_de.csv,BLEURT,avg_user_sat,de,0.06935870516822037,0.7013232287782802
bleurt_evaluation_de.csv,BLEURT,avg_coherence,de,0.2923324656647353,0.09876772450296237
bleurt_evaluation_de.csv,BLEURT,avg_context_qual,de,0.29363539414283074,0.09720505839640971
bleurt_evaluation_de.csv,BLEURT,avg_overall,de,0.3209328133679472,0.06860034864722374
bleurt_evaluation_de.csv,BLEURT,overall_mean,de,0.24684345145521694,0.16609196654752853
bleurt_evaluation_en.csv,BLEURT,avg_hallucination,en,-0.19203305374137297,0.2843429809927072
bleurt_evaluation_en.csv,BLEURT,avg_answer_acc,en,0.0311521304792369,0.8633615115788205
bleurt_evaluation_en.csv,BLEURT,avg_user_sat,en,0.044316136947176475,0.8065486296310176
bleurt_evaluation_en.csv,BLEURT,avg_coherence,en,-0.0651646347932809,0.7186290572942691
bleurt_evaluation_en.csv,BLEURT,avg_context_qual,en,-0.2258686612554575,0.20626758476236742
bleurt_evaluation_en.csv,BLEURT,avg_overall,en,-0.09778971091473654,0.5882325343774063
bleurt_evaluation_en.csv,BLEURT,overall_mean,en,-0.1169875804134996,0.5167509749068785
llm_judge_together_no_ref_de.csv,hallucination_score,avg_hallucination,de,0.1418103642445653,0.4311542488533323
llm_judge_together_no_ref_de.csv,hallucination_score,avg_answer_acc,de,0.04714769147237583,0.794443870848915
llm_judge_together_no_ref_de.csv,hallucination_score,avg_user_sat,de,0.018747869681297446,0.9175228403818231
llm_judge_together_no_ref_de.csv,hallucination_score,avg_coherence,de,0.057277857119700404,0.751540167664059
llm_judge_together_no_ref_de.csv,hallucination_score,avg_context_qual,de,0.08436541356583852,0.6406507698881544
llm_judge_together_no_ref_de.csv,hallucination_score,avg_overall,de,0.05632525729606982,0.7555458705694831
llm_judge_together_no_ref_de.csv,hallucination_score,overall_mean,de,0.05573773013560434,0.7580195610899042
llm_judge_together_no_ref_de.csv,answer_accuracy_score,avg_hallucination,de,-0.014761738739764886,0.9350170523427549
llm_judge_together_no_ref_de.csv,answer_accuracy_score,avg_answer_acc,de,-0.01024676412568562,0.9548679021858884
llm_judge_together_no_ref_de.csv,answer_accuracy_score,avg_user_sat,de,0.148443678716152,0.40968627764482635
llm_judge_together_no_ref_de.csv,answer_accuracy_score,avg_coherence,de,-0.02417162449833412,0.893781608734386
llm_judge_together_no_ref_de.csv,answer_accuracy_score,avg_context_qual,de,-0.0377784711989141,0.8346612488165477
llm_judge_together_no_ref_de.csv,answer_accuracy_score,avg_overall,de,-0.0012875211504607901,0.9943262521316931
llm_judge_together_no_ref_de.csv,answer_accuracy_score,overall_mean,de,0.0122508755180521,0.9460525023581632
llm_judge_together_no_ref_de.csv,user_satisfaction_score,avg_hallucination,de,0.03389000170844545,0.8514801916167193
llm_judge_together_no_ref_de.csv,user_satisfaction_score,avg_answer_acc,de,0.040620289073933585,0.8224143102070651
llm_judge_together_no_ref_de.csv,user_satisfaction_score,avg_user_sat,de,0.29202068887799204,0.09914446431794947
llm_judge_together_no_ref_de.csv,user_satisfaction_score,avg_coherence,de,0.031207052931328985,0.8631228711163728
llm_judge_together_no_ref_de.csv,user_satisfaction_score,avg_context_qual,de,0.055941197673260334,0.7571626250695
llm_judge_together_no_ref_de.csv,user_satisfaction_score,avg_overall,de,0.0641485239624778,0.7228429064895566
llm_judge_together_no_ref_de.csv,user_satisfaction_score,overall_mean,de,0.09611782833069042,0.5946571817083621
llm_judge_together_no_ref_de.csv,coherence_clarity_fluency_score,avg_hallucination,de,0.1418103642445653,0.4311542488533323
llm_judge_together_no_ref_de.csv,coherence_clarity_fluency_score,avg_answer_acc,de,0.04714769147237583,0.794443870848915
llm_judge_together_no_ref_de.csv,coherence_clarity_fluency_score,avg_user_sat,de,0.018747869681297446,0.9175228403818231
llm_judge_together_no_ref_de.csv,coherence_clarity_fluency_score,avg_coherence,de,0.057277857119700404,0.751540167664059
llm_judge_together_no_ref_de.csv,coherence_clarity_fluency_score,avg_context_qual,de,0.08436541356583852,0.6406507698881544
llm_judge_together_no_ref_de.csv,coherence_clarity_fluency_score,avg_overall,de,0.05632525729606982,0.7555458705694831
llm_judge_together_no_ref_de.csv,coherence_clarity_fluency_score,overall_mean,de,0.05573773013560434,0.7580195610899042
llm_judge_together_no_ref_de.csv,context_quality_score,avg_hallucination,de,-0.11420642683848313,0.5268348137463597
llm_judge_together_no_ref_de.csv,context_quality_score,avg_answer_acc,de,0.024763180739233722,0.8911973024331064
llm_judge_together_no_ref_de.csv,context_quality_score,avg_user_sat,de,-0.004923431783140223,0.9783061723850586
llm_judge_together_no_ref_de.csv,context_quality_score,avg_coherence,de,-0.14540505460457664,0.41944443261221065
llm_judge_together_no_ref_de.csv,context_quality_score,avg_context_qual,de,-0.03938745426512178,0.8277223920350518
llm_judge_together_no_ref_de.csv,context_quality_score,avg_overall,de,0.05423636984665526,0.764351458359857
llm_judge_together_no_ref_de.csv,context_quality_score,overall_mean,de,-0.0439123351188244,0.8082785276375697
llm_judge_together_no_ref_de.csv,overall_score,avg_hallucination,de,-0.0055190125455216186,0.9756826308614726
llm_judge_together_no_ref_de.csv,overall_score,avg_answer_acc,de,0.016229427987397814,0.9285718005130644
llm_judge_together_no_ref_de.csv,overall_score,avg_user_sat,de,0.24672337532061947,0.1663047235023385
llm_judge_together_no_ref_de.csv,overall_score,avg_coherence,de,-0.008647582123607825,0.9619059731916715
llm_judge_together_no_ref_de.csv,overall_score,avg_context_qual,de,0.015095885296863905,0.9335493011320792
llm_judge_together_no_ref_de.csv,overall_score,avg_overall,de,0.04469199575876318,0.8049392481847211
llm_judge_together_no_ref_de.csv,overall_score,overall_mean,de,0.057502909331604256,0.7505947425646318
llm_judge_together_no_ref_en.csv,hallucination_score,avg_hallucination,en,-0.23098709945099907,0.19589723447543525
llm_judge_together_no_ref_en.csv,hallucination_score,avg_answer_acc,en,0.033781727495776535,0.8519494699572451
llm_judge_together_no_ref_en.csv,hallucination_score,avg_user_sat,en,0.04049988356300213,0.8229323906499992
llm_judge_together_no_ref_en.csv,hallucination_score,avg_coherence,en,-0.30270163175637677,0.08684647308403569
llm_judge_together_no_ref_en.csv,hallucination_score,avg_context_qual,en,-0.040632270632233396,0.822362760019658
llm_judge_together_no_ref_en.csv,hallucination_score,avg_overall,en,0.06763309609060268,0.7084262945005704
llm_judge_together_no_ref_en.csv,hallucination_score,overall_mean,en,-0.0868256123968497,0.630905824412164
llm_judge_together_no_ref_en.csv,answer_accuracy_score,avg_hallucination,en,-0.18151566680730052,0.31203943686090196
llm_judge_together_no_ref_en.csv,answer_accuracy_score,avg_answer_acc,en,0.22501457928751695,0.2080342213857466
llm_judge_together_no_ref_en.csv,answer_accuracy_score,avg_user_sat,en,0.33799212987119026,0.05437999652209497
llm_judge_together_no_ref_en.csv,answer_accuracy_score,avg_coherence,en,-0.11696953072017907,0.5168161110763021
llm_judge_together_no_ref_en.csv,answer_accuracy_score,avg_context_qual,en,0.0953183017553846,0.5977403029536743
llm_judge_together_no_ref_en.csv,answer_accuracy_score,avg_overall,en,0.19660366541105825,0.2728162544454622
llm_judge_together_no_ref_en.csv,answer_accuracy_score,overall_mean,en,0.09712687205697673,0.5907760174260284
llm_judge_together_no_ref_en.csv,user_satisfaction_score,avg_hallucination,en,-0.13539021184922045,0.4525095772114266
llm_judge_together_no_ref_en.csv,user_satisfaction_score,avg_answer_acc,en,0.30489377335200474,0.08447398132270334
llm_judge_together_no_ref_en.csv,user_satisfaction_score,avg_user_sat,en,0.5545804306301361,0.0008105690433963136
llm_judge_together_no_ref_en.csv,user_satisfaction_score,avg_coherence,en,-0.09063285820139882,0.6159451831331364
llm_judge_together_no_ref_en.csv,user_satisfaction_score,avg_context_qual,en,0.23748252256555907,0.18326749657786817
llm_judge_together_no_ref_en.csv,user_satisfaction_score,avg_overall,en,0.27527796386778447,0.12101997065561154
llm_judge_together_no_ref_en.csv,user_satisfaction_score,overall_mean,en,0.2410216420623839,0.17663273908369467
llm_judge_together_no_ref_en.csv,coherence_clarity_fluency_score,avg_hallucination,en,,
llm_judge_together_no_ref_en.csv,coherence_clarity_fluency_score,avg_answer_acc,en,,
llm_judge_together_no_ref_en.csv,coherence_clarity_fluency_score,avg_user_sat,en,,
llm_judge_together_no_ref_en.csv,coherence_clarity_fluency_score,avg_coherence,en,,
llm_judge_together_no_ref_en.csv,coherence_clarity_fluency_score,avg_context_qual,en,,
llm_judge_together_no_ref_en.csv,coherence_clarity_fluency_score,avg_overall,en,,
llm_judge_together_no_ref_en.csv,coherence_clarity_fluency_score,overall_mean,en,,
llm_judge_together_no_ref_en.csv,context_quality_score,avg_hallucination,en,-0.14096893120566553,0.43392102645441244
llm_judge_together_no_ref_en.csv,context_quality_score,avg_answer_acc,en,0.1570162031457724,0.38285875136131897
llm_judge_together_no_ref_en.csv,context_quality_score,avg_user_sat,en,0.15126584522688322,0.40073935231081437
llm_judge_together_no_ref_en.csv,context_quality_score,avg_coherence,en,-0.22269029093493287,0.2128946443950733
llm_judge_together_no_ref_en.csv,context_quality_score,avg_context_qual,en,0.011241504217991376,0.9504916608044458
llm_judge_together_no_ref_en.csv,context_quality_score,avg_overall,en,0.05613501705527518,0.7563465876250342
llm_judge_together_no_ref_en.csv,context_quality_score,overall_mean,en,0.0055434367019213575,0.9755750472676507
llm_judge_together_no_ref_en.csv,overall_score,avg_hallucination,en,-0.17535348281512106,0.32902547643743874
llm_judge_together_no_ref_en.csv,overall_score,avg_answer_acc,en,0.2795044104068307,0.11518684706468263
llm_judge_together_no_ref_en.csv,overall_score,avg_user_sat,en,0.5005683865456073,0.0030088705353145877
llm_judge_together_no_ref_en.csv,overall_score,avg_coherence,en,-0.12365900418380459,0.492955903545532
llm_judge_together_no_ref_en.csv,overall_score,avg_context_qual,en,0.1875762478801459,0.2958799405769224
llm_judge_together_no_ref_en.csv,overall_score,avg_overall,en,0.25803194479856234,0.14711177456333477
llm_judge_together_no_ref_en.csv,overall_score,overall_mean,en,0.1864286684706115,0.2988981141263684
llm_judge_together_with_ref_de.csv,hallucination_score,avg_hallucination,de,0.2338880431234081,0.19018374784424347
llm_judge_together_with_ref_de.csv,hallucination_score,avg_answer_acc,de,0.20560471383684908,0.2510168153915774
llm_judge_together_with_ref_de.csv,hallucination_score,avg_user_sat,de,0.1690167245665872,0.3470747316955989
llm_judge_together_with_ref_de.csv,hallucination_score,avg_coherence,de,0.280203042459697,0.1142432353915209
llm_judge_together_with_ref_de.csv,hallucination_score,avg_context_qual,de,0.25942101910220355,0.14487027568838132
llm_judge_together_with_ref_de.csv,hallucination_score,avg_overall,de,0.2676702729372853,0.1320689211793549
llm_judge_together_with_ref_de.csv,hallucination_score,overall_mean,de,0.23761133716010618,0.18302297412782176
llm_judge_together_with_ref_de.csv,answer_accuracy_score,avg_hallucination,de,0.1710791406317284,0.34113568677310946
llm_judge_together_with_ref_de.csv,answer_accuracy_score,avg_answer_acc,de,0.2829846475375108,0.11054353045089649
llm_judge_together_with_ref_de.csv,answer_accuracy_score,avg_user_sat,de,0.5773366809654511,0.0004352466463563584
llm_judge_together_with_ref_de.csv,answer_accuracy_score,avg_coherence,de,0.42340471976161476,0.01407984338136967
llm_judge_together_with_ref_de.csv,answer_accuracy_score,avg_context_qual,de,0.37803473697540685,0.03007312027828109
llm_judge_together_with_ref_de.csv,answer_accuracy_score,avg_overall,de,0.4267063219690697,0.013270719498179448
llm_judge_together_with_ref_de.csv,answer_accuracy_score,overall_mean,de,0.39510541634657675,0.022865519860084917
llm_judge_together_with_ref_de.csv,user_satisfaction_score,avg_hallucination,de,0.1710791406317284,0.34113568677310946
llm_judge_together_with_ref_de.csv,user_satisfaction_score,avg_answer_acc,de,0.2829846475375108,0.11054353045089649
llm_judge_together_with_ref_de.csv,user_satisfaction_score,avg_user_sat,de,0.5773366809654511,0.0004352466463563584
llm_judge_together_with_ref_de.csv,user_satisfaction_score,avg_coherence,de,0.42340471976161476,0.01407984338136967
llm_judge_together_with_ref_de.csv,user_satisfaction_score,avg_context_qual,de,0.37803473697540685,0.03007312027828109
llm_judge_together_with_ref_de.csv,user_satisfaction_score,avg_overall,de,0.4267063219690697,0.013270719498179448
llm_judge_together_with_ref_de.csv,user_satisfaction_score,overall_mean,de,0.39510541634657675,0.022865519860084917
llm_judge_together_with_ref_de.csv,coherence_clarity_fluency_score,avg_hallucination,de,0.019861987276257938,0.9126396661245447
llm_judge_together_with_ref_de.csv,coherence_clarity_fluency_score,avg_answer_acc,de,0.15353172058324907,0.3936374870852699
llm_judge_together_with_ref_de.csv,coherence_clarity_fluency_score,avg_user_sat,de,0.2166309984581698,0.22593006580753724
llm_judge_together_with_ref_de.csv,coherence_clarity_fluency_score,avg_coherence,de,0.0952653806029985,0.5979446200201675
llm_judge_together_with_ref_de.csv,coherence_clarity_fluency_score,avg_context_qual,de,0.05415774961454245,0.7646834515185852
llm_judge_together_with_ref_de.csv,coherence_clarity_fluency_score,avg_overall,de,0.1725702676939031,0.3368805362064623
llm_judge_together_with_ref_de.csv,coherence_clarity_fluency_score,overall_mean,de,0.1317370053564732,0.46491001840014
llm_judge_together_with_ref_de.csv,context_quality_score,avg_hallucination,de,0.08750797241193717,0.62821365214067
llm_judge_together_with_ref_de.csv,context_quality_score,avg_answer_acc,de,0.19156794836642363,0.28553324017367904
llm_judge_together_with_ref_de.csv,context_quality_score,avg_user_sat,de,0.5285387314887265,0.001567280568936029
llm_judge_together_with_ref_de.csv,context_quality_score,avg_coherence,de,0.2612819234849259,0.141906518656418
llm_judge_together_with_ref_de.csv,context_quality_score,avg_context_qual,de,0.3219425706735537,0.06768504428853263
llm_judge_together_with_ref_de.csv,context_quality_score,avg_overall,de,0.31399191742517835,0.07515623680046628
llm_judge_together_with_ref_de.csv,context_quality_score,overall_mean,de,0.319602229506986,0.06982120056941017
llm_judge_together_with_ref_de.csv,overall_score,avg_hallucination,de,0.16566778275676108,0.3568507952685597
llm_judge_together_with_ref_de.csv,overall_score,avg_answer_acc,de,0.2693581570140663,0.12955569631028008
llm_judge_together_with_ref_de.csv,overall_score,avg_user_sat,de,0.5950919333811067,0.00025935746422596853
llm_judge_together_with_ref_de.csv,overall_score,avg_coherence,de,0.41628147632283286,0.015967164051577856
llm_judge_together_with_ref_de.csv,overall_score,avg_context_qual,de,0.37868703190102576,0.02976718290935248
llm_judge_together_with_ref_de.csv,overall_score,avg_overall,de,0.42075085191416317,0.01475985290367934
llm_judge_together_with_ref_de.csv,overall_score,overall_mean,de,0.3951809288653335,0.022837135905203636
llm_judge_together_with_ref_en.csv,hallucination_score,avg_hallucination,en,0.27703804491043266,0.11856467818421262
llm_judge_together_with_ref_en.csv,hallucination_score,avg_answer_acc,en,0.2858169748727726,0.10686952460816157
llm_judge_together_with_ref_en.csv,hallucination_score,avg_user_sat,en,0.20185269662894684,0.25995883429833777
llm_judge_together_with_ref_en.csv,hallucination_score,avg_coherence,en,0.0565638888693454,0.7545418284060242
llm_judge_together_with_ref_en.csv,hallucination_score,avg_context_qual,en,0.2873791829089147,0.10488283969533899
llm_judge_together_with_ref_en.csv,hallucination_score,avg_overall,en,0.3058434141793755,0.08346182106862758
llm_judge_together_with_ref_en.csv,hallucination_score,overall_mean,en,0.26881201943736316,0.13036498981636277
llm_judge_together_with_ref_en.csv,answer_accuracy_score,avg_hallucination,en,0.05540656299903051,0.7594149285778864
llm_judge_together_with_ref_en.csv,answer_accuracy_score,avg_answer_acc,en,0.3849177746820614,0.026971053915347293
llm_judge_together_with_ref_en.csv,answer_accuracy_score,avg_user_sat,en,0.6102148724183544,0.0001628424120200623
llm_judge_together_with_ref_en.csv,answer_accuracy_score,avg_coherence,en,-0.05413171451685627,0.7647934001512983
llm_judge_together_with_ref_en.csv,answer_accuracy_score,avg_context_qual,en,0.4565945134735585,0.00756324292492179
llm_judge_together_with_ref_en.csv,answer_accuracy_score,avg_overall,en,0.45830012584353425,0.0073134870920434905
llm_judge_together_with_ref_en.csv,answer_accuracy_score,overall_mean,en,0.40543066515733234,0.019245279010662183
llm_judge_together_with_ref_en.csv,user_satisfaction_score,avg_hallucination,en,0.07104181195771148,0.6944189332695954
llm_judge_together_with_ref_en.csv,user_satisfaction_score,avg_answer_acc,en,0.3644233757958722,0.03706558379781291
llm_judge_together_with_ref_en.csv,user_satisfaction_score,avg_user_sat,en,0.6079809419579516,0.00017469143874904037
llm_judge_together_with_ref_en.csv,user_satisfaction_score,avg_coherence,en,-0.05722663947862269,0.7517553781810812
llm_judge_together_with_ref_en.csv,user_satisfaction_score,avg_context_qual,en,0.4521673430595486,0.008245668518578218
llm_judge_together_with_ref_en.csv,user_satisfaction_score,avg_overall,en,0.42833490630505766,0.01288621584087846
llm_judge_together_with_ref_en.csv,user_satisfaction_score,overall_mean,en,0.3997667632369114,0.021167103591362955
llm_judge_together_with_ref_en.csv,coherence_clarity_fluency_score,avg_hallucination,en,0.19892680698049256,0.26707566825422957
llm_judge_together_with_ref_en.csv,coherence_clarity_fluency_score,avg_answer_acc,en,0.2922516915484631,0.09886522460621422
llm_judge_together_with_ref_en.csv,coherence_clarity_fluency_score,avg_user_sat,en,0.27850053939607794,0.11655291591857024
llm_judge_together_with_ref_en.csv,coherence_clarity_fluency_score,avg_coherence,en,0.041203719496460255,0.8199049737924813
llm_judge_together_with_ref_en.csv,coherence_clarity_fluency_score,avg_context_qual,en,0.3785567156612807,0.029828099283352838
llm_judge_together_with_ref_en.csv,coherence_clarity_fluency_score,avg_overall,en,0.3600651449512312,0.03956391746749785
llm_judge_together_with_ref_en.csv,coherence_clarity_fluency_score,overall_mean,en,0.34223707458365404,0.0512331911274581
llm_judge_together_with_ref_en.csv,context_quality_score,avg_hallucination,en,0.13767412437965298,0.4448482902723925
llm_judge_together_with_ref_en.csv,context_quality_score,avg_answer_acc,en,0.3695451163107114,0.03429437622581669
llm_judge_together_with_ref_en.csv,context_quality_score,avg_user_sat,en,0.5882155023520683,0.0003180683090951455
llm_judge_together_with_ref_en.csv,context_quality_score,avg_coherence,en,-0.001010977136686058,0.9955448910532168
llm_judge_together_with_ref_en.csv,context_quality_score,avg_context_qual,en,0.43887180422825783,0.010618028355890485
llm_judge_together_with_ref_en.csv,context_quality_score,avg_overall,en,0.37191380620931525,0.03307094025399533
llm_judge_together_with_ref_en.csv,context_quality_score,overall_mean,en,0.41026080998051906,0.01772298792695227
llm_judge_together_with_ref_en.csv,overall_score,avg_hallucination,en,0.1129123266789742,0.531559254258715
llm_judge_together_with_ref_en.csv,overall_score,avg_answer_acc,en,0.4171426492896564,0.01572832890443466
llm_judge_together_with_ref_en.csv,overall_score,avg_user_sat,en,0.6179331609522493,0.000127229456654117
llm_judge_together_with_ref_en.csv,overall_score,avg_coherence,en,-0.041133633859162806,0.8202063202698312
llm_judge_together_with_ref_en.csv,overall_score,avg_context_qual,en,0.49445827291837713,0.003444938916295176
llm_judge_together_with_ref_en.csv,overall_score,avg_overall,en,0.4710247096698997,0.005662453302481149
llm_judge_together_with_ref_en.csv,overall_score,overall_mean,en,0.43254315287729295,0.011935791062169576
llm_judge_seperate_no_ref_de.csv,hallucination_score,avg_hallucination,de,0.2629060682478228,0.13935622524307015
llm_judge_seperate_no_ref_de.csv,hallucination_score,avg_answer_acc,de,0.1727649199006679,0.3363274722177293
llm_judge_seperate_no_ref_de.csv,hallucination_score,avg_user_sat,de,0.08566911171743283,0.6354793009661235
llm_judge_seperate_no_ref_de.csv,hallucination_score,avg_coherence,de,0.0822590596208933,0.6490412592358737
llm_judge_seperate_no_ref_de.csv,hallucination_score,avg_context_qual,de,0.25170400918883834,0.15764240105987973
llm_judge_seperate_no_ref_de.csv,hallucination_score,avg_overall,de,0.21162390909593023,0.237102462236931
llm_judge_seperate_no_ref_de.csv,hallucination_score,overall_mean,de,0.2094164661055826,0.2421438344579275
llm_judge_seperate_no_ref_de.csv,answer_accuracy_score,avg_hallucination,de,0.23627966718117097,0.18556192981660236
llm_judge_seperate_no_ref_de.csv,answer_accuracy_score,avg_answer_acc,de,0.19833972016188664,0.2685188706159854
llm_judge_seperate_no_ref_de.csv,answer_accuracy_score,avg_user_sat,de,0.3254784638070256,0.0645549597033703
llm_judge_seperate_no_ref_de.csv,answer_accuracy_score,avg_coherence,de,0.1906432711633766,0.287909093681273
llm_judge_seperate_no_ref_de.csv,answer_accuracy_score,avg_context_qual,de,0.16089306931810712,0.3710706878682436
llm_judge_seperate_no_ref_de.csv,answer_accuracy_score,avg_overall,de,0.21649947315548435,0.2262188895885991
llm_judge_seperate_no_ref_de.csv,answer_accuracy_score,overall_mean,de,0.2609863652476663,0.14237425599306194
llm_judge_seperate_no_ref_de.csv,user_satisfaction_score,avg_hallucination,de,0.1396002900854813,0.43844212260215554
llm_judge_seperate_no_ref_de.csv,user_satisfaction_score,avg_answer_acc,de,0.10586350815422724,0.5576455201731181
llm_judge_seperate_no_ref_de.csv,user_satisfaction_score,avg_user_sat,de,0.42884760574343034,0.012767124976541037
llm_judge_seperate_no_ref_de.csv,user_satisfaction_score,avg_coherence,de,0.17726871149858078,0.3236862548684224
llm_judge_seperate_no_ref_de.csv,user_satisfaction_score,avg_context_qual,de,0.13832260393630924,0.4426858885664495
llm_judge_seperate_no_ref_de.csv,user_satisfaction_score,avg_overall,de,0.12062484854841428,0.5037081571464557
llm_judge_seperate_no_ref_de.csv,user_satisfaction_score,overall_mean,de,0.20688956963781424,0.24800216536084874
llm_judge_seperate_no_ref_de.csv,coherence_clarity_fluency_score,avg_hallucination,de,0.1418103642445653,0.4311542488533323
llm_judge_seperate_no_ref_de.csv,coherence_clarity_fluency_score,avg_answer_acc,de,0.19802030418397848,0.26930620739350347
llm_judge_seperate_no_ref_de.csv,coherence_clarity_fluency_score,avg_user_sat,de,0.26247017553816426,0.14003736318548163
llm_judge_seperate_no_ref_de.csv,coherence_clarity_fluency_score,avg_coherence,de,0.057277857119700404,0.751540167664059
llm_judge_seperate_no_ref_de.csv,coherence_clarity_fluency_score,avg_context_qual,de,0.15935689229102828,0.3757157545718992
llm_judge_seperate_no_ref_de.csv,coherence_clarity_fluency_score,avg_overall,de,0.16897577188820945,0.347193290804823
llm_judge_seperate_no_ref_de.csv,coherence_clarity_fluency_score,overall_mean,de,0.21366129885315,0.23251246385019006
llm_judge_seperate_no_ref_de.csv,context_quality_score,avg_hallucination,de,0.0998312974194319,0.5804288500081587
llm_judge_seperate_no_ref_de.csv,context_quality_score,avg_answer_acc,de,0.18421978035605713,0.30476234709460426
llm_judge_seperate_no_ref_de.csv,context_quality_score,avg_user_sat,de,0.3079325538261476,0.08126802033301186
llm_judge_seperate_no_ref_de.csv,context_quality_score,avg_coherence,de,-0.004864345502935765,0.9785664608761196
llm_judge_seperate_no_ref_de.csv,context_quality_score,avg_context_qual,de,0.17022674180158323,0.3435827586002559
llm_judge_seperate_no_ref_de.csv,context_quality_score,avg_overall,de,0.22960538747021073,0.19866016595602234
llm_judge_seperate_no_ref_de.csv,context_quality_score,overall_mean,de,0.1986076550235529,0.26785959116878433
llm_judge_seperate_no_ref_de.csv,overall_score,avg_hallucination,de,0.2096041474118032,0.241712440508936
llm_judge_seperate_no_ref_de.csv,overall_score,avg_answer_acc,de,0.22915310380059636,0.19957042602166128
llm_judge_seperate_no_ref_de.csv,overall_score,avg_user_sat,de,0.48024043826029517,0.004676292242937584
llm_judge_seperate_no_ref_de.csv,overall_score,avg_coherence,de,0.18199049228900732,0.31075381153266135
llm_judge_seperate_no_ref_de.csv,overall_score,avg_context_qual,de,0.27478763075978413,0.12171066579995939
llm_judge_seperate_no_ref_de.csv,overall_score,avg_overall,de,0.2793279931052301,0.11542604490616144
llm_judge_seperate_no_ref_de.csv,overall_score,overall_mean,de,0.3182688666201749,0.07106153998199284
llm_judge_seperate_no_ref_en.csv,hallucination_score,avg_hallucination,en,0.11601105367136005,0.5202807786253003
llm_judge_seperate_no_ref_en.csv,hallucination_score,avg_answer_acc,en,0.20563808293928845,0.25093821554626566
llm_judge_seperate_no_ref_en.csv,hallucination_score,avg_user_sat,en,0.1451249395312878,0.42035047604726905
llm_judge_seperate_no_ref_en.csv,hallucination_score,avg_coherence,en,-0.1406071221370407,0.4351137094843135
llm_judge_seperate_no_ref_en.csv,hallucination_score,avg_context_qual,en,0.2444630687234185,0.1703460593695917
llm_judge_seperate_no_ref_en.csv,hallucination_score,avg_overall,en,0.2932166332960674,0.09770523604316887
llm_judge_seperate_no_ref_en.csv,hallucination_score,overall_mean,en,0.17314317502251672,0.33525432770986363
llm_judge_seperate_no_ref_en.csv,answer_accuracy_score,avg_hallucination,en,0.031643522270009015,0.8612268164707559
llm_judge_seperate_no_ref_en.csv,answer_accuracy_score,avg_answer_acc,en,0.42837661391436554,0.012876493145818116
llm_judge_seperate_no_ref_en.csv,answer_accuracy_score,avg_user_sat,en,0.49183496749692895,0.003648307685110503
llm_judge_seperate_no_ref_en.csv,answer_accuracy_score,avg_coherence,en,-0.07089594175361072,0.6950163658003119
llm_judge_seperate_no_ref_en.csv,answer_accuracy_score,avg_context_qual,en,0.33829311094922515,0.05415194795842869
llm_judge_seperate_no_ref_en.csv,answer_accuracy_score,avg_overall,en,0.40745600984584723,0.018594245361577957
llm_judge_seperate_no_ref_en.csv,answer_accuracy_score,overall_mean,en,0.3214879729423854,0.06809592986053696
llm_judge_seperate_no_ref_en.csv,user_satisfaction_score,avg_hallucination,en,-0.15891288383537958,0.37706467043719705
llm_judge_seperate_no_ref_en.csv,user_satisfaction_score,avg_answer_acc,en,0.2678666074041726,0.13177475196773633
llm_judge_seperate_no_ref_en.csv,user_satisfaction_score,avg_user_sat,en,0.4707024987486393,0.005699932509583028
llm_judge_seperate_no_ref_en.csv,user_satisfaction_score,avg_coherence,en,-0.03939643191418987,0.8276837106030461
llm_judge_seperate_no_ref_en.csv,user_satisfaction_score,avg_context_qual,en,0.1932874894494882,0.2811486699692284
llm_judge_seperate_no_ref_en.csv,user_satisfaction_score,avg_overall,en,0.31097177242152524,0.07815630083838475
llm_judge_seperate_no_ref_en.csv,user_satisfaction_score,overall_mean,en,0.23089958246395706,0.19607143894890283
llm_judge_seperate_no_ref_en.csv,coherence_clarity_fluency_score,avg_hallucination,en,-0.0661953837071855,0.7143627984224667
llm_judge_seperate_no_ref_en.csv,coherence_clarity_fluency_score,avg_answer_acc,en,0.07523547938632787,0.6773215742223508
llm_judge_seperate_no_ref_en.csv,coherence_clarity_fluency_score,avg_user_sat,en,0.02818672605010608,0.8762633854774208
llm_judge_seperate_no_ref_en.csv,coherence_clarity_fluency_score,avg_coherence,en,-0.14363960436337936,0.4251730743608978
llm_judge_seperate_no_ref_en.csv,coherence_clarity_fluency_score,avg_context_qual,en,0.075410302802338,0.67661217747644
llm_judge_seperate_no_ref_en.csv,coherence_clarity_fluency_score,avg_overall,en,0.09414128554027902,0.6022916078177605
llm_judge_seperate_no_ref_en.csv,coherence_clarity_fluency_score,overall_mean,en,0.04648312985423773,0.797280727640581
llm_judge_seperate_no_ref_en.csv,context_quality_score,avg_hallucination,en,-0.0988734605249944,0.5840843083105762
llm_judge_seperate_no_ref_en.csv,context_quality_score,avg_answer_acc,en,0.11339297192443432,0.5298021567169546
llm_judge_seperate_no_ref_en.csv,context_quality_score,avg_user_sat,en,0.33808094336099215,0.05431262490503334
llm_judge_seperate_no_ref_en.csv,context_quality_score,avg_coherence,en,-0.16002740379599809,0.37368409092643406
llm_judge_seperate_no_ref_en.csv,context_quality_score,avg_context_qual,en,0.25727257549500404,0.1483477447250843
llm_judge_seperate_no_ref_en.csv,context_quality_score,avg_overall,en,0.2177584396255371,0.22346451784346538
llm_judge_seperate_no_ref_en.csv,context_quality_score,overall_mean,en,0.16403578941350927,0.3616739751074135
llm_judge_seperate_no_ref_en.csv,overall_score,avg_hallucination,en,-0.006148446607250203,0.9729102374466123
llm_judge_seperate_no_ref_en.csv,overall_score,avg_answer_acc,en,0.3918906297681007,0.02410119977897585
llm_judge_seperate_no_ref_en.csv,overall_score,avg_user_sat,en,0.5634021346314039,0.0006403303755341081
llm_judge_seperate_no_ref_en.csv,overall_score,avg_coherence,en,-0.07075155854488714,0.6956078854888936
llm_judge_seperate_no_ref_en.csv,overall_score,avg_context_qual,en,0.3672645345192241,0.035506785719706044
llm_judge_seperate_no_ref_en.csv,overall_score,avg_overall,en,0.4598100437684396,0.007098287969413428
llm_judge_seperate_no_ref_en.csv,overall_score,overall_mean,en,0.3517721653851467,0.044692276260899326
llm_judge_seperate_with_ref_de.csv,hallucination_score,avg_hallucination,de,0.05088109221690394,0.7785558403537446
llm_judge_seperate_with_ref_de.csv,hallucination_score,avg_answer_acc,de,0.22506657848664022,0.20792636580218993
llm_judge_seperate_with_ref_de.csv,hallucination_score,avg_user_sat,de,0.1321407246705166,0.46353086924078457
llm_judge_seperate_with_ref_de.csv,hallucination_score,avg_coherence,de,0.17668185940678569,0.3253165302387103
llm_judge_seperate_with_ref_de.csv,hallucination_score,avg_context_qual,de,0.1376002496377759,0.44509499372058214
llm_judge_seperate_with_ref_de.csv,hallucination_score,avg_overall,de,0.2452252799181534,0.16897551708756203
llm_judge_seperate_with_ref_de.csv,hallucination_score,overall_mean,de,0.15950718762832858,0.37525979375572116
llm_judge_seperate_with_ref_de.csv,answer_accuracy_score,avg_hallucination,de,0.3082772796192811,0.08091034089722042
llm_judge_seperate_with_ref_de.csv,answer_accuracy_score,avg_answer_acc,de,0.3928450270934689,0.023728744363124832
llm_judge_seperate_with_ref_de.csv,answer_accuracy_score,avg_user_sat,de,0.5958131614324316,0.00025379750819714484
llm_judge_seperate_with_ref_de.csv,answer_accuracy_score,avg_coherence,de,0.4955117902941349,0.003366068758866669
llm_judge_seperate_with_ref_de.csv,answer_accuracy_score,avg_context_qual,de,0.45437515094207165,0.00789907589723412
llm_judge_seperate_with_ref_de.csv,answer_accuracy_score,avg_overall,de,0.501923590531494,0.002918912703530763
llm_judge_seperate_with_ref_de.csv,answer_accuracy_score,overall_mean,de,0.4876365769395365,0.0039953694874350024
llm_judge_seperate_with_ref_de.csv,user_satisfaction_score,avg_hallucination,de,0.24766703756246558,0.16463793758161901
llm_judge_seperate_with_ref_de.csv,user_satisfaction_score,avg_answer_acc,de,0.3077156753107952,0.0814936727067771
llm_judge_seperate_with_ref_de.csv,user_satisfaction_score,avg_user_sat,de,0.554650122772191,0.0008090814042923989
llm_judge_seperate_with_ref_de.csv,user_satisfaction_score,avg_coherence,de,0.5074559057207747,0.0025754295977703815
llm_judge_seperate_with_ref_de.csv,user_satisfaction_score,avg_context_qual,de,0.3708267583050896,0.03362792079081443
llm_judge_seperate_with_ref_de.csv,user_satisfaction_score,avg_overall,de,0.440277279897787,0.010342698259166073
llm_judge_seperate_with_ref_de.csv,user_satisfaction_score,overall_mean,de,0.42185349786036663,0.014474051803847138
llm_judge_seperate_with_ref_de.csv,coherence_clarity_fluency_score,avg_hallucination,de,0.17547005811633506,0.3286989490590835
llm_judge_seperate_with_ref_de.csv,coherence_clarity_fluency_score,avg_answer_acc,de,0.26655490859048264,0.13374926218563732
llm_judge_seperate_with_ref_de.csv,coherence_clarity_fluency_score,avg_user_sat,de,0.39860625931966914,0.02157985715536163
llm_judge_seperate_with_ref_de.csv,coherence_clarity_fluency_score,avg_coherence,de,0.3827919020449879,0.027899771082472637
llm_judge_seperate_with_ref_de.csv,coherence_clarity_fluency_score,avg_context_qual,de,0.3371226347451662,0.055043078715373506
llm_judge_seperate_with_ref_de.csv,coherence_clarity_fluency_score,avg_overall,de,0.3454502974679935,0.04894896513870579
llm_judge_seperate_with_ref_de.csv,coherence_clarity_fluency_score,overall_mean,de,0.33894990746215553,0.053656937161752426
llm_judge_seperate_with_ref_de.csv,context_quality_score,avg_hallucination,de,0.07451687004193502,0.6802403931469551
llm_judge_seperate_with_ref_de.csv,context_quality_score,avg_answer_acc,de,0.24250661521897202,0.17390018726982423
llm_judge_seperate_with_ref_de.csv,context_quality_score,avg_user_sat,de,0.582774951261207,0.0003725990094962656
llm_judge_seperate_with_ref_de.csv,context_quality_score,avg_coherence,de,0.37447359621197884,0.03178895898304329
llm_judge_seperate_with_ref_de.csv,context_quality_score,avg_context_qual,de,0.31640741696652325,0.07282175875418416
llm_judge_seperate_with_ref_de.csv,context_quality_score,avg_overall,de,0.3866986363652359,0.02621272063494294
llm_judge_seperate_with_ref_de.csv,context_quality_score,overall_mean,de,0.3402361734908101,0.05269791639955178
llm_judge_seperate_with_ref_de.csv,overall_score,avg_hallucination,de,0.18079755490984312,0.313990099001395
llm_judge_seperate_with_ref_de.csv,overall_score,avg_answer_acc,de,0.3574691956436034,0.041115443246592775
llm_judge_seperate_with_ref_de.csv,overall_score,avg_user_sat,de,0.5837282727912804,0.00036248288980423693
llm_judge_seperate_with_ref_de.csv,overall_score,avg_coherence,de,0.44244330711948443,0.009930208557373445
llm_judge_seperate_with_ref_de.csv,overall_score,avg_context_qual,de,0.3900662210431457,0.024826585170371535
llm_judge_seperate_with_ref_de.csv,overall_score,avg_overall,de,0.4725863456948898,0.005483783102711832
llm_judge_seperate_with_ref_de.csv,overall_score,overall_mean,de,0.4239227892104935,0.013950210043318713
llm_judge_seperate_with_ref_en.csv,hallucination_score,avg_hallucination,en,0.2961477558135183,0.0942449883886887
llm_judge_seperate_with_ref_en.csv,hallucination_score,avg_answer_acc,en,0.34747236780986107,0.04755366715345548
llm_judge_seperate_with_ref_en.csv,hallucination_score,avg_user_sat,en,0.25968339069652174,0.14444970409705424
llm_judge_seperate_with_ref_en.csv,hallucination_score,avg_coherence,en,0.16462964474958822,0.3599144234810313
llm_judge_seperate_with_ref_en.csv,hallucination_score,avg_context_qual,en,0.38125569036541374,0.0285870624724755
llm_judge_seperate_with_ref_en.csv,hallucination_score,avg_overall,en,0.42858495335495445,0.012828018264575207
llm_judge_seperate_with_ref_en.csv,hallucination_score,overall_mean,en,0.3770137852944787,0.030557115415102764
llm_judge_seperate_with_ref_en.csv,answer_accuracy_score,avg_hallucination,en,0.1398018277102496,0.4377747625631693
llm_judge_seperate_with_ref_en.csv,answer_accuracy_score,avg_answer_acc,en,0.4320752137971672,0.012038466023883821
llm_judge_seperate_with_ref_en.csv,answer_accuracy_score,avg_user_sat,en,0.6223274822789588,0.00011022946852124977
llm_judge_seperate_with_ref_en.csv,answer_accuracy_score,avg_coherence,en,-0.02736270113618146,0.8798542525295245
llm_judge_seperate_with_ref_en.csv,answer_accuracy_score,avg_context_qual,en,0.5115880749753695,0.0023423227735131034
llm_judge_seperate_with_ref_en.csv,answer_accuracy_score,avg_overall,en,0.5345650916937441,0.0013517884689493397
llm_judge_seperate_with_ref_en.csv,answer_accuracy_score,overall_mean,en,0.46196073139369753,0.006801103616903815
llm_judge_seperate_with_ref_en.csv,user_satisfaction_score,avg_hallucination,en,0.0829213538821061,0.646398435493776
llm_judge_seperate_with_ref_en.csv,user_satisfaction_score,avg_answer_acc,en,0.3973149020562993,0.022046954792918533
llm_judge_seperate_with_ref_en.csv,user_satisfaction_score,avg_user_sat,en,0.6033997072862187,0.00020142735838183507
llm_judge_seperate_with_ref_en.csv,user_satisfaction_score,avg_coherence,en,-0.06122559647283663,0.7350086381539735
llm_judge_seperate_with_ref_en.csv,user_satisfaction_score,avg_context_qual,en,0.442029107106035,0.010007992008771695
llm_judge_seperate_with_ref_en.csv,user_satisfaction_score,avg_overall,en,0.46328144191910786,0.006623923608320472
llm_judge_seperate_with_ref_en.csv,user_satisfaction_score,overall_mean,en,0.4067148337743416,0.018830333747996822
llm_judge_seperate_with_ref_en.csv,coherence_clarity_fluency_score,avg_hallucination,en,0.08059820864277882,0.655687173697063
llm_judge_seperate_with_ref_en.csv,coherence_clarity_fluency_score,avg_answer_acc,en,0.4637112735629614,0.006567116725007406
llm_judge_seperate_with_ref_en.csv,coherence_clarity_fluency_score,avg_user_sat,en,0.652287387208962,3.901582836246327e-05
llm_judge_seperate_with_ref_en.csv,coherence_clarity_fluency_score,avg_coherence,en,0.010920535204405151,0.9519035769708759
llm_judge_seperate_with_ref_en.csv,coherence_clarity_fluency_score,avg_context_qual,en,0.5009044555038535,0.002986341341917581
llm_judge_seperate_with_ref_en.csv,coherence_clarity_fluency_score,avg_overall,en,0.48537864998162544,0.004193531997427437
llm_judge_seperate_with_ref_en.csv,coherence_clarity_fluency_score,overall_mean,en,0.46769542833086164,0.00606008172645638
llm_judge_seperate_with_ref_en.csv,context_quality_score,avg_hallucination,en,-0.0599321902296323,0.7404124604084203
llm_judge_seperate_with_ref_en.csv,context_quality_score,avg_answer_acc,en,0.3341514395464066,0.05735740222701614
llm_judge_seperate_with_ref_en.csv,context_quality_score,avg_user_sat,en,0.5448002371213021,0.0010448299689333439
llm_judge_seperate_with_ref_en.csv,context_quality_score,avg_coherence,en,-0.09758483283704286,0.5890181903907058
llm_judge_seperate_with_ref_en.csv,context_quality_score,avg_context_qual,en,0.3297124331605958,0.060957563200562725
llm_judge_seperate_with_ref_en.csv,context_quality_score,avg_overall,en,0.3364846584362337,0.055533665307793065
llm_judge_seperate_with_ref_en.csv,context_quality_score,overall_mean,en,0.3015159753713456,0.08815080104249169
llm_judge_seperate_with_ref_en.csv,overall_score,avg_hallucination,en,0.12732018022127875,0.4801391083096481
llm_judge_seperate_with_ref_en.csv,overall_score,avg_answer_acc,en,0.45847069603219703,0.007288901479338817
llm_judge_seperate_with_ref_en.csv,overall_score,avg_user_sat,en,0.6300544986060304,8.520380402522016e-05
llm_judge_seperate_with_ref_en.csv,overall_score,avg_coherence,en,-0.019321538023536453,0.9150080730311854
llm_judge_seperate_with_ref_en.csv,overall_score,avg_context_qual,en,0.512036911347912,0.002318143960045157
llm_judge_seperate_with_ref_en.csv,overall_score,avg_overall,en,0.5345730536613221,0.001351521878922242
llm_judge_seperate_with_ref_en.csv,overall_score,overall_mean,en,0.46583547761567773,0.006292451963831593
