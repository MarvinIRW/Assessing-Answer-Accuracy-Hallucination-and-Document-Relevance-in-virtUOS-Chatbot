{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "system_prompt_response_format_all = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across multiple dimensions. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimensions using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "(0 = severe hallucination; multiple claims are incorrect, 4 = no hallucination; all claims are verifiable and correct).\n",
    "-Answer Accuracy: The degree to which an answer accurately addresses the user's question by providing correct, complete, and relevant information that matches the intent of the question. Factual accuracy (no hallucination) is necessary but not sufficient; the answer must also be accurate, comprehensive, and appropriate to the purpose of the question.\n",
    "(0 = inaccurate; fails to answer the question, 4 = fully accurate; directly addresses the question comprehensively).\n",
    "-User Satisfaction: Reflects the user's subjective assessment of the answer's quality, focusing on the effectiveness of understanding the question, answering it, providing meaningful value, and leaving an overall positive impression.\n",
    "(0 = very unsatisfactory; the answer is unhelpful or confusing, 4 = highly satisfactory; the answer provides significant value).\n",
    "-Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "(0 = incoherent or unclear; hard to follow, 4 = highly coherent and clear; easy to read and well-structured).\n",
    "-Context Quality: Assesses the relevance and completeness of the context in supporting the answer. High-quality context is directly related to the user’s question and provides all necessary details for a correct and comprehensive response. If no context is provided, evaluate how its absence affects the quality of the answer.\n",
    "Context is provided:\n",
    "Evaluate how well the provided context (links) aligns with the question and how effectively it supports the answer.\n",
    "(0 = Context is irrelevant or insufficient; does not support the answer, 4 = Context is fully relevant and highly effective; perfectly supports the answer).\n",
    "Context is not provided:\n",
    "Evaluate how the absence of context impacts the quality of the answer.\n",
    "(0 = Severely impacts quality; context would have been essential, 4 = No impact on quality; context would haven been unnecessary).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "For each dimension, follow these steps to ensure thorough and consistent evaluations:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on the evaluation dimensions.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Evaluate Independently: Look at each evaluation dimension independently and do not let one dimension influence the next.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "system_prompt_response_format_all_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across multiple dimensions. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimensions using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "(0 = severe hallucination; multiple claims are incorrect, 4 = no hallucination; all claims are verifiable and correct).\n",
    "-Answer Accuracy: The degree to which an answer accurately addresses the user's question by providing correct, complete, and relevant information that matches the intent of the question. Factual accuracy (no hallucination) is necessary but not sufficient; the answer must also be accurate, comprehensive, and appropriate to the purpose of the question.\n",
    "(0 = inaccurate; fails to answer the question, 4 = fully accurate; directly addresses the question comprehensively).\n",
    "-User Satisfaction: Reflects the user's subjective assessment of the answer's quality, focusing on the effectiveness of understanding the question, answering it, providing meaningful value, and leaving an overall positive impression.\n",
    "(0 = very unsatisfactory; the answer is unhelpful or confusing, 4 = highly satisfactory; the answer provides significant value).\n",
    "-Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "(0 = incoherent or unclear; hard to follow, 4 = highly coherent and clear; easy to read and well-structured).\n",
    "-Context Quality: Assesses the relevance and completeness of the context in supporting the answer. High-quality context is directly related to the user’s question and provides all necessary details for a correct and comprehensive response. If no context is provided, evaluate how its absence affects the quality of the answer.\n",
    "Context is provided:\n",
    "Evaluate how well the provided context (links) aligns with the question and how effectively it supports the answer.\n",
    "(0 = Context is irrelevant or insufficient; does not support the answer, 4 = Context is fully relevant and highly effective; perfectly supports the answer).\n",
    "Context is not provided:\n",
    "Evaluate how the absence of context impacts the quality of the answer.\n",
    "(0 = Severely impacts quality; context would have been essential, 4 = No impact on quality; context would haven been unnecessary).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "For each dimension, follow these steps to ensure thorough and consistent evaluations:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on the evaluation dimensions.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Evaluate Independently: Look at each evaluation dimension independently and do not let one dimension influence the next.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_hallucination = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Hallucination'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "(0 = severe hallucination; multiple claims are incorrect, 4 = no hallucination; all claims are verifiable and correct).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Hallucination:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Hallucination.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "system_prompt_response_format_hallucination_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Hallucination'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "(0 = severe hallucination; multiple claims are incorrect, 4 = no hallucination; all claims are verifiable and correct).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Hallucination:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Hallucination.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_accuracy = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Answer Accuracy'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Answer Accuracy: The degree to which an answer accurately addresses the user's question by providing correct, complete, and relevant information that matches the intent of the question. Factual accuracy (no hallucination) is necessary but not sufficient; the answer must also be accurate, comprehensive, and appropriate to the purpose of the question.\n",
    "(0 = inaccurate; fails to answer the question, 4 = fully accurate; directly addresses the question comprehensively).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Answer Accuracy:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Answer Accuracy.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "system_prompt_response_format_accuracy_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Answer Accuracy'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Answer Accuracy: The degree to which an answer accurately addresses the user's question by providing correct, complete, and relevant information that matches the intent of the question. Factual accuracy (no hallucination) is necessary but not sufficient; the answer must also be accurate, comprehensive, and appropriate to the purpose of the question.\n",
    "(0 = inaccurate; fails to answer the question, 4 = fully accurate; directly addresses the question comprehensively).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Answer Accuracy:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Answer Accuracy.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_satisfaction = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'User Satisfaction'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-User Satisfaction: Reflects the user's subjective assessment of the answer's quality, focusing on the effectiveness of understanding the question, answering it, providing meaningful value, and leaving an overall positive impression.\n",
    "(0 = very unsatisfactory; the answer is unhelpful or confusing, 4 = highly satisfactory; the answer provides significant value).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of User Satisfaction:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on User Satisfaction.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "system_prompt_response_format_satisfaction_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'User Satisfaction'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-User Satisfaction: Reflects the user's subjective assessment of the answer's quality, focusing on the effectiveness of understanding the question, answering it, providing meaningful value, and leaving an overall positive impression.\n",
    "(0 = very unsatisfactory; the answer is unhelpful or confusing, 4 = highly satisfactory; the answer provides significant value).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of User Satisfaction:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on User Satisfaction.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_coherence = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Coherence, Clarity, and Fluency'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "(0 = incoherent or unclear; hard to follow, 4 = highly coherent and clear; easy to read and well-structured).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Coherence, Clarity, and Fluency:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Coherence, Clarity, and Fluency.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "system_prompt_response_format_coherence_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Coherence, Clarity, and Fluency'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "(0 = incoherent or unclear; hard to follow, 4 = highly coherent and clear; easy to read and well-structured).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Coherence, Clarity, and Fluency:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Coherence, Clarity, and Fluency.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_context = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Context Quality'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Context Quality: Assesses the relevance and completeness of the context in supporting the answer. High-quality context is directly related to the user’s question and provides all necessary details for a correct and comprehensive response. If no context is provided, evaluate how its absence affects the quality of the answer.\n",
    "Context is provided:\n",
    "Evaluate how well the provided context (links) aligns with the question and how effectively it supports the answer.\n",
    "(0 = Context is irrelevant or insufficient; does not support the answer, 4 = Context is fully relevant and highly effective; perfectly supports the answer).\n",
    "Context is not provided:\n",
    "Evaluate how the absence of context impacts the quality of the answer.\n",
    "(0 = Severely impacts quality; context would have been essential, 4 = No impact on quality; context would haven been unnecessary).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Context Quality:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Context Quality.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "system_prompt_response_format_context_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Context Quality'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Context Quality: Assesses the relevance and completeness of the context in supporting the answer. High-quality context is directly related to the user’s question and provides all necessary details for a correct and comprehensive response. If no context is provided, evaluate how its absence affects the quality of the answer.\n",
    "Context is provided:\n",
    "Evaluate how well the provided context (links) aligns with the question and how effectively it supports the answer.\n",
    "(0 = Context is irrelevant or insufficient; does not support the answer, 4 = Context is fully relevant and highly effective; perfectly supports the answer).\n",
    "Context is not provided:\n",
    "Evaluate how the absence of context impacts the quality of the answer.\n",
    "(0 = Severely impacts quality; context would have been essential, 4 = No impact on quality; context would haven been unnecessary).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Context Quality:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Context Quality.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Define the schema using pydantic\n",
    "class DimensionScore(BaseModel):\n",
    "    score: int  # 0-4\n",
    "    comment: str\n",
    "\n",
    "class EvaluationOutput_all(BaseModel):\n",
    "    hallucination: DimensionScore\n",
    "    answer_accuracy: DimensionScore\n",
    "    user_satisfaction: DimensionScore\n",
    "    coherence_clarity_fluency: DimensionScore\n",
    "    context_quality: DimensionScore\n",
    "\n",
    "class EvaluationOutput_hallucination(BaseModel):\n",
    "    hallucination: DimensionScore\n",
    "\n",
    "class EvaluationOutput_accuracy(BaseModel):\n",
    "    answer_accuracy: DimensionScore\n",
    "\n",
    "class EvaluationOutput_satisfaction(BaseModel):\n",
    "    user_satisfaction: DimensionScore\n",
    "\n",
    "class EvaluationOutput_coherence(BaseModel):\n",
    "    coherence_clarity_fluency: DimensionScore\n",
    "\n",
    "class EvaluationOutput_context(BaseModel):\n",
    "    context_quality: DimensionScore\n",
    "    \n",
    "def evaluate_with_llm_as_judge_structured(\n",
    "    context: str,\n",
    "    user_question: str,\n",
    "    system_answer: str,\n",
    "    reference_answer: str,\n",
    "    client: OpenAI,\n",
    "    model_name: str,\n",
    "    evaluation_style: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the OpenAI API with structured outputs, enforcing the defined schema.\n",
    "    \n",
    "    If evaluation_style == 'together', do a single call with the multi-dimension\n",
    "    prompt and schema (with or without a reference).\n",
    "    \n",
    "    If evaluation_style == 'separate', do five separate calls (one per dimension);\n",
    "    if reference answers exist for each dimension, use the reference prompt, else\n",
    "    use the default dimension prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1) EVALUATION STYLE: \"TOGETHER\"\n",
    "    # -------------------------------------------------------\n",
    "    if evaluation_style == \"together\":\n",
    "        # If a reference answer is provided and not empty\n",
    "        if reference_answer:\n",
    "            # Use the \"all dimensions with reference\" prompt\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_response_format_all_reference\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "                        Context:\n",
    "                        <context>\n",
    "                        {context}\n",
    "                        </context>\n",
    "\n",
    "                        User Question:\n",
    "                        <user_question>\n",
    "                        {user_question}\n",
    "                        </user_question>\n",
    "\n",
    "                        Reference Answer:\n",
    "                        <reference_answer>\n",
    "                        {reference_answer}\n",
    "                        </reference_answer>\n",
    "\n",
    "                        System Answer:\n",
    "                        <system_answer>\n",
    "                        {system_answer}\n",
    "                        </system_answer>\n",
    "                        \"\"\".strip()\n",
    "                }\n",
    "            ]\n",
    "        else:\n",
    "            # No reference answer => use the default \"all-dimensions\" prompt\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_response_format_all\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "                        Context:\n",
    "                        <context>\n",
    "                        {context}\n",
    "                        </context>\n",
    "\n",
    "                        User Question:\n",
    "                        <user_question>\n",
    "                        {user_question}\n",
    "                        </user_question>\n",
    "\n",
    "                        System Answer:\n",
    "                        <system_answer>\n",
    "                        {system_answer}\n",
    "                        </system_answer>\n",
    "                        \"\"\".strip()\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        # Single call, enforcing the multi-dimension schema\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            response_format=EvaluationOutput_all\n",
    "        )\n",
    "        return completion.choices[0].message.parsed, messages\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2) EVALUATION STYLE: \"SEPARATE\"\n",
    "    # -------------------------------------------------------\n",
    "    elif evaluation_style == \"separate\":\n",
    "        dimension_prompts_and_schemas_no_ref = {\n",
    "            \"hallucination\": (\n",
    "                system_prompt_response_format_hallucination,\n",
    "                EvaluationOutput_hallucination\n",
    "            ),\n",
    "            \"answer_accuracy\": (\n",
    "                system_prompt_response_format_accuracy,\n",
    "                EvaluationOutput_accuracy\n",
    "            ),\n",
    "            \"user_satisfaction\": (\n",
    "                system_prompt_response_format_satisfaction,\n",
    "                EvaluationOutput_satisfaction\n",
    "            ),\n",
    "            \"coherence_clarity_fluency\": (\n",
    "                system_prompt_response_format_coherence,\n",
    "                EvaluationOutput_coherence\n",
    "            ),\n",
    "            \"context_quality\": (\n",
    "                system_prompt_response_format_context,\n",
    "                EvaluationOutput_context\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Reference versions\n",
    "        dimension_prompts_and_schemas_ref = {\n",
    "            \"hallucination\": (\n",
    "                system_prompt_response_format_hallucination_reference, \n",
    "                EvaluationOutput_hallucination\n",
    "            ),\n",
    "            \"answer_accuracy\": (\n",
    "                system_prompt_response_format_accuracy_reference,\n",
    "                EvaluationOutput_accuracy\n",
    "            ),\n",
    "            \"user_satisfaction\": (\n",
    "                system_prompt_response_format_satisfaction_reference,\n",
    "                EvaluationOutput_satisfaction\n",
    "            ),\n",
    "            \"coherence_clarity_fluency\": (\n",
    "                system_prompt_response_format_coherence_reference,\n",
    "                EvaluationOutput_coherence\n",
    "            ),\n",
    "            \"context_quality\": (\n",
    "                system_prompt_response_format_context_reference,\n",
    "                EvaluationOutput_context\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Decide which dictionary to use\n",
    "        if reference_answer:\n",
    "            dimension_prompts_and_schemas = dimension_prompts_and_schemas_ref\n",
    "        else:\n",
    "            dimension_prompts_and_schemas = dimension_prompts_and_schemas_no_ref\n",
    "\n",
    "        combined_dimensions = {}\n",
    "        all_messages = []\n",
    "\n",
    "        # Loop over each dimension\n",
    "        for dim_name, (dim_prompt, dim_schema) in dimension_prompts_and_schemas.items():\n",
    "            # Build the user content\n",
    "            user_content = f\"\"\"\n",
    "                    Context:\n",
    "                    <context>\n",
    "                    {context}\n",
    "                    </context>\n",
    "\n",
    "                    User Question:\n",
    "                    <user_question>\n",
    "                    {user_question}\n",
    "                    </user_question>\n",
    "                    \"\"\".strip()\n",
    "\n",
    "            # If we do have a reference, include it in the prompt\n",
    "            if reference_answer:\n",
    "                user_content += f\"\"\"\n",
    "                    Reference Answer:\n",
    "                    <reference_answer>\n",
    "                    {reference_answer}\n",
    "                    </reference_answer>\n",
    "                    \"\"\".strip()\n",
    "\n",
    "            # Always include the system answer\n",
    "            user_content += f\"\"\"\n",
    "                System Answer:\n",
    "                <system_answer>\n",
    "                {system_answer}\n",
    "                </system_answer>\n",
    "                \"\"\".strip()\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": dim_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "            #print(messages)\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                temperature=0.0,\n",
    "                response_format=dim_schema\n",
    "            )\n",
    "            parsed_output = completion.choices[0].message.parsed\n",
    "            all_messages.append(messages)\n",
    "\n",
    "            # E.g., parsed_output might be {\"hallucination\": {\"score\": ..., \"comment\": ...}}\n",
    "            dimension_score_obj = getattr(parsed_output, dim_name)\n",
    "            combined_dimensions[dim_name] = dimension_score_obj\n",
    "\n",
    "        # Combine into a single instance of EvaluationOutput_all\n",
    "        combined_parsed = EvaluationOutput_all(\n",
    "            hallucination=combined_dimensions[\"hallucination\"],\n",
    "            answer_accuracy=combined_dimensions[\"answer_accuracy\"],\n",
    "            user_satisfaction=combined_dimensions[\"user_satisfaction\"],\n",
    "            coherence_clarity_fluency=combined_dimensions[\"coherence_clarity_fluency\"],\n",
    "            context_quality=combined_dimensions[\"context_quality\"]\n",
    "        )\n",
    "\n",
    "        return combined_parsed, all_messages\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def compute_weighted_score(evaluation_dict):\n",
    "    \"\"\"\n",
    "    A toy weighting:\n",
    "    hallucination:              0.3\n",
    "    answer_accuracy:            0.3\n",
    "    user_satisfaction:          0.1\n",
    "    coherence_clarity_fluency:  0.1\n",
    "    context_quality:            0.2\n",
    "    \"\"\"\n",
    "    h = evaluation_dict[\"hallucination\"][\"score\"]\n",
    "    a = evaluation_dict[\"answer_accuracy\"][\"score\"]\n",
    "    s = evaluation_dict[\"user_satisfaction\"][\"score\"]\n",
    "    c_c_f = evaluation_dict[\"coherence_clarity_fluency\"][\"score\"]\n",
    "    c_qual  = evaluation_dict[\"context_quality\"][\"score\"]\n",
    "\n",
    "    weighted = (\n",
    "        0.3 * h +\n",
    "        0.3 * a +\n",
    "        0.1 * s +\n",
    "        0.1 * c_c_f +\n",
    "        0.2 * c_qual\n",
    "    )\n",
    "    return weighted\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def calculate_api_call_cost(input_text: str, output_text: str, encoding: tiktoken, model_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cost of an API call based on input and output texts.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text to be tokenized.\n",
    "        output_text (str): The output text to be tokenized.\n",
    "        model (str): The model name to determine the appropriate tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        float: The total cost of the API call.\n",
    "    \"\"\"\n",
    "    # Cast the input and output texts to strings\n",
    "    input_text = str(input_text)\n",
    "    output_text = str(output_text)\n",
    "    # Define the cost per million tokens\n",
    "    if model_name == \"gpt-4o-2024-08-06\":\n",
    "        input_cost_per_million = 2.50\n",
    "        output_cost_per_million = 10.00\n",
    "    elif model_name == \"gpt-4o-mini-2024-07-18\":\n",
    "        input_cost_per_million = 0.15\n",
    "        output_cost_per_million = 0.6\n",
    "    else:\n",
    "        return \"Not defined for this model\"\n",
    "\n",
    "    # Tokenize the input and output texts\n",
    "    input_tokens = encoding.encode(input_text)\n",
    "    output_tokens = encoding.encode(output_text)\n",
    "\n",
    "    # Calculate the number of tokens\n",
    "    num_input_tokens = len(input_tokens)\n",
    "    num_output_tokens = len(output_tokens)\n",
    "\n",
    "    if num_input_tokens > 128000 or num_output_tokens > 128000:\n",
    "        print(f\"Number of input tokens: {num_input_tokens}\", f\"Number of output tokens: {num_output_tokens} - Too many tokens for this model\")\n",
    "\n",
    "    # Calculate the cost for input and output tokens\n",
    "    input_cost = (num_input_tokens / 1_000_000) * input_cost_per_million\n",
    "    output_cost = (num_output_tokens / 1_000_000) * output_cost_per_million\n",
    "\n",
    "    # Total cost is the sum of input and output costs\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "\n",
    "def run_llm_judge_evaluation_structured(\n",
    "    df: pd.DataFrame,\n",
    "    context_col: str,\n",
    "    question_col: str,\n",
    "    system_answer_col: str,\n",
    "    question_id_col: str,\n",
    "    output_csv_path: str,\n",
    "    evaluation_style: str,\n",
    "    client: OpenAI,\n",
    "    reference_answer_col: str = None,\n",
    "    model: str = \"gpt-4o-2024-08-06\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates each row in `df` by calling the LLM (one time per row) using the provided\n",
    "    system_prompt. Saves results to `output_csv_path`.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the rows to evaluate.\n",
    "        context_col (str): The column name in `df` that holds the 'context'.\n",
    "        question_col (str): The column name for the user question.\n",
    "        system_answer_col (str): The column name for the system's answer.\n",
    "        question_id_col (str): The column name for a unique question or row ID.\n",
    "        output_csv_path (str): Where to save the final evaluations as CSV.\n",
    "        evaluation_style (str): The evaluation style to use. One of 'together' or 'separate'.\n",
    "        client (OpenAI): The OpenAI client instance.\n",
    "        model (str): The model name to use for the evaluation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the evaluation scores & comment, \n",
    "                      plus the weighted overall score.\n",
    "    \"\"\"\n",
    "    # Initialize the tokenizer for the specified model\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Model '{model}' not found. Please provide a valid model name.\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        context = row[context_col]\n",
    "        user_question = row[question_col]\n",
    "        system_answer = row[system_answer_col]\n",
    "        question_id = row[question_id_col]\n",
    "\n",
    "        if reference_answer_col:\n",
    "            reference_answer = row[reference_answer_col]\n",
    "        else:\n",
    "            reference_answer = None\n",
    "\n",
    "        # Call the API with structured outputs\n",
    "        eval_output, input_message = evaluate_with_llm_as_judge_structured(\n",
    "            context=context,\n",
    "            user_question=user_question,\n",
    "            system_answer=system_answer,\n",
    "            reference_answer=reference_answer,\n",
    "            client=client,\n",
    "            model_name=model,\n",
    "            evaluation_style=evaluation_style\n",
    "        )\n",
    "        # Flatten the output into score and comment\n",
    "        eval_dict = eval_output.dict()  # Convert the Pydantic model to a dictionary\n",
    "        flattened_results = {}\n",
    "\n",
    "        for dim, value in eval_dict.items():\n",
    "            # Ensure nested dictionaries for each dimension are properly handled\n",
    "            if isinstance(value, dict):  # Check if the value is a dictionary\n",
    "                flattened_results[f\"{dim}_score\"] = value.get(\"score\")\n",
    "                flattened_results[f\"{dim}_comment\"] = value.get(\"comment\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected value type for dimension {dim}: {type(value)}\")\n",
    "\n",
    "        # Compute weighted score\n",
    "        weighted_overall = compute_weighted_score(eval_dict)\n",
    "\n",
    "        # Calculate API cost\n",
    "        total_cost = calculate_api_call_cost(input_message, eval_output, encoding, model)\n",
    "\n",
    "        # Append results\n",
    "        all_results.append({\n",
    "            question_id_col: question_id,\n",
    "            **flattened_results,\n",
    "            \"weighted_overall_score\": weighted_overall,\n",
    "            \"api_call_cost\": total_cost\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame and save\n",
    "    df_eval = pd.DataFrame(all_results)\n",
    "    df_eval.to_csv(output_csv_path, index=False, quoting=1)\n",
    "    print(f\"Saved LLM judge evaluation to: {output_csv_path}\")\n",
    "\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM judge evaluation to: ../../data/eval/evaluation_llm_judge_chatbot_en.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id_q</th>\n",
       "      <th>hallucination_score</th>\n",
       "      <th>hallucination_comment</th>\n",
       "      <th>answer_accuracy_score</th>\n",
       "      <th>answer_accuracy_comment</th>\n",
       "      <th>user_satisfaction_score</th>\n",
       "      <th>user_satisfaction_comment</th>\n",
       "      <th>coherence_clarity_fluency_score</th>\n",
       "      <th>coherence_clarity_fluency_comment</th>\n",
       "      <th>context_quality_score</th>\n",
       "      <th>context_quality_comment</th>\n",
       "      <th>weighted_overall_score</th>\n",
       "      <th>api_call_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>153</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer accurately lists various sch...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer accurately and comprehensive...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer provides a comprehensive and...</td>\n",
       "      <td>4</td>\n",
       "      <td>The answer is well-structured, clearly present...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer provides a comprehensive and...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>The system answer includes a Master's program ...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer accurately identifies the re...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer effectively addresses the us...</td>\n",
       "      <td>4</td>\n",
       "      <td>The answer is well-structured, clearly outline...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer provides relevant and detail...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.003961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id_q  hallucination_score  \\\n",
       "0            153                    4   \n",
       "1              9                    1   \n",
       "\n",
       "                               hallucination_comment  answer_accuracy_score  \\\n",
       "0  The system answer accurately lists various sch...                      4   \n",
       "1  The system answer includes a Master's program ...                      4   \n",
       "\n",
       "                             answer_accuracy_comment  user_satisfaction_score  \\\n",
       "0  The system answer accurately and comprehensive...                        4   \n",
       "1  The system answer accurately identifies the re...                        4   \n",
       "\n",
       "                           user_satisfaction_comment  \\\n",
       "0  The system answer provides a comprehensive and...   \n",
       "1  The system answer effectively addresses the us...   \n",
       "\n",
       "   coherence_clarity_fluency_score  \\\n",
       "0                                4   \n",
       "1                                4   \n",
       "\n",
       "                   coherence_clarity_fluency_comment  context_quality_score  \\\n",
       "0  The answer is well-structured, clearly present...                      4   \n",
       "1  The answer is well-structured, clearly outline...                      4   \n",
       "\n",
       "                             context_quality_comment  weighted_overall_score  \\\n",
       "0  The system answer provides a comprehensive and...                     4.0   \n",
       "1  The system answer provides relevant and detail...                     3.1   \n",
       "\n",
       "   api_call_cost  \n",
       "0       0.002793  \n",
       "1       0.003961  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# Usage Example\n",
    "# ============================\n",
    "# Example: Call the function with structured outputs\n",
    "# Load the environment variables\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "# model_name = \"gpt-4o-2024-08-06\"\n",
    "model_name = \"gpt-4o-mini-2024-07-18\"\n",
    "# evaluation_style = \"together\"\n",
    "evaluation_style = \"separate\"\n",
    "\n",
    "# Load your dataset\n",
    "df_en = pd.read_csv(\"../../data/final_merged_dataset_short_en_2.csv\")\n",
    "df_en = df_en.head(2).copy()  # For testing\n",
    "df_eval_en = run_llm_judge_evaluation_structured(\n",
    "    df=df_en,\n",
    "    context_col=\"chatbot_context_en\",\n",
    "    question_col=\"english_question_text_q\",\n",
    "    system_answer_col=\"chatbot_answer_en\",\n",
    "    question_id_col=\"question_id_q\",\n",
    "    output_csv_path=\"../../data/eval/evaluation_llm_judge_chatbot_en.csv\",\n",
    "    evaluation_style=evaluation_style,\n",
    "    client=client,\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "\n",
    "# # Load your dataset\n",
    "# df_de = pd.read_csv(\"../../data/final_merged_dataset_short_de_2.csv\")\n",
    "# df_de = df_de.head(2).copy()  # For testing\n",
    "# df_eval_de = run_llm_judge_evaluation_structured(\n",
    "#     df=df_de,\n",
    "#     context_col=\"chatbot_context_de\",\n",
    "#     question_col=\"german_question_text_q\",\n",
    "#     system_answer_col=\"chatbot_answer_de\",\n",
    "#     #reference_answer_col=\"human_answer_de\",\n",
    "#     question_id_col=\"question_id_q\",\n",
    "#     output_csv_path=\"../../data/eval/evaluation_llm_judge_chatbot_de.csv\",\n",
    "#     evaluation_style=evaluation_style,\n",
    "#     client=client,\n",
    "#     model=model_name,\n",
    "# )\n",
    "# calculate the mean of the evaluation\n",
    "df_mean = pd.read_csv(\"../../data/eval/mean_eval.csv\")\n",
    "# English\n",
    "# mean_llm_en = df_eval_en[\"weighted_overall_score\"].mean()\n",
    "# df_mean[\"llm_as_judge_en\"] = mean_llm_en\n",
    "# German\n",
    "# mean_llm_de = df_eval_de[\"weighted_overall_score\"].mean()\n",
    "# df_mean[\"llm_as_judge_de\"] = mean_llm_de\n",
    "\n",
    "\n",
    "# df_mean.to_csv(\"../../data/eval/mean_eval.csv\", index=False)\n",
    "df_eval_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system answer includes a Master's program in Cognitive Computing that is not mentioned in the provided context, indicating a hallucination.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0067536"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_eval_en['hallucination_comment'][1])\n",
    "df_eval_en[\"api_call_cost\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer accurately reflects the context without introducing any false information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0223275"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_eval_de['hallucination_comment'][1])\n",
    "df_eval_de[\"api_call_cost\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOund Links: ['https://www.uni-osnabrueck.de/studieninteressierte/stipendien-und-foerderung/;https://www.uni-osnabrueck.de/studieninteressierte/studieninteressierte-aus-dem-ausland/kosten-und-finanzierung/']\n",
      "FOund Links: ['https://www.ikw.uni-osnabrueck.de/studieninteressierte/willkommen.html;https://www.uni-osnabrueck.de/studieninteressierte/studiengaenge-a-z/cognitive-science-master-of-science/']\n",
      "Extracted HTML text from: https://www.ikw.uni-osnabrueck.de/studieninteressierte/willkommen.html;https://www.uni-osnabrueck.de/studieninteressierte/studiengaenge-a-z/cognitive-science-master-of-science/\n",
      "FOund Links: ['https://www.wiwi.uni-osnabrueck.de/fileadmin/documents/public/1_fachbereich/00_Allgemein/1.04_jahresberichte/Jahresbericht_FB9_2012-2013.pdf;https://www.uni-osnabrueck.de/fileadmin/documents/public/1_universitaet/1.3_organisation/d4_akad._angelegenheiten/d4_akad._angelegenheiten/mitteilungsblaetter/2016-2020/2019-05-09_Nr03.pdf']\n"
     ]
    }
   ],
   "source": [
    "# code form chatbot to extract text from pdf and html\n",
    "import requests\n",
    "import re\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_html_text(href: str, response) -> str:\n",
    "\n",
    "    link_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    div_content = link_soup.find(\"div\", class_=\"eb2\")\n",
    "    if div_content:\n",
    "        text = re.sub(r\"\\n+\", \"\\n\", div_content.text.strip())\n",
    "        content_with_link = \"\"\n",
    "        for link in div_content.find_all(\"a\", href=True):\n",
    "            text_anchor_tag = re.sub(r\"\\n+\", \"\\n\", link.text.strip())\n",
    "            content_with_link += f\" - {text_anchor_tag}: {link['href']}\"\n",
    "        return text + \"\\nHref found in the text:\\n\" + content_with_link\n",
    "    print(f\"Failed to fetch html content from: {href}\")\n",
    "    return \"\"\n",
    "def read_pdf_from_url(pdf_bytes: bytes, num_pages: int = 7) -> str:\n",
    "    \"\"\"\n",
    "    Read the content of a PDF file from a given byte stream.\n",
    "\n",
    "    Args:\n",
    "        pdf_bytes (bytes): Raw bytes of the PDF content.\n",
    "        num_pages (int, optional): Number of pages to process. If None, process all pages. Defaults to 7.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text content from the PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_stream = io.BytesIO(pdf_bytes)\n",
    "\n",
    "    pdf_text = \"\"\n",
    "    with pdf_stream as f:\n",
    "        reader = PdfReader(f)\n",
    "        if num_pages is None:\n",
    "            num_pages = len(reader.pages)\n",
    "        else:\n",
    "            num_pages = min(num_pages, len(reader.pages))\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            pdf_text += page.extract_text() or \"\"\n",
    "\n",
    "    return pdf_text\n",
    "\n",
    "def extract_pdf_text(href: str, response) -> str:\n",
    "\n",
    "    text = read_pdf_from_url(response)\n",
    "    return re.sub(r\"(\\n\\s*|\\n\\s+\\n\\s+)\", \"\\n\", text.strip())\n",
    "\n",
    "def context_from_links(links):\n",
    "    contents = []\n",
    "    taken_from = \"Information taken from:\"\n",
    "    search_result_text = \"Content not found\"\n",
    "    for tag in links:\n",
    "        href = tag\n",
    "        try:\n",
    "            # TODO I/O operation (use async code)\n",
    "            response = requests.get(href)\n",
    "            #print(f\"Fetching: {href}\")\n",
    "        except:\n",
    "            print(f\"Error while fetching: {href}\")\n",
    "            continue\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            if href.endswith(\".pdf\"):\n",
    "                print(f\"Extracted PDF text from: {href}\")\n",
    "                text = extract_pdf_text(href, response)\n",
    "            else:\n",
    "                print(f\"Extracted HTML text from: {href}\")\n",
    "                text = extract_html_text(href, response)\n",
    "\n",
    "            if text:\n",
    "                text = f\"{taken_from}{href}\\n{text}\"\n",
    "                contents.append(text)\n",
    "\n",
    "    return \"\\n\".join(contents) if contents else search_result_text, links\n",
    "\n",
    "# code from parsing links and getting context\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_links_from_string(links_str: str):\n",
    "    \"\"\"\n",
    "    Extracts all URLs (including Markdown-style) from a string.\n",
    "    Returns a list of raw link URLs.\n",
    "    \"\"\"\n",
    "    if not isinstance(links_str, str) or not links_str.strip():\n",
    "        return []\n",
    "\n",
    "    # Regex approach:\n",
    "    # 1) Find Markdown links: [some text](http://some-url.com)\n",
    "    # 2) Also find bare URLs like http://... or https://...\n",
    "    pattern = r'\\((https?://[^\\)]+)\\)|(https?://[^\\s]+)'\n",
    "    # Explanation:\n",
    "    #   \\((https?://[^\\)]+)\\) matches markdown ( capturing URL inside parentheses )\n",
    "    #   (https?://[^\\s]+) captures bare URLs\n",
    "    matches = re.findall(pattern, links_str)\n",
    "\n",
    "    # matches is a list of tuples, e.g. [(\"https://...\",\"\"), (\"\",\"https://...\")]\n",
    "    # We gather the non-empty group from each tuple\n",
    "    urls = []\n",
    "    for md_link, normal_link in matches:\n",
    "        if md_link:\n",
    "            urls.append(md_link)\n",
    "        elif normal_link:\n",
    "            urls.append(normal_link)\n",
    "    print(f\"FOund Links: {urls}\")\n",
    "    return urls\n",
    "\n",
    "def gather_context_from_links(links_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Parses the given links_str (which may be plain or Markdown),\n",
    "    fetches each link, extracts text, and concatenates them into\n",
    "    a single context string.\n",
    "    \"\"\"\n",
    "    links = parse_links_from_string(links_str)\n",
    "    if not links:\n",
    "        return \"No links or no content found.\"\n",
    "\n",
    "    all_contents = context_from_links(links)\n",
    "\n",
    "    \n",
    "    return all_contents\n",
    "\n",
    "def add_content_to_df(df, links_col, context_col):\n",
    "    \"\"\"\n",
    "    Adds context to the DataFrame by fetching and parsing the content\n",
    "    from the URLs in the specified column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to update.\n",
    "        links_col (str): The column name containing the URLs.\n",
    "        context_col (str): The column name to store the extracted content.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame.\n",
    "    \"\"\"\n",
    "    context_rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        links_str = row[links_col]\n",
    "        context = gather_context_from_links(links_str)\n",
    "        context_rows.append(context)\n",
    "\n",
    "    df[context_col] = context_rows\n",
    "    return df\n",
    "\n",
    "df_en = pd.read_csv(\"../../data/final_merged_dataset_short_en.csv\")\n",
    "df_en = df_en.head(3).copy()\n",
    "df_en = add_content_to_df(df_en, \"chatbot_visited_urls_en\", \"chatbot_context\")\n",
    "\n",
    "df_en.head()\n",
    "\n",
    "df_en.to_csv(\"../../data/final_merged_dataset_short_en_testing.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
