{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "system_prompt_response_format_all = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across multiple dimensions. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimensions using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "(0 = severe hallucination; multiple claims are incorrect, 4 = no hallucination; all claims are verifiable and correct).\n",
    "-Answer Accuracy: The degree to which an answer accurately addresses the user's question by providing correct, complete, and relevant information that matches the intent of the question. Factual accuracy (no hallucination) is necessary but not sufficient; the answer must also be accurate, comprehensive, and appropriate to the purpose of the question.\n",
    "(0 = inaccurate; fails to answer the question, 4 = fully accurate; directly addresses the question comprehensively).\n",
    "-User Satisfaction: Reflects the user's subjective assessment of the answer's quality, focusing on the effectiveness of understanding the question, answering it, providing meaningful value, and leaving an overall positive impression.\n",
    "(0 = very unsatisfactory; the answer is unhelpful or confusing, 4 = highly satisfactory; the answer provides significant value).\n",
    "-Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "(0 = incoherent or unclear; hard to follow, 4 = highly coherent and clear; easy to read and well-structured).\n",
    "-Context Quality: Assesses the relevance and completeness of the context in supporting the answer. High-quality context is directly related to the user’s question and provides all necessary details for a correct and comprehensive response. If no context is provided, evaluate how its absence affects the quality of the answer.\n",
    "Context is provided:\n",
    "Evaluate how well the provided context (links) aligns with the question and how effectively it supports the answer.\n",
    "(0 = Context is irrelevant or insufficient; does not support the answer, 4 = Context is fully relevant and highly effective; perfectly supports the answer).\n",
    "Context is not provided:\n",
    "Evaluate how the absence of context impacts the quality of the answer.\n",
    "(0 = Severely impacts quality; context would have been essential, 4 = No impact on quality; context would haven been unnecessary).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "For each dimension, follow these steps to ensure thorough and consistent evaluations:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on the evaluation dimensions.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Evaluate Independently: Look at each evaluation dimension independently and do not let one dimension influence the next.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "system_prompt_response_format_all_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across multiple dimensions. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimensions using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "(0 = severe hallucination; multiple claims are incorrect, 4 = no hallucination; all claims are verifiable and correct).\n",
    "-Answer Accuracy: The degree to which an answer accurately addresses the user's question by providing correct, complete, and relevant information that matches the intent of the question. Factual accuracy (no hallucination) is necessary but not sufficient; the answer must also be accurate, comprehensive, and appropriate to the purpose of the question.\n",
    "(0 = inaccurate; fails to answer the question, 4 = fully accurate; directly addresses the question comprehensively).\n",
    "-User Satisfaction: Reflects the user's subjective assessment of the answer's quality, focusing on the effectiveness of understanding the question, answering it, providing meaningful value, and leaving an overall positive impression.\n",
    "(0 = very unsatisfactory; the answer is unhelpful or confusing, 4 = highly satisfactory; the answer provides significant value).\n",
    "-Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "(0 = incoherent or unclear; hard to follow, 4 = highly coherent and clear; easy to read and well-structured).\n",
    "-Context Quality: Assesses the relevance and completeness of the context in supporting the answer. High-quality context is directly related to the user’s question and provides all necessary details for a correct and comprehensive response. If no context is provided, evaluate how its absence affects the quality of the answer.\n",
    "Context is provided:\n",
    "Evaluate how well the provided context (links) aligns with the question and how effectively it supports the answer.\n",
    "(0 = Context is irrelevant or insufficient; does not support the answer, 4 = Context is fully relevant and highly effective; perfectly supports the answer).\n",
    "Context is not provided:\n",
    "Evaluate how the absence of context impacts the quality of the answer.\n",
    "(0 = Severely impacts quality; context would have been essential, 4 = No impact on quality; context would haven been unnecessary).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "For each dimension, follow these steps to ensure thorough and consistent evaluations:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on the evaluation dimensions.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Evaluate Independently: Look at each evaluation dimension independently and do not let one dimension influence the next.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_hallucination = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Hallucination'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "(0 = severe hallucination; multiple claims are incorrect, 4 = no hallucination; all claims are verifiable and correct).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Hallucination:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Hallucination.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "system_prompt_response_format_hallucination_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Hallucination'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "(0 = severe hallucination; multiple claims are incorrect, 4 = no hallucination; all claims are verifiable and correct).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Hallucination:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Hallucination.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_accuracy = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Answer Accuracy'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Answer Accuracy: The degree to which an answer accurately addresses the user's question by providing correct, complete, and relevant information that matches the intent of the question. Factual accuracy (no hallucination) is necessary but not sufficient; the answer must also be accurate, comprehensive, and appropriate to the purpose of the question.\n",
    "(0 = inaccurate; fails to answer the question, 4 = fully accurate; directly addresses the question comprehensively).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Answer Accuracy:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Answer Accuracy.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "system_prompt_response_format_accuracy_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Answer Accuracy'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Answer Accuracy: The degree to which an answer accurately addresses the user's question by providing correct, complete, and relevant information that matches the intent of the question. Factual accuracy (no hallucination) is necessary but not sufficient; the answer must also be accurate, comprehensive, and appropriate to the purpose of the question.\n",
    "(0 = inaccurate; fails to answer the question, 4 = fully accurate; directly addresses the question comprehensively).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Answer Accuracy:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Answer Accuracy.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_satisfaction = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'User Satisfaction'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-User Satisfaction: Reflects the user's subjective assessment of the answer's quality, focusing on the effectiveness of understanding the question, answering it, providing meaningful value, and leaving an overall positive impression.\n",
    "(0 = very unsatisfactory; the answer is unhelpful or confusing, 4 = highly satisfactory; the answer provides significant value).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of User Satisfaction:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on User Satisfaction.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "system_prompt_response_format_satisfaction_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'User Satisfaction'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-User Satisfaction: Reflects the user's subjective assessment of the answer's quality, focusing on the effectiveness of understanding the question, answering it, providing meaningful value, and leaving an overall positive impression.\n",
    "(0 = very unsatisfactory; the answer is unhelpful or confusing, 4 = highly satisfactory; the answer provides significant value).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of User Satisfaction:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on User Satisfaction.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_coherence = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Coherence, Clarity, and Fluency'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "(0 = incoherent or unclear; hard to follow, 4 = highly coherent and clear; easy to read and well-structured).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Coherence, Clarity, and Fluency:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Coherence, Clarity, and Fluency.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "system_prompt_response_format_coherence_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Coherence, Clarity, and Fluency'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "(0 = incoherent or unclear; hard to follow, 4 = highly coherent and clear; easy to read and well-structured).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Coherence, Clarity, and Fluency:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Coherence, Clarity, and Fluency.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_context = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Context Quality'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Context Quality: Assesses the relevance and completeness of the context in supporting the answer. High-quality context is directly related to the user’s question and provides all necessary details for a correct and comprehensive response. If no context is provided, evaluate how its absence affects the quality of the answer.\n",
    "Context is provided:\n",
    "Evaluate how well the provided context (links) aligns with the question and how effectively it supports the answer.\n",
    "(0 = Context is irrelevant or insufficient; does not support the answer, 4 = Context is fully relevant and highly effective; perfectly supports the answer).\n",
    "Context is not provided:\n",
    "Evaluate how the absence of context impacts the quality of the answer.\n",
    "(0 = Severely impacts quality; context would have been essential, 4 = No impact on quality; context would haven been unnecessary).\n",
    "\n",
    "3. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Context Quality:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Context Quality.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "4. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n",
    "system_prompt_response_format_context_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across the dimension 'Context Quality'. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The chatbot to be evaluated has the task of answering questions about Osnabrück University. If a question is not related to the university, the chatbot is instructed to politely decline.\n",
    "The chatbot's answers were generated on January 9, 2025.\n",
    "Please take this into account when evaluating the answers.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Rate the answer on the following dimension using a scale of 0 to 4 (0 = Very Bad, 1 = Bad, 2 = Neutral, 3 = Good, 4 = Very Good):\n",
    "-Context Quality: Assesses the relevance and completeness of the context in supporting the answer. High-quality context is directly related to the user’s question and provides all necessary details for a correct and comprehensive response. If no context is provided, evaluate how its absence affects the quality of the answer.\n",
    "Context is provided:\n",
    "Evaluate how well the provided context (links) aligns with the question and how effectively it supports the answer.\n",
    "(0 = Context is irrelevant or insufficient; does not support the answer, 4 = Context is fully relevant and highly effective; perfectly supports the answer).\n",
    "Context is not provided:\n",
    "Evaluate how the absence of context impacts the quality of the answer.\n",
    "(0 = Severely impacts quality; context would have been essential, 4 = No impact on quality; context would haven been unnecessary).\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "Follow these steps to ensure thorough and consistent evaluation of Context Quality:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on Context Quality.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Take your time: Carefully read the user’s question and any provided context before evaluating.\n",
    "- Be Objective: Base your evaluations strictly on the provided content and criteria.\n",
    "- Handle Ambiguities Thoughtfully: If a question is unclear, evaluate based on the simplest and most logical interpretation.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure a consistent and high-quality evaluation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the schema using pydantic\n",
    "class DimensionScore(BaseModel):\n",
    "    score: int  # 0-4\n",
    "    comment: str\n",
    "\n",
    "class EvaluationOutput_all(BaseModel):\n",
    "    hallucination: DimensionScore\n",
    "    answer_accuracy: DimensionScore\n",
    "    user_satisfaction: DimensionScore\n",
    "    coherence_clarity_fluency: DimensionScore\n",
    "    context_quality: DimensionScore\n",
    "\n",
    "class EvaluationOutput_hallucination(BaseModel):\n",
    "    hallucination: DimensionScore\n",
    "\n",
    "class EvaluationOutput_accuracy(BaseModel):\n",
    "    answer_accuracy: DimensionScore\n",
    "\n",
    "class EvaluationOutput_satisfaction(BaseModel):\n",
    "    user_satisfaction: DimensionScore\n",
    "\n",
    "class EvaluationOutput_coherence(BaseModel):\n",
    "    coherence_clarity_fluency: DimensionScore\n",
    "\n",
    "class EvaluationOutput_context(BaseModel):\n",
    "    context_quality: DimensionScore\n",
    "    \n",
    "def evaluate_with_llm_as_judge_structured(\n",
    "    context: str,\n",
    "    user_question: str,\n",
    "    system_answer: str,\n",
    "    reference_answer: str,\n",
    "    client: OpenAI,\n",
    "    model_name: str,\n",
    "    evaluation_style: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the OpenAI API with structured outputs, enforcing the defined schema.\n",
    "    \n",
    "    If evaluation_style == 'together', do a single call with the multi-dimension\n",
    "    prompt and schema (with or without a reference).\n",
    "    \n",
    "    If evaluation_style == 'separate', do five separate calls (one per dimension);\n",
    "    if reference answers exist for each dimension, use the reference prompt, else\n",
    "    use the default dimension prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1) EVALUATION STYLE: \"TOGETHER\"\n",
    "    # -------------------------------------------------------\n",
    "    if evaluation_style == \"together\":\n",
    "        # If a reference answer is provided and not empty\n",
    "        if reference_answer:\n",
    "            # Use the \"all dimensions with reference\" prompt\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_response_format_all_reference\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "                        Context:\n",
    "                        <context>\n",
    "                        {context}\n",
    "                        </context>\n",
    "\n",
    "                        User Question:\n",
    "                        <user_question>\n",
    "                        {user_question}\n",
    "                        </user_question>\n",
    "\n",
    "                        Reference Answer:\n",
    "                        <reference_answer>\n",
    "                        {reference_answer}\n",
    "                        </reference_answer>\n",
    "\n",
    "                        System Answer:\n",
    "                        <system_answer>\n",
    "                        {system_answer}\n",
    "                        </system_answer>\n",
    "                        \"\"\".strip()\n",
    "                }\n",
    "            ]\n",
    "        else:\n",
    "            # No reference answer => use the default \"all-dimensions\" prompt\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_response_format_all\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "                        Context:\n",
    "                        <context>\n",
    "                        {context}\n",
    "                        </context>\n",
    "\n",
    "                        User Question:\n",
    "                        <user_question>\n",
    "                        {user_question}\n",
    "                        </user_question>\n",
    "\n",
    "                        System Answer:\n",
    "                        <system_answer>\n",
    "                        {system_answer}\n",
    "                        </system_answer>\n",
    "                        \"\"\".strip()\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        # Single call, enforcing the multi-dimension schema\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            response_format=EvaluationOutput_all\n",
    "        )\n",
    "        return completion.choices[0].message.parsed, messages\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2) EVALUATION STYLE: \"SEPARATE\"\n",
    "    # -------------------------------------------------------\n",
    "    elif evaluation_style == \"separate\":\n",
    "        dimension_prompts_and_schemas_no_ref = {\n",
    "            \"hallucination\": (\n",
    "                system_prompt_response_format_hallucination,\n",
    "                EvaluationOutput_hallucination\n",
    "            ),\n",
    "            \"answer_accuracy\": (\n",
    "                system_prompt_response_format_accuracy,\n",
    "                EvaluationOutput_accuracy\n",
    "            ),\n",
    "            \"user_satisfaction\": (\n",
    "                system_prompt_response_format_satisfaction,\n",
    "                EvaluationOutput_satisfaction\n",
    "            ),\n",
    "            \"coherence_clarity_fluency\": (\n",
    "                system_prompt_response_format_coherence,\n",
    "                EvaluationOutput_coherence\n",
    "            ),\n",
    "            \"context_quality\": (\n",
    "                system_prompt_response_format_context,\n",
    "                EvaluationOutput_context\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Reference versions\n",
    "        dimension_prompts_and_schemas_ref = {\n",
    "            \"hallucination\": (\n",
    "                system_prompt_response_format_hallucination_reference, \n",
    "                EvaluationOutput_hallucination\n",
    "            ),\n",
    "            \"answer_accuracy\": (\n",
    "                system_prompt_response_format_accuracy_reference,\n",
    "                EvaluationOutput_accuracy\n",
    "            ),\n",
    "            \"user_satisfaction\": (\n",
    "                system_prompt_response_format_satisfaction_reference,\n",
    "                EvaluationOutput_satisfaction\n",
    "            ),\n",
    "            \"coherence_clarity_fluency\": (\n",
    "                system_prompt_response_format_coherence_reference,\n",
    "                EvaluationOutput_coherence\n",
    "            ),\n",
    "            \"context_quality\": (\n",
    "                system_prompt_response_format_context_reference,\n",
    "                EvaluationOutput_context\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Decide which dictionary to use\n",
    "        if reference_answer:\n",
    "            dimension_prompts_and_schemas = dimension_prompts_and_schemas_ref\n",
    "        else:\n",
    "            dimension_prompts_and_schemas = dimension_prompts_and_schemas_no_ref\n",
    "\n",
    "        combined_dimensions = {}\n",
    "        all_messages = []\n",
    "\n",
    "        # Loop over each dimension\n",
    "        for dim_name, (dim_prompt, dim_schema) in dimension_prompts_and_schemas.items():\n",
    "            # Build the user content\n",
    "            user_content = f\"\"\"\n",
    "                    Context:\n",
    "                    <context>\n",
    "                    {context}\n",
    "                    </context>\n",
    "\n",
    "                    User Question:\n",
    "                    <user_question>\n",
    "                    {user_question}\n",
    "                    </user_question>\n",
    "                    \"\"\".strip()\n",
    "\n",
    "            # If we do have a reference, include it in the prompt\n",
    "            if reference_answer:\n",
    "                user_content += f\"\"\"\n",
    "                    Reference Answer:\n",
    "                    <reference_answer>\n",
    "                    {reference_answer}\n",
    "                    </reference_answer>\n",
    "                    \"\"\".strip()\n",
    "\n",
    "            # Always include the system answer\n",
    "            user_content += f\"\"\"\n",
    "                System Answer:\n",
    "                <system_answer>\n",
    "                {system_answer}\n",
    "                </system_answer>\n",
    "                \"\"\".strip()\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": dim_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "            #print(messages)\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                temperature=0.0,\n",
    "                response_format=dim_schema\n",
    "            )\n",
    "            parsed_output = completion.choices[0].message.parsed\n",
    "            all_messages.append(messages)\n",
    "\n",
    "            # E.g., parsed_output might be {\"hallucination\": {\"score\": ..., \"comment\": ...}}\n",
    "            dimension_score_obj = getattr(parsed_output, dim_name)\n",
    "            combined_dimensions[dim_name] = dimension_score_obj\n",
    "\n",
    "        # Combine into a single instance of EvaluationOutput_all\n",
    "        combined_parsed = EvaluationOutput_all(\n",
    "            hallucination=combined_dimensions[\"hallucination\"],\n",
    "            answer_accuracy=combined_dimensions[\"answer_accuracy\"],\n",
    "            user_satisfaction=combined_dimensions[\"user_satisfaction\"],\n",
    "            coherence_clarity_fluency=combined_dimensions[\"coherence_clarity_fluency\"],\n",
    "            context_quality=combined_dimensions[\"context_quality\"]\n",
    "        )\n",
    "\n",
    "        return combined_parsed, all_messages\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def compute_overall_score(evaluation_dict):\n",
    "    \"\"\"\n",
    "    could be modified to include different weights for each dimension\n",
    "    hallucination:              0.2\n",
    "    answer_accuracy:            0.2\n",
    "    user_satisfaction:          0.2\n",
    "    coherence_clarity_fluency:  0.2\n",
    "    context_quality:            0.2\n",
    "    \"\"\"\n",
    "    h = evaluation_dict[\"hallucination\"][\"score\"]\n",
    "    a = evaluation_dict[\"answer_accuracy\"][\"score\"]\n",
    "    s = evaluation_dict[\"user_satisfaction\"][\"score\"]\n",
    "    c_c_f = evaluation_dict[\"coherence_clarity_fluency\"][\"score\"]\n",
    "    c_qual  = evaluation_dict[\"context_quality\"][\"score\"]\n",
    "\n",
    "    weighted = (\n",
    "        0.2 * h +\n",
    "        0.2 * a +\n",
    "        0.2 * s +\n",
    "        0.2 * c_c_f +\n",
    "        0.2 * c_qual\n",
    "    )\n",
    "    return weighted\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def calculate_api_call_cost(input_text: str, output_text: str, encoding: tiktoken, model_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cost of an API call based on input and output texts.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text to be tokenized.\n",
    "        output_text (str): The output text to be tokenized.\n",
    "        model (str): The model name to determine the appropriate tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        float: The total cost of the API call.\n",
    "    \"\"\"\n",
    "    # Cast the input and output texts to strings\n",
    "    input_text = str(input_text)\n",
    "    output_text = str(output_text)\n",
    "    # Define the cost per million tokens\n",
    "    if model_name == \"gpt-4o-2024-08-06\":\n",
    "        input_cost_per_million = 2.50\n",
    "        output_cost_per_million = 10.00\n",
    "    elif model_name == \"gpt-4o-mini-2024-07-18\":\n",
    "        input_cost_per_million = 0.15\n",
    "        output_cost_per_million = 0.6\n",
    "    else:\n",
    "        return \"Not defined for this model\"\n",
    "\n",
    "    # Tokenize the input and output texts\n",
    "    input_tokens = encoding.encode(input_text)\n",
    "    output_tokens = encoding.encode(output_text)\n",
    "\n",
    "    # Calculate the number of tokens\n",
    "    num_input_tokens = len(input_tokens)\n",
    "    num_output_tokens = len(output_tokens)\n",
    "\n",
    "    # Calculate the cost for input and output tokens\n",
    "    input_cost = (num_input_tokens / 1_000_000) * input_cost_per_million\n",
    "    output_cost = (num_output_tokens / 1_000_000) * output_cost_per_million\n",
    "\n",
    "    # Total cost is the sum of input and output costs\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def run_llm_judge_evaluation_structured(\n",
    "    df: pd.DataFrame,\n",
    "    context_col: str,\n",
    "    question_col: str,\n",
    "    system_answer_col: str,\n",
    "    question_id_col: str,\n",
    "    output_csv_path: str,\n",
    "    evaluation_style: str,\n",
    "    client: OpenAI,\n",
    "    reference_answer_col: str = None,\n",
    "    model: str = \"gpt-4o-2024-08-06\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates each row in `df` by calling the LLM (one time per row) using the provided\n",
    "    system_prompt. Saves results to `output_csv_path`.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the rows to evaluate.\n",
    "        context_col (str): The column name in `df` that holds the 'context'.\n",
    "        question_col (str): The column name for the user question.\n",
    "        system_answer_col (str): The column name for the system's answer.\n",
    "        question_id_col (str): The column name for a unique question or row ID.\n",
    "        output_csv_path (str): Where to save the final evaluations as CSV.\n",
    "        evaluation_style (str): The evaluation style to use. One of 'together' or 'separate'.\n",
    "        client (OpenAI): The OpenAI client instance.\n",
    "        model (str): The model name to use for the evaluation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the evaluation scores & comment, \n",
    "                      plus the weighted overall score.\n",
    "    \"\"\"\n",
    "    # Initialize the tokenizer for the specified model\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Model '{model}' not found. Please provide a valid model name.\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating rows\"):\n",
    "        context = row[context_col]\n",
    "        user_question = row[question_col]\n",
    "        system_answer = row[system_answer_col]\n",
    "        question_id = row[question_id_col]\n",
    "\n",
    "        if reference_answer_col:\n",
    "            reference_answer = row[reference_answer_col]\n",
    "        else:\n",
    "            reference_answer = None\n",
    "\n",
    "        # Call the API with structured outputs\n",
    "        eval_output, input_message = evaluate_with_llm_as_judge_structured(\n",
    "            context=context,\n",
    "            user_question=user_question,\n",
    "            system_answer=system_answer,\n",
    "            reference_answer=reference_answer,\n",
    "            client=client,\n",
    "            model_name=model,\n",
    "            evaluation_style=evaluation_style\n",
    "        )\n",
    "        # Flatten the output into score and comment\n",
    "        eval_dict = eval_output.dict()  # Convert the Pydantic model to a dictionary\n",
    "        flattened_results = {}\n",
    "\n",
    "        for dim, value in eval_dict.items():\n",
    "            # Ensure nested dictionaries for each dimension are properly handled\n",
    "            if isinstance(value, dict):  # Check if the value is a dictionary\n",
    "                flattened_results[f\"{dim}_score\"] = value.get(\"score\")\n",
    "                flattened_results[f\"{dim}_comment\"] = value.get(\"comment\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected value type for dimension {dim}: {type(value)}\")\n",
    "\n",
    "        # Compute weighted score\n",
    "        overall_score = compute_overall_score(eval_dict)\n",
    "\n",
    "        # Calculate API cost\n",
    "        total_cost = calculate_api_call_cost(input_message, eval_output, encoding, model)\n",
    "\n",
    "        # Append results\n",
    "        all_results.append({\n",
    "            question_id_col: question_id,\n",
    "            **flattened_results,\n",
    "            \"overall_score\": overall_score,\n",
    "            \"api_call_cost\": total_cost\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame and save\n",
    "    df_eval = pd.DataFrame(all_results)\n",
    "    df_eval.to_csv(output_csv_path, index=False, quoting=1)\n",
    "    print(f\"Saved LLM judge evaluation to: {output_csv_path}\")\n",
    "\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rows: 100%|██████████| 33/33 [01:47<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM judge evaluation to: ../../data/eval/llm_judge_together_no_ref_en.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rows: 100%|██████████| 33/33 [01:44<00:00,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM judge evaluation to: ../../data/eval/llm_judge_together_no_ref_de.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# both datasets TOGETHER CONFIG NO REFERENCES\n",
    "# Load the environment variables\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "model_name = \"gpt-4o-2024-08-06\"\n",
    "# model_name = \"gpt-4o-mini-2024-07-18\"\n",
    "evaluation_style = \"together\"\n",
    "# evaluation_style = \"separate\"\n",
    "\n",
    "# Load your dataset\n",
    "df_en = pd.read_csv(\"../../data/short_dataset_en.csv\")\n",
    "# df_en = df_en.head(2).copy()  # For testing\n",
    "df_eval_en = run_llm_judge_evaluation_structured(\n",
    "    df=df_en,\n",
    "    context_col=\"chatbot_context_seen_by_agent_en\",\n",
    "    question_col=\"english_question_text_q\",\n",
    "    system_answer_col=\"chatbot_answer_en\",\n",
    "    question_id_col=\"question_id_q\",\n",
    "    output_csv_path=\"../../data/eval/llm_judge_together_no_ref_en.csv\",\n",
    "    evaluation_style=evaluation_style,\n",
    "    client=client,\n",
    "    reference_answer_col=None,\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "df_de = pd.read_csv(\"../../data/short_dataset_de.csv\")\n",
    "# df_de = df_de.head(2).copy()  # For testing\n",
    "df_eval_de = run_llm_judge_evaluation_structured(\n",
    "    df=df_de,\n",
    "    context_col=\"chatbot_context_seen_by_agent_de\",\n",
    "    question_col=\"german_question_text_q\",\n",
    "    system_answer_col=\"chatbot_answer_de\",\n",
    "    #reference_answer_col=\"human_answer_de\",\n",
    "    question_id_col=\"question_id_q\",\n",
    "    output_csv_path=\"../../data/eval/llm_judge_together_no_ref_de.csv\",\n",
    "    evaluation_style=evaluation_style,\n",
    "    client=client,\n",
    "    reference_answer_col=None,\n",
    "    model=model_name,\n",
    ")\n",
    "#calculate the mean of the evaluation\n",
    "df_mean = pd.read_csv(\"../../data/eval/mean_eval.csv\")\n",
    "#English\n",
    "mean_llm_en = df_eval_en[\"overall_score\"].mean()\n",
    "# add new row with metric == 'llm_judge_together_no_ref_en', value == mean_llm_en\n",
    "row = {\"metric\": 'llm_judge_together_no_ref_en', \"value\": mean_llm_en}\n",
    "df_mean = pd.concat([df_mean, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# #German\n",
    "mean_llm_de = df_eval_de[\"overall_score\"].mean()\n",
    "row = {\"metric\": 'llm_judge_together_no_ref_de', \"value\": mean_llm_de}\n",
    "df_mean = pd.concat([df_mean, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "df_mean.to_csv(\"../../data/eval/mean_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rows: 100%|██████████| 33/33 [05:23<00:00,  9.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM judge evaluation to: ../../data/eval/llm_judge_seperate_no_ref_en.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rows: 100%|██████████| 33/33 [05:06<00:00,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM judge evaluation to: ../../data/eval/llm_judge_seperate_no_ref_de.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# both datasets SEPERATE CONFIG NO REFERENCES\n",
    "# Load the environment variables\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "model_name = \"gpt-4o-2024-08-06\"\n",
    "# model_name = \"gpt-4o-mini-2024-07-18\"\n",
    "# evaluation_style = \"together\"\n",
    "evaluation_style = \"separate\"\n",
    "\n",
    "# Load your dataset\n",
    "df_en = pd.read_csv(\"../../data/short_dataset_en.csv\")\n",
    "# df_en = df_en.head(2).copy()  # For testing\n",
    "df_eval_en = run_llm_judge_evaluation_structured(\n",
    "    df=df_en,\n",
    "    context_col=\"chatbot_context_seen_by_agent_en\",\n",
    "    question_col=\"english_question_text_q\",\n",
    "    system_answer_col=\"chatbot_answer_en\",\n",
    "    question_id_col=\"question_id_q\",\n",
    "    output_csv_path=\"../../data/eval/llm_judge_seperate_no_ref_en.csv\",\n",
    "    evaluation_style=evaluation_style,\n",
    "    client=client,\n",
    "    reference_answer_col=None,\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "df_de = pd.read_csv(\"../../data/short_dataset_de.csv\")\n",
    "# df_de = df_de.head(2).copy()  # For testing\n",
    "df_eval_de = run_llm_judge_evaluation_structured(\n",
    "    df=df_de,\n",
    "    context_col=\"chatbot_context_seen_by_agent_de\",\n",
    "    question_col=\"german_question_text_q\",\n",
    "    system_answer_col=\"chatbot_answer_de\",\n",
    "    #reference_answer_col=\"human_answer_de\",\n",
    "    question_id_col=\"question_id_q\",\n",
    "    output_csv_path=\"../../data/eval/llm_judge_seperate_no_ref_de.csv\",\n",
    "    evaluation_style=evaluation_style,\n",
    "    client=client,\n",
    "    reference_answer_col=None,\n",
    "    model=model_name,\n",
    ")\n",
    "#calculate the mean of the evaluation\n",
    "df_mean = pd.read_csv(\"../../data/eval/mean_eval.csv\")\n",
    "#English\n",
    "mean_llm_en = df_eval_en[\"overall_score\"].mean()\n",
    "# add new row with metric == 'llm_judge_seperate_no_ref_en', value == mean_llm_en\n",
    "row = {\"metric\": 'llm_judge_seperate_no_ref_en', \"value\": mean_llm_en}\n",
    "df_mean = pd.concat([df_mean, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# #German\n",
    "mean_llm_de = df_eval_de[\"overall_score\"].mean()\n",
    "row = {\"metric\": 'llm_judge_seperate_no_ref_de', \"value\": mean_llm_de}\n",
    "df_mean = pd.concat([df_mean, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "df_mean.to_csv(\"../../data/eval/mean_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rows: 100%|██████████| 33/33 [03:03<00:00,  5.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM judge evaluation to: ../../data/eval/llm_judge_together_with_ref_en.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rows: 100%|██████████| 33/33 [02:38<00:00,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM judge evaluation to: ../../data/eval/llm_judge_together_with_ref_de.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# both datasets TOGETHER CONFIG WITH REFERENCES\n",
    "# Load the environment variables\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "model_name = \"gpt-4o-2024-08-06\"\n",
    "# model_name = \"gpt-4o-mini-2024-07-18\"\n",
    "evaluation_style = \"together\"\n",
    "# evaluation_style = \"separate\"\n",
    "\n",
    "# Load your dataset\n",
    "df_en = pd.read_csv(\"../../data/short_dataset_en.csv\")\n",
    "# df_en = df_en.head(2).copy()  # For testing\n",
    "df_eval_en = run_llm_judge_evaluation_structured(\n",
    "    df=df_en,\n",
    "    context_col=\"chatbot_context_seen_by_agent_en\",\n",
    "    question_col=\"english_question_text_q\",\n",
    "    system_answer_col=\"chatbot_answer_en\",\n",
    "    question_id_col=\"question_id_q\",\n",
    "    output_csv_path=\"../../data/eval/llm_judge_together_with_ref_en.csv\",\n",
    "    evaluation_style=evaluation_style,\n",
    "    client=client,\n",
    "    reference_answer_col=\"human_answer_en\",\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "df_de = pd.read_csv(\"../../data/short_dataset_de.csv\")\n",
    "# df_de = df_de.head(2).copy()  # For testing\n",
    "df_eval_de = run_llm_judge_evaluation_structured(\n",
    "    df=df_de,\n",
    "    context_col=\"chatbot_context_seen_by_agent_de\",\n",
    "    question_col=\"german_question_text_q\",\n",
    "    system_answer_col=\"chatbot_answer_de\",\n",
    "    question_id_col=\"question_id_q\",\n",
    "    output_csv_path=\"../../data/eval/llm_judge_together_with_ref_de.csv\",\n",
    "    evaluation_style=evaluation_style,\n",
    "    client=client,\n",
    "    reference_answer_col=\"human_answer_de\",\n",
    "    model=model_name,\n",
    ")\n",
    "#calculate the mean of the evaluation\n",
    "df_mean = pd.read_csv(\"../../data/eval/mean_eval.csv\")\n",
    "#English\n",
    "mean_llm_en = df_eval_en[\"overall_score\"].mean()\n",
    "# add new row with metric == 'llm_judge_together_with_ref_en', value == mean_llm_en\n",
    "row = {\"metric\": 'llm_judge_together_with_ref_en', \"value\": mean_llm_en}\n",
    "df_mean = pd.concat([df_mean, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# #German\n",
    "mean_llm_de = df_eval_de[\"overall_score\"].mean()\n",
    "row = {\"metric\": 'llm_judge_together_with_ref_de', \"value\": mean_llm_de}\n",
    "df_mean = pd.concat([df_mean, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "df_mean.to_csv(\"../../data/eval/mean_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rows: 100%|██████████| 33/33 [08:11<00:00, 14.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM judge evaluation to: ../../data/eval/llm_judge_seperate_with_ref_en.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating rows: 100%|██████████| 33/33 [05:46<00:00, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM judge evaluation to: ../../data/eval/llm_judge_seperate_with_ref_de.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# both datasets SEPERATE CONFIG WITH REFERENCES\n",
    "# Load the environment variables\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "model_name = \"gpt-4o-2024-08-06\"\n",
    "# model_name = \"gpt-4o-mini-2024-07-18\"\n",
    "# evaluation_style = \"together\"\n",
    "evaluation_style = \"separate\"\n",
    "\n",
    "# Load your dataset\n",
    "df_en = pd.read_csv(\"../../data/short_dataset_en.csv\")\n",
    "# df_en = df_en.head(2).copy()  # For testing\n",
    "df_eval_en = run_llm_judge_evaluation_structured(\n",
    "    df=df_en,\n",
    "    context_col=\"chatbot_context_seen_by_agent_en\",\n",
    "    question_col=\"english_question_text_q\",\n",
    "    system_answer_col=\"chatbot_answer_en\",\n",
    "    question_id_col=\"question_id_q\",\n",
    "    output_csv_path=\"../../data/eval/llm_judge_seperate_with_ref_en.csv\",\n",
    "    evaluation_style=evaluation_style,\n",
    "    client=client,\n",
    "    reference_answer_col=\"human_answer_en\",\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "df_de = pd.read_csv(\"../../data/short_dataset_de.csv\")\n",
    "# df_de = df_de.head(2).copy()  # For testing\n",
    "df_eval_de = run_llm_judge_evaluation_structured(\n",
    "    df=df_de,\n",
    "    context_col=\"chatbot_context_seen_by_agent_de\",\n",
    "    question_col=\"german_question_text_q\",\n",
    "    system_answer_col=\"chatbot_answer_de\",\n",
    "    question_id_col=\"question_id_q\",\n",
    "    output_csv_path=\"../../data/eval/llm_judge_seperate_with_ref_de.csv\",\n",
    "    evaluation_style=evaluation_style,\n",
    "    client=client,\n",
    "    reference_answer_col=\"human_answer_de\",\n",
    "    model=model_name,\n",
    ")\n",
    "#calculate the mean of the evaluation\n",
    "df_mean = pd.read_csv(\"../../data/eval/mean_eval.csv\")\n",
    "#English\n",
    "mean_llm_en = df_eval_en[\"overall_score\"].mean()\n",
    "# add new row with metric == 'llm_judge_seperate_with_ref_en', value == mean_llm_en\n",
    "row = {\"metric\": 'llm_judge_seperate_with_ref_en', \"value\": mean_llm_en}\n",
    "df_mean = pd.concat([df_mean, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# #German\n",
    "mean_llm_de = df_eval_de[\"overall_score\"].mean()\n",
    "row = {\"metric\": 'llm_judge_seperate_with_ref_de', \"value\": mean_llm_de}\n",
    "df_mean = pd.concat([df_mean, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "df_mean.to_csv(\"../../data/eval/mean_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for together_no_ref_en: 0.72€ for 33 pairs\n",
      "Cost for together_no_ref_de: 0.58€ for 33 pairs\n",
      "Cost for seperate_no_ref_en: 3.23€ for 33 pairs\n",
      "Cost for seperate_no_ref_de: 2.55€ for 33 pairs\n",
      "Cost for together_with_ref_en: 0.74€ for 33 pairs\n",
      "Cost for together_with_ref_de: 0.61€ for 33 pairs\n",
      "Cost for seperate_with_ref_en: 3.35€ for 33 pairs\n",
      "Cost for seperate_with_ref_de: 2.68€ for 33 pairs\n"
     ]
    }
   ],
   "source": [
    "# load the different evaluation files\n",
    "df_together_no_ref_en = pd.read_csv(\"../../data/eval/llm_judge_together_no_ref_en.csv\")\n",
    "df_together_no_ref_de = pd.read_csv(\"../../data/eval/llm_judge_together_no_ref_de.csv\")\n",
    "df_seperate_no_ref_en = pd.read_csv(\"../../data/eval/llm_judge_seperate_no_ref_en.csv\")\n",
    "df_seperate_no_ref_de = pd.read_csv(\"../../data/eval/llm_judge_seperate_no_ref_de.csv\")\n",
    "df_together_with_ref_en = pd.read_csv(\"../../data/eval/llm_judge_together_with_ref_en.csv\")\n",
    "df_together_with_ref_de = pd.read_csv(\"../../data/eval/llm_judge_together_with_ref_de.csv\")\n",
    "df_seperate_with_ref_en = pd.read_csv(\"../../data/eval/llm_judge_seperate_with_ref_en.csv\")\n",
    "df_seperate_with_ref_de = pd.read_csv(\"../../data/eval/llm_judge_seperate_with_ref_de.csv\")\n",
    "\n",
    "# calculate the cost via the 'api_call_cost' column for every evaluation\n",
    "cost_together_no_ref_en = df_together_no_ref_en[\"api_call_cost\"].sum()\n",
    "cost_together_no_ref_de = df_together_no_ref_de[\"api_call_cost\"].sum()\n",
    "cost_seperate_no_ref_en = df_seperate_no_ref_en[\"api_call_cost\"].sum()\n",
    "cost_seperate_no_ref_de = df_seperate_no_ref_de[\"api_call_cost\"].sum()\n",
    "cost_together_with_ref_en = df_together_with_ref_en[\"api_call_cost\"].sum()\n",
    "cost_together_with_ref_de = df_together_with_ref_de[\"api_call_cost\"].sum()\n",
    "cost_seperate_with_ref_en = df_seperate_with_ref_en[\"api_call_cost\"].sum()\n",
    "cost_seperate_with_ref_de = df_seperate_with_ref_de[\"api_call_cost\"].sum()\n",
    "# print the cost\n",
    "print(f\"Cost for together_no_ref_en: {cost_together_no_ref_en:.2f}€ for {len(df_together_no_ref_en)} pairs\")\n",
    "print(f\"Cost for together_no_ref_de: {cost_together_no_ref_de:.2f}€ for {len(df_together_no_ref_de)} pairs\")\n",
    "print(f\"Cost for seperate_no_ref_en: {cost_seperate_no_ref_en:.2f}€ for {len(df_seperate_no_ref_en)} pairs\")\n",
    "print(f\"Cost for seperate_no_ref_de: {cost_seperate_no_ref_de:.2f}€ for {len(df_seperate_no_ref_de)} pairs\")\n",
    "print(f\"Cost for together_with_ref_en: {cost_together_with_ref_en:.2f}€ for {len(df_together_with_ref_en)} pairs\")\n",
    "print(f\"Cost for together_with_ref_de: {cost_together_with_ref_de:.2f}€ for {len(df_together_with_ref_de)} pairs\")\n",
    "print(f\"Cost for seperate_with_ref_en: {cost_seperate_with_ref_en:.2f}€ for {len(df_seperate_with_ref_en)} pairs\")\n",
    "print(f\"Cost for seperate_with_ref_de: {cost_seperate_with_ref_de:.2f}€ for {len(df_seperate_with_ref_de)} pairs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
