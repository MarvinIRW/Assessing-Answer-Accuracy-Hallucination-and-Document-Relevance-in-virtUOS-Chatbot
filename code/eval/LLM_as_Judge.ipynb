{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "system_prompt_response_format_all = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across multiple dimensions. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to the University of Osnabrück. If a question is unrelated to the University, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Evaluation Dimensions (Rate each dimension on a scale of 0 to 4):\n",
    "- Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "- Answer Accuracy: The degree to which a response precisely addresses the user’s question by providing correct, complete, and relevant information that aligns with the question’s intent. Factual correctness is necessary but not sufficient; the response must also be on-point, thorough, and responsive to the exact context and purpose of the question.\n",
    "- User Satisfaction: Reflects the user's subjective evaluation of the answer's quality, emphasizing its effectiveness in addressing their question, delivering meaningful value, and creating a positive overall experience.\n",
    "- Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "- Context Quality: Evaluates the adequacy and relevance of the provided context in supporting the answer. A high-quality context is directly tied to the user’s question, providing all necessary information to formulate a complete and accurate response. If no context is provided, its absence is assessed for its impact on the quality of the answer.\n",
    "\n",
    "3. Scoring Guidelines:\n",
    "For each dimension, assign a score from 0 to 4 and follow these interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "For each dimension, follow these steps to ensure thorough and consistent evaluations:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for the impact on the evaluation dimensions.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for the dimension.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated dimensions.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "system_prompt_response_format_all_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions across multiple dimensions. Follow these detailed instructions to provide your evaluation:\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the university, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Evaluation Dimensions (Rate each dimension on a scale of 0 to 4):\n",
    "- Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "- Answer Accuracy: The degree to which a response precisely addresses the user’s question by providing correct, complete, and relevant information that aligns with the question’s intent. Factual correctness is necessary but not sufficient; the response must also be on-point, thorough, and responsive to the exact context and purpose of the question.\n",
    "- User Satisfaction: Reflects the user's subjective evaluation of the answer's quality, emphasizing its effectiveness in addressing their question, delivering meaningful value, and creating a positive overall experience.\n",
    "- Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "- Context Quality: Evaluates the adequacy and relevance of the provided context in supporting the answer. A high-quality context is directly tied to the user’s question, providing all necessary information to formulate a complete and accurate response. If no context is provided, its absence is assessed for its impact on the quality of the answer.\n",
    "\n",
    "\n",
    "4. Scoring Guidelines:\n",
    "For each dimension, assign a score from 0 to 4 and follow these interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "5. Evaluation Steps (Chain of Thought):\n",
    "For each dimension, follow these steps to ensure thorough and consistent evaluations:\n",
    "\n",
    "Step 1. Understand the Question, Context, and Reference Answer:\n",
    "\t- Read the user question carefully.\n",
    "\t- Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\t- Review the reference answer to establish a benchmark the evaluation dimensions.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "\t-Break down the system-generated answer into key components or claims.\n",
    "\t-Compare each component to the question, context, and reference answer for alignment and impact on the evaluation dimensions.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "\t-Identify specific aspects of the system-generated answer that align well with the evaluation dimension.\n",
    "\t-Note any shortcomings or inconsistencies, such as irrelevance, factual errors, or unclear phrasing, especially in comparison to the reference answer.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "\t-Based on your analysis, assign a score (0–4) for the dimension.\n",
    "\t-Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "6. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated dimensions.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_hallucination = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions. Follow these detailed instructions to provide your evaluation of the dimension: Hallucination.\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the University, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Evaluation Dimension:\n",
    "- Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "\n",
    "3. Scoring Guidelines:\n",
    "Assign a score from 0 to 4 based on the following interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "For this dimension, follow these steps to ensure a thorough and consistent evaluation:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for its impact on Hallucination.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with Hallucination.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for Hallucination.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated aspects of the system-generated answer.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "system_prompt_response_format_hallucination_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions. Follow these detailed instructions to provide your evaluation of the dimension: Hallucination.\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the university, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Evaluation Dimension:\n",
    "- Hallucination: Refers to the presence of factually incorrect or unfaithful information in the answer. Any claim that cannot be verified using the provided context or widely known facts is considered a hallucination.\n",
    "\n",
    "4. Scoring Guidelines:\n",
    "For each dimension, assign a score from 0 to 4 and follow these interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "5. Evaluation Steps (Chain of Thought):\n",
    "For this dimension, follow these steps to ensure a thorough and consistent evaluation:\n",
    "\n",
    "Step 1. Understand the Question, Context, and Reference Answer:\n",
    "\t- Read the user question carefully.\n",
    "\t- Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\t- Review the reference answer to establish a benchmark for Hallucination.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "\t-Break down the system-generated answer into key components or claims.\n",
    "\t-Compare each component to the question, context, and reference answer for alignment and impact on Hallucination.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "\t-Identify specific aspects of the system-generated answer that align well with Hallucination.\n",
    "\t-Note any shortcomings or inconsistencies, such as irrelevance, factual errors, or unclear phrasing, especially in comparison to the reference answer.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "\t-Based on your analysis, assign a score (0–4) for Hallucination.\n",
    "\t-Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "6. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated aspects of the system-generated answer.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_accuracy = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions. Follow these detailed instructions to provide your evaluation of the dimension: Answer Accuracy.\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the University, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Evaluation Dimension:\n",
    "- Answer Accuracy: The degree to which a response precisely addresses the user’s question by providing correct, complete, and relevant information that aligns with the question’s intent. Factual correctness is necessary but not sufficient; the response must also be on-point, thorough, and responsive to the exact context and purpose of the question.\n",
    "\n",
    "3. Scoring Guidelines:\n",
    "Assign a score from 0 to 4 based on the following interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "For this dimension, follow these steps to ensure a thorough and consistent evaluation:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for its impact on Answer Accuracy.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with Answer Accuracy.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for Answer Accuracy.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated aspects of the system-generated answer.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "system_prompt_response_format_accuracy_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions. Follow these detailed instructions to provide your evaluation of the dimension: Answer Accuracy.\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the university, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Evaluation Dimension:\n",
    "- Answer Accuracy: The degree to which a response precisely addresses the user’s question by providing correct, complete, and relevant information that aligns with the question’s intent. Factual correctness is necessary but not sufficient; the response must also be on-point, thorough, and responsive to the exact context and purpose of the question.\n",
    "\n",
    "4. Scoring Guidelines:\n",
    "For each dimension, assign a score from 0 to 4 and follow these interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "5. Evaluation Steps (Chain of Thought):\n",
    "For this dimension, follow these steps to ensure a thorough and consistent evaluation:\n",
    "\n",
    "Step 1. Understand the Question, Context, and Reference Answer:\n",
    "\t- Read the user question carefully.\n",
    "\t- Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\t- Review the reference answer to establish a benchmark for Answer Accuracy.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "\t-Break down the system-generated answer into key components or claims.\n",
    "\t-Compare each component to the question, context, and reference answer for alignment and impact on Answer Accuracy.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "\t-Identify specific aspects of the system-generated answer that align well with Answer Accuracy.\n",
    "\t-Note any shortcomings or inconsistencies, such as irrelevance, factual errors, or unclear phrasing, especially in comparison to the reference answer.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "\t-Based on your analysis, assign a score (0–4) for Answer Accuracy.\n",
    "\t-Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "6. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated aspects of the system-generated answer.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_satisfaction = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions. Follow these detailed instructions to provide your evaluation of the dimension: User Satisfaction.\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the University, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Evaluation Dimension:\n",
    "- User Satisfaction: Reflects the user's subjective evaluation of the answer's quality, emphasizing its effectiveness in addressing their question, delivering meaningful value, and creating a positive overall experience.\n",
    "\n",
    "3. Scoring Guidelines:\n",
    "Assign a score from 0 to 4 based on the following interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "For this dimension, follow these steps to ensure a thorough and consistent evaluation:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for its impact on User Satisfaction.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with User Satisfaction.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for User Satisfaction.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated aspects of the system-generated answer.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "system_prompt_response_format_satisfaction_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions. Follow these detailed instructions to provide your evaluation of the dimension: User Satisfaction.\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the university, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Evaluation Dimension:\n",
    "- User Satisfaction: Reflects the user's subjective evaluation of the answer's quality, emphasizing its effectiveness in addressing their question, delivering meaningful value, and creating a positive overall experience.\n",
    "\n",
    "4. Scoring Guidelines:\n",
    "For each dimension, assign a score from 0 to 4 and follow these interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "5. Evaluation Steps (Chain of Thought):\n",
    "For this dimension, follow these steps to ensure a thorough and consistent evaluation:\n",
    "\n",
    "Step 1. Understand the Question, Context, and Reference Answer:\n",
    "\t- Read the user question carefully.\n",
    "\t- Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\t- Review the reference answer to establish a benchmark for User Satisfaction.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "\t-Break down the system-generated answer into key components or claims.\n",
    "\t-Compare each component to the question, context, and reference answer for alignment and impact on User Satisfaction.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "\t-Identify specific aspects of the system-generated answer that align well with User Satisfaction.\n",
    "\t-Note any shortcomings or inconsistencies, such as irrelevance, factual errors, or unclear phrasing, especially in comparison to the reference answer.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "\t-Based on your analysis, assign a score (0–4) for User Satisfaction.\n",
    "\t-Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "6. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated aspects of the system-generated answer.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_coherence = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions. Follow these detailed instructions to provide your evaluation of the dimension: Coherence, Clarity, and Fluency.\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the University, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Evaluation Dimension:\n",
    "- Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "\n",
    "3. Scoring Guidelines:\n",
    "Assign a score from 0 to 4 based on the following interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "For this dimension, follow these steps to ensure a thorough and consistent evaluation:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for its impact on Coherence, Clarity, and Fluency.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with Coherence, Clarity, and Fluency.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for Coherence, Clarity, and Fluency.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated aspects of the system-generated answer.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "system_prompt_response_format_coherence_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions. Follow these detailed instructions to provide your evaluation of the dimension: Coherence, Clarity, and Fluency.\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the university, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Evaluation Dimension:\n",
    "- Coherence, Clarity, and Fluency: Evaluates the overall readability and presentation of the answer. A response that scores well in this dimension is logically structured, free of grammatical errors, easy to understand, and expressed in a natural, flowing manner.\n",
    "\n",
    "4. Scoring Guidelines:\n",
    "For each dimension, assign a score from 0 to 4 and follow these interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "5. Evaluation Steps (Chain of Thought):\n",
    "For this dimension, follow these steps to ensure a thorough and consistent evaluation:\n",
    "\n",
    "Step 1. Understand the Question, Context, and Reference Answer:\n",
    "\t- Read the user question carefully.\n",
    "\t- Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\t- Review the reference answer to establish a benchmark for Coherence, Clarity, and Fluency.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "\t-Break down the system-generated answer into key components or claims.\n",
    "\t-Compare each component to the question, context, and reference answer for alignment and impact on Coherence, Clarity, and Fluency.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "\t-Identify specific aspects of the system-generated answer that align well with Coherence, Clarity, and Fluency.\n",
    "\t-Note any shortcomings or inconsistencies, such as irrelevance, factual errors, or unclear phrasing, especially in comparison to the reference answer.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "\t-Based on your analysis, assign a score (0–4) for Coherence, Clarity, and Fluency.\n",
    "\t-Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "6. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated aspects of the system-generated answer.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_response_format_context = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions. Follow these detailed instructions to provide your evaluation of the dimension: Context Quality.\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the University, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Evaluation Dimension:\n",
    "- Context Quality: Evaluates the adequacy and relevance of the provided context in supporting the answer. A high-quality context is directly tied to the user’s question, providing all necessary information to formulate a complete and accurate response. If no context is provided, its absence is assessed for its impact on the quality of the answer.\n",
    "\n",
    "3. Scoring Guidelines:\n",
    "Assign a score from 0 to 4 based on the following interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "4. Evaluation Steps (Chain of Thought):\n",
    "For this dimension, follow these steps to ensure a thorough and consistent evaluation:\n",
    "\n",
    "Step 1. Understand the Question and Context:\n",
    "   - Read the user question carefully.\n",
    "   - Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "   - Break down the system-generated answer into key components or claims.\n",
    "   - Compare each component to the question and context for its impact on Context Quality.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "   - Identify specific aspects of the system-generated answer that align well with Context Quality.\n",
    "   - Note any shortcomings or inconsistencies, such as irrelevant details, factual errors, or unclear phrasing.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "   - Based on your analysis, assign a score (0–4) for Context Quality.\n",
    "   - Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "5. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated aspects of the system-generated answer.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n",
    "system_prompt_response_format_context_reference = \"\"\"\\\n",
    "You are an expert evaluator tasked with assessing the quality of system-generated answers to user questions. Follow these detailed instructions to provide your evaluation of the dimension: Context Quality.\n",
    "\n",
    "1. System Setting:\n",
    "The system to be evaluated is tasked with answering questions related to Osnabrück University. If a question is unrelated to the university, the system is instructed to politely decline. Consider this directive when evaluating.\n",
    "\n",
    "2. Reference Answer:\n",
    "In addition to the system-generated answer, a human-provided reference answer is available for comparison. Use the reference answer to assess the quality of the system's response. The reference answer represents a reliable benchmark for evaluating correctness, completeness, and appropriateness.\n",
    "\n",
    "3. Evaluation Dimension:\n",
    "- Context Quality: Evaluates the adequacy and relevance of the provided context in supporting the answer. A high-quality context is directly tied to the user’s question, providing all necessary information to formulate a complete and accurate response. If no context is provided, its absence is assessed for its impact on the quality of the answer.\n",
    "\n",
    "4. Scoring Guidelines:\n",
    "For each dimension, assign a score from 0 to 4 and follow these interpretations:\n",
    "- 0: Very Bad\n",
    "- 1: Bad\n",
    "- 2: Neutral\n",
    "- 3: Good\n",
    "- 4: Very Good\n",
    "\n",
    "5. Evaluation Steps (Chain of Thought):\n",
    "For this dimension, follow these steps to ensure a thorough and consistent evaluation:\n",
    "\n",
    "Step 1. Understand the Question, Context, and Reference Answer:\n",
    "\t- Read the user question carefully.\n",
    "\t- Examine the provided context (if any) and background of the question to understand the information need of the user.\n",
    "\t- Review the reference answer to establish a benchmark for Context Quality.\n",
    "\n",
    "Step 2. Analyze the System Answer:\n",
    "\t-Break down the system-generated answer into key components or claims.\n",
    "\t-Compare each component to the question, context, and reference answer for alignment and impact on Context Quality.\n",
    "\n",
    "Step 3. Assess Strengths and Weaknesses:\n",
    "\t-Identify specific aspects of the system-generated answer that align well with Context Quality.\n",
    "\t-Note any shortcomings or inconsistencies, such as irrelevance, factual errors, or unclear phrasing, especially in comparison to the reference answer.\n",
    "\n",
    "Step 4. Provide Justifications and Scores:\n",
    "\t-Based on your analysis, assign a score (0–4) for Context Quality.\n",
    "\t-Write a clear and concise explanation for your score, referring to observed strengths and weaknesses.\n",
    "\n",
    "6. Best Practices:\n",
    "- Objectivity: Base evaluations strictly on the provided content and guidelines.\n",
    "- Clarity: Be concise in your comment (1 sentence), focusing on specific observations.\n",
    "- Consider Ambiguities: For ambiguous or multi-faceted questions, assess based on the most straightforward interpretation unless otherwise stated.\n",
    "- Context Quality: If context is missing or insufficient, clearly describe its impact, but avoid penalizing unrelated aspects of the system-generated answer.\n",
    "\n",
    "Adhere strictly to these instructions, using the chain-of-thought reasoning process to ensure consistent and high-quality evaluations.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Define the schema using pydantic\n",
    "class DimensionScore(BaseModel):\n",
    "    score: int  # 0-4\n",
    "    comment: str\n",
    "\n",
    "class EvaluationOutput_all(BaseModel):\n",
    "    hallucination: DimensionScore\n",
    "    answer_accuracy: DimensionScore\n",
    "    user_satisfaction: DimensionScore\n",
    "    coherence_clarity_fluency: DimensionScore\n",
    "    context_quality: DimensionScore\n",
    "\n",
    "class EvaluationOutput_hallucination(BaseModel):\n",
    "    hallucination: DimensionScore\n",
    "\n",
    "class EvaluationOutput_accuracy(BaseModel):\n",
    "    answer_accuracy: DimensionScore\n",
    "\n",
    "class EvaluationOutput_satisfaction(BaseModel):\n",
    "    user_satisfaction: DimensionScore\n",
    "\n",
    "class EvaluationOutput_coherence(BaseModel):\n",
    "    coherence_clarity_fluency: DimensionScore\n",
    "\n",
    "class EvaluationOutput_context(BaseModel):\n",
    "    context_quality: DimensionScore\n",
    "    \n",
    "def evaluate_with_llm_as_judge_structured(\n",
    "    context: str,\n",
    "    user_question: str,\n",
    "    system_answer: str,\n",
    "    reference_answer: str,\n",
    "    client: OpenAI,\n",
    "    model_name: str,\n",
    "    evaluation_style: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the OpenAI API with structured outputs, enforcing the defined schema.\n",
    "    \n",
    "    If evaluation_style == 'together', do a single call with the multi-dimension\n",
    "    prompt and schema (with or without a reference).\n",
    "    \n",
    "    If evaluation_style == 'separate', do five separate calls (one per dimension);\n",
    "    if reference answers exist for each dimension, use the reference prompt, else\n",
    "    use the default dimension prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 1) EVALUATION STYLE: \"TOGETHER\"\n",
    "    # -------------------------------------------------------\n",
    "    if evaluation_style == \"together\":\n",
    "        # If a reference answer is provided and not empty\n",
    "        if reference_answer:\n",
    "            # Use the \"all dimensions with reference\" prompt\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_response_format_all_reference\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "                        Context:\n",
    "                        <context>\n",
    "                        {context}\n",
    "                        </context>\n",
    "\n",
    "                        User Question:\n",
    "                        <user_question>\n",
    "                        {user_question}\n",
    "                        </user_question>\n",
    "\n",
    "                        Reference Answer:\n",
    "                        <reference_answer>\n",
    "                        {reference_answer}\n",
    "                        </reference_answer>\n",
    "\n",
    "                        System Answer:\n",
    "                        <system_answer>\n",
    "                        {system_answer}\n",
    "                        </system_answer>\n",
    "                        \"\"\".strip()\n",
    "                }\n",
    "            ]\n",
    "        else:\n",
    "            # No reference answer => use the default \"all-dimensions\" prompt\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_response_format_all\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "                        Context:\n",
    "                        <context>\n",
    "                        {context}\n",
    "                        </context>\n",
    "\n",
    "                        User Question:\n",
    "                        <user_question>\n",
    "                        {user_question}\n",
    "                        </user_question>\n",
    "\n",
    "                        System Answer:\n",
    "                        <system_answer>\n",
    "                        {system_answer}\n",
    "                        </system_answer>\n",
    "                        \"\"\".strip()\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        # Single call, enforcing the multi-dimension schema\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            response_format=EvaluationOutput_all\n",
    "        )\n",
    "        return completion.choices[0].message.parsed, messages\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 2) EVALUATION STYLE: \"SEPARATE\"\n",
    "    # -------------------------------------------------------\n",
    "    elif evaluation_style == \"separate\":\n",
    "        dimension_prompts_and_schemas_no_ref = {\n",
    "            \"hallucination\": (\n",
    "                system_prompt_response_format_hallucination,\n",
    "                EvaluationOutput_hallucination\n",
    "            ),\n",
    "            \"answer_accuracy\": (\n",
    "                system_prompt_response_format_accuracy,\n",
    "                EvaluationOutput_accuracy\n",
    "            ),\n",
    "            \"user_satisfaction\": (\n",
    "                system_prompt_response_format_satisfaction,\n",
    "                EvaluationOutput_satisfaction\n",
    "            ),\n",
    "            \"coherence_clarity_fluency\": (\n",
    "                system_prompt_response_format_coherence,\n",
    "                EvaluationOutput_coherence\n",
    "            ),\n",
    "            \"context_quality\": (\n",
    "                system_prompt_response_format_context,\n",
    "                EvaluationOutput_context\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Reference versions\n",
    "        dimension_prompts_and_schemas_ref = {\n",
    "            \"hallucination\": (\n",
    "                system_prompt_response_format_hallucination_reference, \n",
    "                EvaluationOutput_hallucination\n",
    "            ),\n",
    "            \"answer_accuracy\": (\n",
    "                system_prompt_response_format_accuracy_reference,\n",
    "                EvaluationOutput_accuracy\n",
    "            ),\n",
    "            \"user_satisfaction\": (\n",
    "                system_prompt_response_format_satisfaction_reference,\n",
    "                EvaluationOutput_satisfaction\n",
    "            ),\n",
    "            \"coherence_clarity_fluency\": (\n",
    "                system_prompt_response_format_coherence_reference,\n",
    "                EvaluationOutput_coherence\n",
    "            ),\n",
    "            \"context_quality\": (\n",
    "                system_prompt_response_format_context_reference,\n",
    "                EvaluationOutput_context\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Decide which dictionary to use\n",
    "        if reference_answer:\n",
    "            dimension_prompts_and_schemas = dimension_prompts_and_schemas_ref\n",
    "        else:\n",
    "            dimension_prompts_and_schemas = dimension_prompts_and_schemas_no_ref\n",
    "\n",
    "        combined_dimensions = {}\n",
    "        all_messages = []\n",
    "\n",
    "        # Loop over each dimension\n",
    "        for dim_name, (dim_prompt, dim_schema) in dimension_prompts_and_schemas.items():\n",
    "            # Build the user content\n",
    "            user_content = f\"\"\"\n",
    "                    Context:\n",
    "                    <context>\n",
    "                    {context}\n",
    "                    </context>\n",
    "\n",
    "                    User Question:\n",
    "                    <user_question>\n",
    "                    {user_question}\n",
    "                    </user_question>\n",
    "                    \"\"\".strip()\n",
    "\n",
    "            # If we do have a reference, include it in the prompt\n",
    "            if reference_answer:\n",
    "                user_content += f\"\"\"\n",
    "                    Reference Answer:\n",
    "                    <reference_answer>\n",
    "                    {reference_answer}\n",
    "                    </reference_answer>\n",
    "                    \"\"\".strip()\n",
    "\n",
    "            # Always include the system answer\n",
    "            user_content += f\"\"\"\n",
    "                System Answer:\n",
    "                <system_answer>\n",
    "                {system_answer}\n",
    "                </system_answer>\n",
    "                \"\"\".strip()\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": dim_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ]\n",
    "            #print(messages)\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                temperature=0.0,\n",
    "                response_format=dim_schema\n",
    "            )\n",
    "            parsed_output = completion.choices[0].message.parsed\n",
    "            all_messages.append(messages)\n",
    "\n",
    "            # E.g., parsed_output might be {\"hallucination\": {\"score\": ..., \"comment\": ...}}\n",
    "            dimension_score_obj = getattr(parsed_output, dim_name)\n",
    "            combined_dimensions[dim_name] = dimension_score_obj\n",
    "\n",
    "        # Combine into a single instance of EvaluationOutput_all\n",
    "        combined_parsed = EvaluationOutput_all(\n",
    "            hallucination=combined_dimensions[\"hallucination\"],\n",
    "            answer_accuracy=combined_dimensions[\"answer_accuracy\"],\n",
    "            user_satisfaction=combined_dimensions[\"user_satisfaction\"],\n",
    "            coherence_clarity_fluency=combined_dimensions[\"coherence_clarity_fluency\"],\n",
    "            context_quality=combined_dimensions[\"context_quality\"]\n",
    "        )\n",
    "\n",
    "        return combined_parsed, all_messages\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def compute_weighted_score(evaluation_dict):\n",
    "    \"\"\"\n",
    "    A toy weighting:\n",
    "    hallucination:              0.3\n",
    "    answer_accuracy:            0.3\n",
    "    user_satisfaction:          0.1\n",
    "    coherence_clarity_fluency:  0.1\n",
    "    context_quality:            0.2\n",
    "    \"\"\"\n",
    "    h = evaluation_dict[\"hallucination\"][\"score\"]\n",
    "    a = evaluation_dict[\"answer_accuracy\"][\"score\"]\n",
    "    s = evaluation_dict[\"user_satisfaction\"][\"score\"]\n",
    "    c_c_f = evaluation_dict[\"coherence_clarity_fluency\"][\"score\"]\n",
    "    c_qual  = evaluation_dict[\"context_quality\"][\"score\"]\n",
    "\n",
    "    weighted = (\n",
    "        0.3 * h +\n",
    "        0.3 * a +\n",
    "        0.1 * s +\n",
    "        0.1 * c_c_f +\n",
    "        0.2 * c_qual\n",
    "    )\n",
    "    return weighted\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def calculate_api_call_cost(input_text: str, output_text: str, encoding: tiktoken, model_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the cost of an API call based on input and output texts.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text to be tokenized.\n",
    "        output_text (str): The output text to be tokenized.\n",
    "        model (str): The model name to determine the appropriate tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        float: The total cost of the API call.\n",
    "    \"\"\"\n",
    "    # Cast the input and output texts to strings\n",
    "    input_text = str(input_text)\n",
    "    output_text = str(output_text)\n",
    "    # Define the cost per million tokens\n",
    "    if model_name == \"gpt-4o-2024-08-06\":\n",
    "        input_cost_per_million = 2.50\n",
    "        output_cost_per_million = 10.00\n",
    "    elif model_name == \"gpt-4o-mini-2024-07-18\":\n",
    "        input_cost_per_million = 0.15\n",
    "        output_cost_per_million = 0.6\n",
    "    else:\n",
    "        return \"Not defined for this model\"\n",
    "\n",
    "    # Tokenize the input and output texts\n",
    "    input_tokens = encoding.encode(input_text)\n",
    "    output_tokens = encoding.encode(output_text)\n",
    "\n",
    "    # Calculate the number of tokens\n",
    "    num_input_tokens = len(input_tokens)\n",
    "    num_output_tokens = len(output_tokens)\n",
    "\n",
    "    if num_input_tokens > 128000 or num_output_tokens > 128000:\n",
    "        print(f\"Number of input tokens: {num_input_tokens}\", f\"Number of output tokens: {num_output_tokens} - Too many tokens for this model\")\n",
    "\n",
    "    # Calculate the cost for input and output tokens\n",
    "    input_cost = (num_input_tokens / 1_000_000) * input_cost_per_million\n",
    "    output_cost = (num_output_tokens / 1_000_000) * output_cost_per_million\n",
    "\n",
    "    # Total cost is the sum of input and output costs\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "\n",
    "def run_llm_judge_evaluation_structured(\n",
    "    df: pd.DataFrame,\n",
    "    context_col: str,\n",
    "    question_col: str,\n",
    "    system_answer_col: str,\n",
    "    question_id_col: str,\n",
    "    output_csv_path: str,\n",
    "    evaluation_style: str,\n",
    "    client: OpenAI,\n",
    "    reference_answer_col: str = None,\n",
    "    model: str = \"gpt-4o-2024-08-06\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates each row in `df` by calling the LLM (one time per row) using the provided\n",
    "    system_prompt. Saves results to `output_csv_path`.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the rows to evaluate.\n",
    "        context_col (str): The column name in `df` that holds the 'context'.\n",
    "        question_col (str): The column name for the user question.\n",
    "        system_answer_col (str): The column name for the system's answer.\n",
    "        question_id_col (str): The column name for a unique question or row ID.\n",
    "        output_csv_path (str): Where to save the final evaluations as CSV.\n",
    "        evaluation_style (str): The evaluation style to use. One of 'together' or 'separate'.\n",
    "        client (OpenAI): The OpenAI client instance.\n",
    "        model (str): The model name to use for the evaluation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the evaluation scores & comment, \n",
    "                      plus the weighted overall score.\n",
    "    \"\"\"\n",
    "    # Initialize the tokenizer for the specified model\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Model '{model}' not found. Please provide a valid model name.\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        context = row[context_col]\n",
    "        user_question = row[question_col]\n",
    "        system_answer = row[system_answer_col]\n",
    "        question_id = row[question_id_col]\n",
    "\n",
    "        if reference_answer_col:\n",
    "            reference_answer = row[reference_answer_col]\n",
    "        else:\n",
    "            reference_answer = None\n",
    "\n",
    "        # Call the API with structured outputs\n",
    "        eval_output, input_message = evaluate_with_llm_as_judge_structured(\n",
    "            context=context,\n",
    "            user_question=user_question,\n",
    "            system_answer=system_answer,\n",
    "            reference_answer=reference_answer,\n",
    "            client=client,\n",
    "            model_name=model,\n",
    "            evaluation_style=evaluation_style\n",
    "        )\n",
    "        # Flatten the output into score and comment\n",
    "        eval_dict = eval_output.dict()  # Convert the Pydantic model to a dictionary\n",
    "        flattened_results = {}\n",
    "\n",
    "        for dim, value in eval_dict.items():\n",
    "            # Ensure nested dictionaries for each dimension are properly handled\n",
    "            if isinstance(value, dict):  # Check if the value is a dictionary\n",
    "                flattened_results[f\"{dim}_score\"] = value.get(\"score\")\n",
    "                flattened_results[f\"{dim}_comment\"] = value.get(\"comment\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected value type for dimension {dim}: {type(value)}\")\n",
    "\n",
    "        # Compute weighted score\n",
    "        weighted_overall = compute_weighted_score(eval_dict)\n",
    "\n",
    "        # Calculate API cost\n",
    "        total_cost = calculate_api_call_cost(input_message, eval_output, encoding, model)\n",
    "\n",
    "        # Append results\n",
    "        all_results.append({\n",
    "            question_id_col: question_id,\n",
    "            **flattened_results,\n",
    "            \"weighted_overall_score\": weighted_overall,\n",
    "            \"api_call_cost\": total_cost\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame and save\n",
    "    df_eval = pd.DataFrame(all_results)\n",
    "    df_eval.to_csv(output_csv_path, index=False, quoting=1)\n",
    "    print(f\"Saved LLM judge evaluation to: {output_csv_path}\")\n",
    "\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LLM judge evaluation to: ../../data/eval/evaluation_llm_judge_chatbot_en.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id_q</th>\n",
       "      <th>hallucination_score</th>\n",
       "      <th>hallucination_comment</th>\n",
       "      <th>answer_accuracy_score</th>\n",
       "      <th>answer_accuracy_comment</th>\n",
       "      <th>user_satisfaction_score</th>\n",
       "      <th>user_satisfaction_comment</th>\n",
       "      <th>coherence_clarity_fluency_score</th>\n",
       "      <th>coherence_clarity_fluency_comment</th>\n",
       "      <th>context_quality_score</th>\n",
       "      <th>context_quality_comment</th>\n",
       "      <th>weighted_overall_score</th>\n",
       "      <th>api_call_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>153</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer accurately lists various sch...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer accurately and comprehensive...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer provides a comprehensive and...</td>\n",
       "      <td>4</td>\n",
       "      <td>The answer is well-structured, clearly present...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer provides a comprehensive and...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.002793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>The system answer includes a Master's program ...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer accurately identifies the re...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer effectively addresses the us...</td>\n",
       "      <td>4</td>\n",
       "      <td>The answer is well-structured, clearly outline...</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer provides relevant and detail...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.003961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id_q  hallucination_score  \\\n",
       "0            153                    4   \n",
       "1              9                    1   \n",
       "\n",
       "                               hallucination_comment  answer_accuracy_score  \\\n",
       "0  The system answer accurately lists various sch...                      4   \n",
       "1  The system answer includes a Master's program ...                      4   \n",
       "\n",
       "                             answer_accuracy_comment  user_satisfaction_score  \\\n",
       "0  The system answer accurately and comprehensive...                        4   \n",
       "1  The system answer accurately identifies the re...                        4   \n",
       "\n",
       "                           user_satisfaction_comment  \\\n",
       "0  The system answer provides a comprehensive and...   \n",
       "1  The system answer effectively addresses the us...   \n",
       "\n",
       "   coherence_clarity_fluency_score  \\\n",
       "0                                4   \n",
       "1                                4   \n",
       "\n",
       "                   coherence_clarity_fluency_comment  context_quality_score  \\\n",
       "0  The answer is well-structured, clearly present...                      4   \n",
       "1  The answer is well-structured, clearly outline...                      4   \n",
       "\n",
       "                             context_quality_comment  weighted_overall_score  \\\n",
       "0  The system answer provides a comprehensive and...                     4.0   \n",
       "1  The system answer provides relevant and detail...                     3.1   \n",
       "\n",
       "   api_call_cost  \n",
       "0       0.002793  \n",
       "1       0.003961  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================\n",
    "# Usage Example\n",
    "# ============================\n",
    "# Example: Call the function with structured outputs\n",
    "# Load the environment variables\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "# model_name = \"gpt-4o-2024-08-06\"\n",
    "model_name = \"gpt-4o-mini-2024-07-18\"\n",
    "# evaluation_style = \"together\"\n",
    "evaluation_style = \"separate\"\n",
    "\n",
    "# Load your dataset\n",
    "df_en = pd.read_csv(\"../../data/final_merged_dataset_short_en_2.csv\")\n",
    "df_en = df_en.head(2).copy()  # For testing\n",
    "df_eval_en = run_llm_judge_evaluation_structured(\n",
    "    df=df_en,\n",
    "    context_col=\"chatbot_context_en\",\n",
    "    question_col=\"english_question_text_q\",\n",
    "    system_answer_col=\"chatbot_answer_en\",\n",
    "    question_id_col=\"question_id_q\",\n",
    "    output_csv_path=\"../../data/eval/evaluation_llm_judge_chatbot_en.csv\",\n",
    "    evaluation_style=evaluation_style,\n",
    "    client=client,\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "\n",
    "# # Load your dataset\n",
    "# df_de = pd.read_csv(\"../../data/final_merged_dataset_short_de_2.csv\")\n",
    "# df_de = df_de.head(2).copy()  # For testing\n",
    "# df_eval_de = run_llm_judge_evaluation_structured(\n",
    "#     df=df_de,\n",
    "#     context_col=\"chatbot_context_de\",\n",
    "#     question_col=\"german_question_text_q\",\n",
    "#     system_answer_col=\"chatbot_answer_de\",\n",
    "#     #reference_answer_col=\"human_answer_de\",\n",
    "#     question_id_col=\"question_id_q\",\n",
    "#     output_csv_path=\"../../data/eval/evaluation_llm_judge_chatbot_de.csv\",\n",
    "#     evaluation_style=evaluation_style,\n",
    "#     client=client,\n",
    "#     model=model_name,\n",
    "# )\n",
    "# calculate the mean of the evaluation\n",
    "df_mean = pd.read_csv(\"../../data/eval/mean_eval.csv\")\n",
    "# English\n",
    "# mean_llm_en = df_eval_en[\"weighted_overall_score\"].mean()\n",
    "# df_mean[\"llm_as_judge_en\"] = mean_llm_en\n",
    "# German\n",
    "# mean_llm_de = df_eval_de[\"weighted_overall_score\"].mean()\n",
    "# df_mean[\"llm_as_judge_de\"] = mean_llm_de\n",
    "\n",
    "\n",
    "# df_mean.to_csv(\"../../data/eval/mean_eval.csv\", index=False)\n",
    "df_eval_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system answer includes a Master's program in Cognitive Computing that is not mentioned in the provided context, indicating a hallucination.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0067536"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_eval_en['hallucination_comment'][1])\n",
    "df_eval_en[\"api_call_cost\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer accurately reflects the context without introducing any false information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0223275"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_eval_de['hallucination_comment'][1])\n",
    "df_eval_de[\"api_call_cost\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOund Links: ['https://www.uni-osnabrueck.de/studieninteressierte/stipendien-und-foerderung/;https://www.uni-osnabrueck.de/studieninteressierte/studieninteressierte-aus-dem-ausland/kosten-und-finanzierung/']\n",
      "FOund Links: ['https://www.ikw.uni-osnabrueck.de/studieninteressierte/willkommen.html;https://www.uni-osnabrueck.de/studieninteressierte/studiengaenge-a-z/cognitive-science-master-of-science/']\n",
      "Extracted HTML text from: https://www.ikw.uni-osnabrueck.de/studieninteressierte/willkommen.html;https://www.uni-osnabrueck.de/studieninteressierte/studiengaenge-a-z/cognitive-science-master-of-science/\n",
      "FOund Links: ['https://www.wiwi.uni-osnabrueck.de/fileadmin/documents/public/1_fachbereich/00_Allgemein/1.04_jahresberichte/Jahresbericht_FB9_2012-2013.pdf;https://www.uni-osnabrueck.de/fileadmin/documents/public/1_universitaet/1.3_organisation/d4_akad._angelegenheiten/d4_akad._angelegenheiten/mitteilungsblaetter/2016-2020/2019-05-09_Nr03.pdf']\n"
     ]
    }
   ],
   "source": [
    "# code form chatbot to extract text from pdf and html\n",
    "import requests\n",
    "import re\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfReader\n",
    "def extract_html_text(href: str, response) -> str:\n",
    "\n",
    "    link_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    div_content = link_soup.find(\"div\", class_=\"eb2\")\n",
    "    if div_content:\n",
    "        text = re.sub(r\"\\n+\", \"\\n\", div_content.text.strip())\n",
    "        content_with_link = \"\"\n",
    "        for link in div_content.find_all(\"a\", href=True):\n",
    "            text_anchor_tag = re.sub(r\"\\n+\", \"\\n\", link.text.strip())\n",
    "            content_with_link += f\" - {text_anchor_tag}: {link['href']}\"\n",
    "        return text + \"\\nHref found in the text:\\n\" + content_with_link\n",
    "    print(f\"Failed to fetch html content from: {href}\")\n",
    "    return \"\"\n",
    "def read_pdf_from_url(pdf_bytes: bytes, num_pages: int = 7) -> str:\n",
    "    \"\"\"\n",
    "    Read the content of a PDF file from a given byte stream.\n",
    "\n",
    "    Args:\n",
    "        pdf_bytes (bytes): Raw bytes of the PDF content.\n",
    "        num_pages (int, optional): Number of pages to process. If None, process all pages. Defaults to 7.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text content from the PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_stream = io.BytesIO(pdf_bytes)\n",
    "\n",
    "    pdf_text = \"\"\n",
    "    with pdf_stream as f:\n",
    "        reader = PdfReader(f)\n",
    "        if num_pages is None:\n",
    "            num_pages = len(reader.pages)\n",
    "        else:\n",
    "            num_pages = min(num_pages, len(reader.pages))\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            pdf_text += page.extract_text() or \"\"\n",
    "\n",
    "    return pdf_text\n",
    "\n",
    "def extract_pdf_text(href: str, response) -> str:\n",
    "\n",
    "    text = read_pdf_from_url(response)\n",
    "    return re.sub(r\"(\\n\\s*|\\n\\s+\\n\\s+)\", \"\\n\", text.strip())\n",
    "\n",
    "def context_from_links(links):\n",
    "    contents = []\n",
    "    taken_from = \"Information taken from:\"\n",
    "    search_result_text = \"Content not found\"\n",
    "    for tag in links:\n",
    "        href = tag\n",
    "        try:\n",
    "            # TODO I/O operation (use async code)\n",
    "            response = requests.get(href)\n",
    "            #print(f\"Fetching: {href}\")\n",
    "        except:\n",
    "            print(f\"Error while fetching: {href}\")\n",
    "            continue\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            if href.endswith(\".pdf\"):\n",
    "                print(f\"Extracted PDF text from: {href}\")\n",
    "                text = extract_pdf_text(href, response)\n",
    "            else:\n",
    "                print(f\"Extracted HTML text from: {href}\")\n",
    "                text = extract_html_text(href, response)\n",
    "\n",
    "            if text:\n",
    "                text = f\"{taken_from}{href}\\n{text}\"\n",
    "                contents.append(text)\n",
    "\n",
    "    return \"\\n\".join(contents) if contents else search_result_text, links\n",
    "\n",
    "# code from parsing links and getting context\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_links_from_string(links_str: str):\n",
    "    \"\"\"\n",
    "    Extracts all URLs (including Markdown-style) from a string.\n",
    "    Returns a list of raw link URLs.\n",
    "    \"\"\"\n",
    "    if not isinstance(links_str, str) or not links_str.strip():\n",
    "        return []\n",
    "\n",
    "    # Regex approach:\n",
    "    # 1) Find Markdown links: [some text](http://some-url.com)\n",
    "    # 2) Also find bare URLs like http://... or https://...\n",
    "    pattern = r'\\((https?://[^\\)]+)\\)|(https?://[^\\s]+)'\n",
    "    # Explanation:\n",
    "    #   \\((https?://[^\\)]+)\\) matches markdown ( capturing URL inside parentheses )\n",
    "    #   (https?://[^\\s]+) captures bare URLs\n",
    "    matches = re.findall(pattern, links_str)\n",
    "\n",
    "    # matches is a list of tuples, e.g. [(\"https://...\",\"\"), (\"\",\"https://...\")]\n",
    "    # We gather the non-empty group from each tuple\n",
    "    urls = []\n",
    "    for md_link, normal_link in matches:\n",
    "        if md_link:\n",
    "            urls.append(md_link)\n",
    "        elif normal_link:\n",
    "            urls.append(normal_link)\n",
    "    print(f\"FOund Links: {urls}\")\n",
    "    return urls\n",
    "\n",
    "def gather_context_from_links(links_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Parses the given links_str (which may be plain or Markdown),\n",
    "    fetches each link, extracts text, and concatenates them into\n",
    "    a single context string.\n",
    "    \"\"\"\n",
    "    links = parse_links_from_string(links_str)\n",
    "    if not links:\n",
    "        return \"No links or no content found.\"\n",
    "\n",
    "    all_contents = context_from_links(links)\n",
    "\n",
    "    \n",
    "    return all_contents\n",
    "\n",
    "def add_content_to_df(df, links_col, context_col):\n",
    "    \"\"\"\n",
    "    Adds context to the DataFrame by fetching and parsing the content\n",
    "    from the URLs in the specified column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to update.\n",
    "        links_col (str): The column name containing the URLs.\n",
    "        context_col (str): The column name to store the extracted content.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame.\n",
    "    \"\"\"\n",
    "    context_rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        links_str = row[links_col]\n",
    "        context = gather_context_from_links(links_str)\n",
    "        context_rows.append(context)\n",
    "\n",
    "    df[context_col] = context_rows\n",
    "    return df\n",
    "\n",
    "df_en = pd.read_csv(\"../../data/final_merged_dataset_short_en.csv\")\n",
    "df_en = df_en.head(3).copy()\n",
    "df_en = add_content_to_df(df_en, \"chatbot_visited_urls_en\", \"chatbot_context\")\n",
    "\n",
    "df_en.head()\n",
    "\n",
    "df_en.to_csv(\"../../data/final_merged_dataset_short_en_testing.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
