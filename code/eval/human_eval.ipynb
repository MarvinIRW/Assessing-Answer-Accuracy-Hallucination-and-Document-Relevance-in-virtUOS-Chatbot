{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def add_question_groups(\n",
    "    input_lss: str,\n",
    "    output_lss: str,\n",
    "    original_group_id: str,\n",
    "    new_group_count: int,\n",
    "    gid_start: int,\n",
    "    qid_start: int,\n",
    "    questions_per_group: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Clones a group in a LimeSurvey .lss file and creates multiple new copies.\n",
    "    Also updates placeholders for newly added question codes like 'qid1' and 'langq1'.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Parse the LSS and find main sections\n",
    "    tree = ET.parse(input_lss)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    groups_elem       = root.find(\"./groups\")\n",
    "    questions_elem    = root.find(\"./questions\")\n",
    "    subquestions_elem = root.find(\"./subquestions\")\n",
    "    answers_elem      = root.find(\"./answers\")\n",
    "    qattributes_elem  = root.find(\"./question_attributes\")  # For copying random_order, etc.\n",
    "\n",
    "    if not (groups_elem and questions_elem and subquestions_elem and answers_elem):\n",
    "        raise ValueError(\"Could not locate <groups>, <questions>, <subquestions>, or <answers> in the LSS.\")\n",
    "\n",
    "    group_rows      = groups_elem.find(\"rows\")\n",
    "    question_rows   = questions_elem.find(\"rows\")\n",
    "    subquestion_rows= subquestions_elem.find(\"rows\")\n",
    "    answer_rows     = answers_elem.find(\"rows\")\n",
    "\n",
    "    if not (group_rows and question_rows and subquestion_rows and answer_rows):\n",
    "        raise ValueError(\"One of the <rows> sections is missing inside <groups>, <questions>, <subquestions>, or <answers>.\")\n",
    "\n",
    "    # We'll also gather the question_attributes rows if present\n",
    "    qattr_rows = None\n",
    "    if qattributes_elem is not None:\n",
    "        qattr_rows = qattributes_elem.find(\"rows\")\n",
    "\n",
    "    # 2) Gather the original group entries (both languages) with gid=original_group_id\n",
    "    original_group_entries = [\n",
    "        row for row in group_rows.findall(\"row\")\n",
    "        if row.find(\"gid\").text == original_group_id\n",
    "    ]\n",
    "    if not original_group_entries:\n",
    "        raise ValueError(f\"Could not find group with gid={original_group_id}.\")\n",
    "\n",
    "    # 3) Identify all QIDs from that group (questions + subquestions)\n",
    "    original_qid_list = []\n",
    "\n",
    "    # A) Main questions in that group\n",
    "    these_question_rows = [\n",
    "        row for row in question_rows.findall(\"row\")\n",
    "        if row.find(\"gid\").text == original_group_id\n",
    "    ]\n",
    "    original_qid_list.extend(row.find(\"qid\").text for row in these_question_rows)\n",
    "\n",
    "    # B) Subquestions in that group\n",
    "    these_subquestion_rows = [\n",
    "        row for row in subquestion_rows.findall(\"row\")\n",
    "        if row.find(\"gid\").text == original_group_id\n",
    "    ]\n",
    "    original_qid_list.extend(row.find(\"qid\").text for row in these_subquestion_rows)\n",
    "\n",
    "    # Make the list unique\n",
    "    original_qid_list = list(set(original_qid_list))\n",
    "\n",
    "    # 4) Create a mapping {old_qid -> new_qid}, so each old QID gets a new unique QID\n",
    "    qid_counter = qid_start\n",
    "    old_qid_to_new_qid = {}\n",
    "    for old_qid in sorted(original_qid_list):\n",
    "        old_qid_to_new_qid[old_qid] = str(qid_counter)\n",
    "        qid_counter += 1\n",
    "\n",
    "    # 5) We replicate the group until we have new_group_count total\n",
    "    #    If new_group_count=2, that means we add 1 new copy (since 1 original already exists).\n",
    "    additional_copies = new_group_count - 1\n",
    "    gid_counter = gid_start\n",
    "\n",
    "    # Prepare lists to hold new <row> elements\n",
    "    all_new_group_rows       = []\n",
    "    all_new_question_rows    = []\n",
    "    all_new_subquestion_rows = []\n",
    "    all_new_answer_rows      = []\n",
    "    all_new_qattr_rows       = []\n",
    "\n",
    "    for i in range(1, additional_copies + 1):\n",
    "        # e.g. \"ans1\" => \"ans2\" if i=1\n",
    "        # also \"qid1\" => \"qid2\", \"langq1\" => \"langq2\" if i=1, etc.\n",
    "        placeholder_number = i + 1\n",
    "\n",
    "        # For every questions_per_group new copies, we increment randomgroup by 1\n",
    "        # e.g., if questions_per_group=6, i=1..6 => randomgroup==1\n",
    "        index           = i\n",
    "        randomgroup_val = (index // questions_per_group) + 1\n",
    "\n",
    "        #\n",
    "        # (A) Clone the group row(s) for each language\n",
    "        #\n",
    "        for orig_grp in original_group_entries:\n",
    "            new_grp = ET.fromstring(ET.tostring(orig_grp))\n",
    "\n",
    "            # Set new GID\n",
    "            new_grp.find(\"gid\").text = str(gid_counter)\n",
    "\n",
    "            # Optionally shift group_order\n",
    "            go_el = new_grp.find(\"group_order\")\n",
    "            if go_el is not None and go_el.text.isdigit():\n",
    "                old_val = int(go_el.text)\n",
    "                go_el.text = str(old_val + i)\n",
    "\n",
    "            # Update placeholders in <description>\n",
    "            desc_el = new_grp.find(\"description\")\n",
    "            if desc_el is not None and desc_el.text:\n",
    "                desc_text = desc_el.text\n",
    "\n",
    "                # Replace placeholders\n",
    "                desc_text = desc_text.replace(\"PLACEHOLDER_QUESTION_1\",  f\"PLACEHOLDER_QUESTION_{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"PLACEHOLDER_ANSWER_1\",    f\"PLACEHOLDER_ANSWER_{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"PLACEHOLDER_CONTEXT_1\",   f\"PLACEHOLDER_CONTEXT_{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"PLACEHOLDER_ID_1\",        f\"PLACEHOLDER_ID_{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"PLACEHOLDER_LANG_1\",      f\"PLACEHOLDER_LANG_{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"ans1\",                    f\"ans{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"qid1\",                    f\"qid{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"langq1\",                  f\"langq{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"comment1\",                f\"comment{placeholder_number}\")\n",
    "\n",
    "                desc_el.text = desc_text\n",
    "\n",
    "            # Update grelevance so that after questions_per_group copies, randomgroup==2, etc.\n",
    "            grel_el = new_grp.find(\"grelevance\")\n",
    "            if grel_el is not None:\n",
    "                old_expr = grel_el.text or \"\"\n",
    "                if \"==1\" in old_expr:\n",
    "                    new_expr = old_expr.replace(\"==1\", f\"=={randomgroup_val}\")\n",
    "                elif not old_expr.strip():\n",
    "                    new_expr = f\"randomgroup=={randomgroup_val}\"\n",
    "                else:\n",
    "                    new_expr = old_expr\n",
    "                grel_el.text = new_expr\n",
    "\n",
    "            all_new_group_rows.append(new_grp)\n",
    "\n",
    "        #\n",
    "        # (B) Clone all questions in that group, applying new QIDs\n",
    "        #\n",
    "        for qrow in these_question_rows:\n",
    "            old_qid = qrow.find(\"qid\").text\n",
    "            new_qid = old_qid_to_new_qid[old_qid]\n",
    "\n",
    "            new_q = ET.fromstring(ET.tostring(qrow))\n",
    "            # Overwrite GID and QID\n",
    "            new_q.find(\"gid\").text = str(gid_counter)\n",
    "            new_q.find(\"qid\").text = new_qid\n",
    "\n",
    "            # Shift question_order\n",
    "            qo_el = new_q.find(\"question_order\")\n",
    "            if qo_el is not None and qo_el.text.isdigit():\n",
    "                old_val = int(qo_el.text)\n",
    "                qo_el.text = str(old_val + i)\n",
    "\n",
    "            # Replace placeholders in <title> and <question>\n",
    "            title_el = new_q.find(\"title\")\n",
    "            if title_el is not None and title_el.text:\n",
    "                tmp = title_el.text\n",
    "                tmp = tmp.replace(\"ans1\",   f\"ans{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"qid1\",   f\"qid{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"langq1\", f\"langq{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"comment1\", f\"comment{placeholder_number}\")\n",
    "                title_el.text = tmp\n",
    "\n",
    "            question_el = new_q.find(\"question\")\n",
    "            if question_el is not None and question_el.text:\n",
    "                tmp = question_el.text\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_QUESTION_1\",  f\"PLACEHOLDER_QUESTION_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_ANSWER_1\",    f\"PLACEHOLDER_ANSWER_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_CONTEXT_1\",   f\"PLACEHOLDER_CONTEXT_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_ID_1\",        f\"PLACEHOLDER_ID_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_LANG_1\",      f\"PLACEHOLDER_LANG_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"ans1\",                    f\"ans{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"qid1\",                    f\"qid{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"langq1\",                  f\"langq{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"comment1\",                f\"comment{placeholder_number}\")\n",
    "                question_el.text = tmp\n",
    "\n",
    "            all_new_question_rows.append(new_q)\n",
    "\n",
    "        #\n",
    "        # (C) Clone subquestions in that group\n",
    "        #\n",
    "        for sqrow in these_subquestion_rows:\n",
    "            old_subq_qid = sqrow.find(\"qid\").text\n",
    "            new_subq_qid = old_qid_to_new_qid[old_subq_qid]\n",
    "\n",
    "            old_parent_qid = sqrow.find(\"parent_qid\").text\n",
    "            new_parent_qid = old_qid_to_new_qid[old_parent_qid]\n",
    "\n",
    "            new_sq = ET.fromstring(ET.tostring(sqrow))\n",
    "            new_sq.find(\"qid\").text        = new_subq_qid\n",
    "            new_sq.find(\"parent_qid\").text = new_parent_qid\n",
    "            new_sq.find(\"gid\").text        = str(gid_counter)\n",
    "\n",
    "            qo_el = new_sq.find(\"question_order\")\n",
    "            if qo_el is not None and qo_el.text.isdigit():\n",
    "                old_val = int(qo_el.text)\n",
    "                qo_el.text = str(old_val + i)\n",
    "\n",
    "            # placeholders in <title> and <question>\n",
    "            title_el = new_sq.find(\"title\")\n",
    "            if title_el is not None and title_el.text:\n",
    "                tmp = title_el.text\n",
    "                tmp = tmp.replace(\"ans1\",    f\"ans{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"qid1\",    f\"qid{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"langq1\",  f\"langq{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"comment1\", f\"comment{placeholder_number}\")\n",
    "                title_el.text = tmp\n",
    "\n",
    "            question_el = new_sq.find(\"question\")\n",
    "            if question_el is not None and question_el.text:\n",
    "                tmp = question_el.text\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_QUESTION_1\", f\"PLACEHOLDER_QUESTION_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_ANSWER_1\",   f\"PLACEHOLDER_ANSWER_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_CONTEXT_1\",  f\"PLACEHOLDER_CONTEXT_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_ID_1\",        f\"PLACEHOLDER_ID_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_LANG_1\",      f\"PLACEHOLDER_LANG_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"ans1\",                   f\"ans{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"qid1\",                   f\"qid{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"langq1\",                 f\"langq{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"comment1\",               f\"comment{placeholder_number}\")\n",
    "                question_el.text = tmp\n",
    "\n",
    "            all_new_subquestion_rows.append(new_sq)\n",
    "\n",
    "        #\n",
    "        # (D) Clone all answers for these QIDs\n",
    "        #\n",
    "        for old_qid in original_qid_list:\n",
    "            new_qid = old_qid_to_new_qid[old_qid]\n",
    "            relevant_answers = [\n",
    "                row for row in answer_rows.findall(\"row\")\n",
    "                if row.find(\"qid\").text == old_qid\n",
    "            ]\n",
    "            for ansrow in relevant_answers:\n",
    "                new_ans = ET.fromstring(ET.tostring(ansrow))\n",
    "                new_ans.find(\"qid\").text = new_qid\n",
    "\n",
    "                # placeholders in <answer> text\n",
    "                ans_text_el = new_ans.find(\"answer\")\n",
    "                if ans_text_el is not None and ans_text_el.text:\n",
    "                    tmp = ans_text_el.text\n",
    "                    tmp = tmp.replace(\"PLACEHOLDER_QUESTION_1\", f\"PLACEHOLDER_QUESTION_{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"PLACEHOLDER_ANSWER_1\",   f\"PLACEHOLDER_ANSWER_{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"PLACEHOLDER_CONTEXT_1\",  f\"PLACEHOLDER_CONTEXT_{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"PLACEHOLDER_ID_1\",        f\"PLACEHOLDER_ID_{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"PLACEHOLDER_LANG_1\",      f\"PLACEHOLDER_LANG_{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"ans1\",                   f\"ans{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"qid1\",                   f\"qid{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"langq1\",                 f\"langq{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"comment1\",               f\"comment{placeholder_number}\")\n",
    "                    ans_text_el.text = tmp\n",
    "\n",
    "                all_new_answer_rows.append(new_ans)\n",
    "\n",
    "        #\n",
    "        # (E) Clone question_attributes for these QIDs (e.g. random_order, hidden, etc.)\n",
    "        #\n",
    "        if qattr_rows is not None:\n",
    "            for old_qid in original_qid_list:\n",
    "                new_qid = old_qid_to_new_qid[old_qid]\n",
    "                # find all attribute rows for old_qid\n",
    "                relevant_attrs = [\n",
    "                    row for row in qattr_rows.findall(\"row\")\n",
    "                    if row.find(\"qid\").text == old_qid\n",
    "                ]\n",
    "                for attrrow in relevant_attrs:\n",
    "                    new_attr = ET.fromstring(ET.tostring(attrrow))\n",
    "                    # update the qid to the new question\n",
    "                    new_attr.find(\"qid\").text = new_qid\n",
    "                    all_new_qattr_rows.append(new_attr)\n",
    "\n",
    "        #\n",
    "        # (F) Bump GID for the next group and re-init QIDs\n",
    "        #\n",
    "        gid_counter += 1\n",
    "\n",
    "        # Re-init the QID mapping so each iteration reuses the old QIDs \n",
    "        # (that way every new copy references brand-new QIDs).\n",
    "        old_qid_to_new_qid = {}\n",
    "        tmp_qid_counter = qid_counter\n",
    "        for old_qid in sorted(original_qid_list):\n",
    "            old_qid_to_new_qid[old_qid] = str(tmp_qid_counter)\n",
    "            tmp_qid_counter += 1\n",
    "        qid_counter = tmp_qid_counter\n",
    "\n",
    "    #\n",
    "    # 6) Append all newly created rows\n",
    "    #\n",
    "    for elem in all_new_group_rows:\n",
    "        group_rows.append(elem)\n",
    "    for elem in all_new_question_rows:\n",
    "        question_rows.append(elem)\n",
    "    for elem in all_new_subquestion_rows:\n",
    "        subquestion_rows.append(elem)\n",
    "    for elem in all_new_answer_rows:\n",
    "        answer_rows.append(elem)\n",
    "\n",
    "    # If question_attributes exist, append new ones too\n",
    "    if qattr_rows is not None:\n",
    "        for elem in all_new_qattr_rows:\n",
    "            qattr_rows.append(elem)\n",
    "\n",
    "    #\n",
    "    # 7) Write the final .lss to disk\n",
    "    #\n",
    "    tree.write(output_lss, encoding=\"utf-8\", xml_declaration=True)\n",
    "    print(f\"Done! Created {additional_copies} new copy/copies of group {original_group_id}.\")\n",
    "    #print(f\"Saved to: {output_lss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from markdown import markdown\n",
    "import re\n",
    "\n",
    "def convert_markdown_to_html_with_target_blank(df, markdown_column, html_column):\n",
    "    \"\"\"\n",
    "    Converts Markdown content in a DataFrame column to HTML and ensures links open in a new tab.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing chatbot answers.\n",
    "        markdown_column (str): The name of the column with Markdown content.\n",
    "        html_column (str): The name of the column where the HTML output will be stored.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with the HTML content.\n",
    "    \"\"\"\n",
    "    # Check if the markdown_column exists in the DataFrame\n",
    "    if markdown_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{markdown_column}' not found in the DataFrame.\")\n",
    "    \n",
    "    def add_target_blank_to_links(html):\n",
    "        # Regex to find all <a> tags and add target=\"_blank\"\n",
    "        return re.sub(r'(<a href=\"[^\"]+\")', r'\\1 target=\"_blank\"', html)\n",
    "    \n",
    "    # Convert Markdown to HTML and add target=\"_blank\" to links\n",
    "    df[html_column] = df[markdown_column].apply(\n",
    "        lambda x: add_target_blank_to_links(markdown(x)) if isinstance(x, str) else x\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_context_links(df, context_column, output_column, no_context_text):\n",
    "    \"\"\"\n",
    "    Extracts all links after 'Information taken from:' and formats them as a simple HTML list.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing context data.\n",
    "        context_column (str): Column name containing the chatbot context.\n",
    "        output_column (str): Column name to store the formatted HTML list.\n",
    "        no_context_text (str): Text to display if no context\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with HTML-formatted links.\n",
    "    \"\"\"\n",
    "    def extract_links(context):\n",
    "        # Extract all links after \"Information taken from:\"\n",
    "        matches = re.findall(r'Information taken from:(https?://[^\\s]+)', context)\n",
    "        if matches:\n",
    "            # Format each link into an HTML list item\n",
    "            links_html = ''.join(f'<li><a href=\"{link}\" target=\"_blank\">{link}</a></li>' for link in matches)\n",
    "            return f\"<ul>{links_html}</ul>\"\n",
    "        return f\"<p>{no_context_text}</p>\"\n",
    "\n",
    "    # Apply the transformation\n",
    "    df[output_column] = df[context_column].apply(\n",
    "        lambda x: extract_links(x) if isinstance(x, str) else f\"<p>{no_context_text}</p>\"\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from random import seed as py_seed\n",
    "\n",
    "def build_shuffled_questions_and_clone(\n",
    "    df_en: pd.DataFrame,\n",
    "    df_de: pd.DataFrame,\n",
    "    input_lss: str,\n",
    "    output_lss: str,\n",
    "    original_group_id: str,\n",
    "    gid_start: int,\n",
    "    qid_start: int,\n",
    "    questions_per_group: int,\n",
    "    shuffle_seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Takes two DataFrames (English + German) which already have columns like:\n",
    "         - english_question_text_q, chatbot_answer_en_html, formatted_context_en_html, question_id_q, question_language_q\n",
    "         - german_question_text_q,  chatbot_answer_de_html, formatted_context_de_html, question_id_q, question_language_q\n",
    "\n",
    "    2) Renames/standardizes these columns into:\n",
    "         [question_text, answer_html, context_html, question_id, language]\n",
    "\n",
    "    3) Stacks them into one DataFrame (df_together), each row = a question with:\n",
    "         question_text, answer_html, context_html, question_id, language\n",
    "\n",
    "    4) If shuffle_seed != None shuffles df_together using a fixed seed.\n",
    "\n",
    "    5) Calls the existing add_question_groups(...) function to replicate your .lss\n",
    "       so that the total # of groups = len(df_together). (One per row in df_together.)\n",
    "\n",
    "    6) Finally, parses that newly generated .lss and replaces the placeholders:\n",
    "         PLACEHOLDER_QUESTION_i  => df_together.iloc[i-1][\"question_text\"]\n",
    "         PLACEHOLDER_ANSWER_i    => df_together.iloc[i-1][\"answer_html\"]\n",
    "         PLACEHOLDER_CONTEXT_i   => df_together.iloc[i-1][\"context_html\"]\n",
    "         PLACEHOLDER_ID_i        => df_together.iloc[i-1][\"question_id\"]\n",
    "         PLACEHOLDER_LANG_i      => df_together.iloc[i-1][\"language\"]\n",
    "\n",
    "       for i in 1..len(df_together). The result is saved back to output_lss.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The final stacked & shuffled DataFrame (df_together).\n",
    "                      (For logging/inspection. The placeholders are replaced in the .lss on disk.)\n",
    "    \"\"\"\n",
    "    # --- 1) Standardize columns for df_en\n",
    "    df_en_copy = df_en.copy()\n",
    "    df_en_copy[\"question_text\"] = df_en_copy[\"english_question_text_q\"]\n",
    "    df_en_copy[\"answer_html\"]   = df_en_copy[\"chatbot_answer_en_html\"]\n",
    "    df_en_copy[\"context_html\"]  = df_en_copy[\"formatted_context_en_html\"]\n",
    "    df_en_copy[\"question_id\"]   = df_en_copy[\"question_id_q\"]\n",
    "    df_en_copy[\"language\"]      = df_en_copy[\"question_language_q\"]\n",
    "\n",
    "    # Keep only the unified columns\n",
    "    df_en_final = df_en_copy[[\"question_text\", \"answer_html\", \"context_html\", \"question_id\", \"language\"]]\n",
    "\n",
    "    # --- 2) Standardize columns for df_de\n",
    "    df_de_copy = df_de.copy()\n",
    "    df_de_copy[\"question_text\"] = df_de_copy[\"german_question_text_q\"]\n",
    "    df_de_copy[\"answer_html\"]   = df_de_copy[\"chatbot_answer_de_html\"]\n",
    "    df_de_copy[\"context_html\"]  = df_de_copy[\"formatted_context_de_html\"]\n",
    "    df_de_copy[\"question_id\"]   = df_de_copy[\"question_id_q\"]\n",
    "    df_de_copy[\"language\"]      = df_de_copy[\"question_language_q\"]\n",
    "\n",
    "    df_de_final = df_de_copy[[\"question_text\", \"answer_html\", \"context_html\", \"question_id\", \"language\"]]\n",
    "\n",
    "    # --- 3) Concatenate into one df_together\n",
    "    df_together = pd.concat([df_en_final, df_de_final], ignore_index=True)\n",
    "\n",
    "    # --- 4) Shuffle\n",
    "    if shuffle_seed is not None:\n",
    "        df_together = df_together.sample(frac=1, random_state=shuffle_seed).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # --- 5) The total # of groups to produce = len(df_together)\n",
    "    new_group_count = len(df_together)\n",
    "\n",
    "    # --- 6) Clone the group in the .lss (using your existing add_question_groups function)\n",
    "    add_question_groups(\n",
    "        input_lss=input_lss,\n",
    "        output_lss=output_lss,\n",
    "        original_group_id=original_group_id,\n",
    "        new_group_count=new_group_count,\n",
    "        gid_start=gid_start,\n",
    "        qid_start=qid_start,\n",
    "        questions_per_group=questions_per_group\n",
    "    )\n",
    "\n",
    "    # --- 7) Now fill in the newly created placeholders with the actual data from df_together\n",
    "    _fill_placeholders_with_data(output_lss, df_together)\n",
    "\n",
    "    return df_together\n",
    "\n",
    "\n",
    "def _fill_placeholders_with_data(output_lss: str, df_together: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Internal helper to parse the newly generated LSS (output_lss) \n",
    "    and replace placeholder text with real data from df_together.\n",
    "\n",
    "    For i in [1..N], we replace:\n",
    "      PLACEHOLDER_QUESTION_{i} -> df_together.iloc[i-1][\"question_text\"]\n",
    "      PLACEHOLDER_ANSWER_{i}   -> df_together.iloc[i-1][\"answer_html\"]\n",
    "      PLACEHOLDER_CONTEXT_{i}  -> df_together.iloc[i-1][\"context_html\"]\n",
    "      PLACEHOLDER_ID_{i}       -> df_together.iloc[i-1][\"question_id\"]\n",
    "      PLACEHOLDER_LANG_{i}     -> df_together.iloc[i-1][\"language\"]\n",
    "\n",
    "    The function overwrites the same .lss on disk.\n",
    "    \"\"\"\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    # Load the .lss we just created\n",
    "    tree = ET.parse(output_lss)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # We define a small helper that does multi-line text replacement:\n",
    "    def replace_placeholders_in_text(original_text):\n",
    "        if not original_text:\n",
    "            return original_text\n",
    "        new_text = original_text\n",
    "\n",
    "        # **Key change**: we do replacements in descending order:\n",
    "        #   i = len(df_together), len(df_together)-1, ..., 2, 1\n",
    "        for i in range(len(df_together), 0, -1):\n",
    "            row = df_together.iloc[i - 1]\n",
    "            idx_str = str(i)\n",
    "\n",
    "            new_text = new_text.replace(f\"PLACEHOLDER_QUESTION_{idx_str}\", str(row[\"question_text\"]))\n",
    "            new_text = new_text.replace(f\"PLACEHOLDER_ANSWER_{idx_str}\",   str(row[\"answer_html\"]))\n",
    "            new_text = new_text.replace(f\"PLACEHOLDER_CONTEXT_{idx_str}\",  str(row[\"context_html\"]))\n",
    "            new_text = new_text.replace(f\"PLACEHOLDER_ID_{idx_str}\",       str(row[\"question_id\"]))\n",
    "            new_text = new_text.replace(f\"PLACEHOLDER_LANG_{idx_str}\",     str(row[\"language\"]))\n",
    "\n",
    "        return new_text\n",
    "\n",
    "    # Replace in <groups> -> <rows> -> <row> -> <description>\n",
    "    groups_elem = root.find(\"./groups\")\n",
    "    if groups_elem is not None:\n",
    "        rows_el = groups_elem.find(\"rows\")\n",
    "        if rows_el is not None:\n",
    "            for row_el in rows_el.findall(\"row\"):\n",
    "                desc_el = row_el.find(\"description\")\n",
    "                if desc_el is not None:\n",
    "                    desc_el.text = replace_placeholders_in_text(desc_el.text)\n",
    "\n",
    "    # Replace in <questions> -> <rows> -> <row> -> <question>/<title>\n",
    "    questions_elem = root.find(\"./questions\")\n",
    "    if questions_elem is not None:\n",
    "        qrows_el = questions_elem.find(\"rows\")\n",
    "        if qrows_el is not None:\n",
    "            for row_el in qrows_el.findall(\"row\"):\n",
    "                question_el = row_el.find(\"question\")\n",
    "                if question_el is not None:\n",
    "                    question_el.text = replace_placeholders_in_text(question_el.text)\n",
    "\n",
    "                title_el = row_el.find(\"title\")\n",
    "                if title_el is not None:\n",
    "                    title_el.text = replace_placeholders_in_text(title_el.text)\n",
    "\n",
    "    # Replace in <subquestions> -> <rows> -> <row> -> <question>/<title>\n",
    "    subquestions_elem = root.find(\"./subquestions\")\n",
    "    if subquestions_elem is not None:\n",
    "        sqrows_el = subquestions_elem.find(\"rows\")\n",
    "        if sqrows_el is not None:\n",
    "            for row_el in sqrows_el.findall(\"row\"):\n",
    "                question_el = row_el.find(\"question\")\n",
    "                if question_el is not None:\n",
    "                    question_el.text = replace_placeholders_in_text(question_el.text)\n",
    "\n",
    "                title_el = row_el.find(\"title\")\n",
    "                if title_el is not None:\n",
    "                    title_el.text = replace_placeholders_in_text(title_el.text)\n",
    "\n",
    "    # Replace in <answers> -> <rows> -> <row> -> <answer>\n",
    "    answers_elem = root.find(\"./answers\")\n",
    "    if answers_elem is not None:\n",
    "        arows_el = answers_elem.find(\"rows\")\n",
    "        if arows_el is not None:\n",
    "            for row_el in arows_el.findall(\"row\"):\n",
    "                ans_el = row_el.find(\"answer\")\n",
    "                if ans_el is not None:\n",
    "                    ans_el.text = replace_placeholders_in_text(ans_el.text)\n",
    "\n",
    "    # Finally, write back to the same .lss\n",
    "    tree.write(output_lss, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german length: 18 english length: 18\n",
      "Done! Created 35 new copy/copies of group 99600.\n"
     ]
    }
   ],
   "source": [
    "# English dataset\n",
    "df_en = pd.read_csv(\"../../data/final_merged_dataset_short_en_2.csv\")\n",
    "df_en = convert_markdown_to_html_with_target_blank(\n",
    "    df_en, \n",
    "    markdown_column=\"chatbot_answer_en\", \n",
    "    html_column=\"chatbot_answer_en_html\")\n",
    "\n",
    "df_en = extract_context_links(\n",
    "    df_en, \n",
    "    context_column=\"chatbot_context_en\", \n",
    "    output_column=\"formatted_context_en_html\", \n",
    "    no_context_text=\"No context was used by the chatbot.\")\n",
    "# German dataset\n",
    "df_de = pd.read_csv(\"../../data/final_merged_dataset_short_de_2.csv\")\n",
    "df_de = convert_markdown_to_html_with_target_blank(\n",
    "    df_de, \n",
    "    markdown_column=\"chatbot_answer_de\", \n",
    "    html_column=\"chatbot_answer_de_html\")\n",
    "df_de = extract_context_links(\n",
    "    df_de, \n",
    "    context_column=\"chatbot_context_de\", \n",
    "    output_column=\"formatted_context_de_html\", \n",
    "    no_context_text=\"Der Chatbot hat keinen Kontext verwendet.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------\n",
    "input_lss = \"../../../survey/eval/eval_test_2.lss\"      # Path to your existing .lss file\n",
    "output_lss = \"../../data/human_eval/survey_output.lss\"  # Path for the new .lss file\n",
    "\n",
    "original_group_id = \"99600\"    # The group we want to clone\n",
    "questions_per_group = 3        # How many questions untill we increment randomgroup\n",
    "\n",
    "# Starting offset for new GIDs and QIDs:\n",
    "gid_start = 200000\n",
    "qid_start = 300000\n",
    "\n",
    "print(f\"german length: {len(df_de)}\", f\"english length: {len(df_en)}\")\n",
    "\n",
    "# 2) Now call our new function:\n",
    "df_final = build_shuffled_questions_and_clone(\n",
    "    df_en=df_en,\n",
    "    df_de=df_de,\n",
    "    input_lss=input_lss,\n",
    "    output_lss=output_lss,\n",
    "    original_group_id=original_group_id,\n",
    "    gid_start=gid_start,\n",
    "    qid_start=qid_start,\n",
    "    questions_per_group=questions_per_group,\n",
    "    shuffle_seed=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of draws to complete 3 collections of 7 items: 39.5895\n"
     ]
    }
   ],
   "source": [
    "# using equastion from https://mat.uab.cat/matmat_antiga/PDFv2014/v2014n02.pdf [(Ferrante & Saltalamacchia, 2006)]\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "from math import factorial, exp\n",
    "\n",
    "def Sm(t, m):\n",
    "    \"\"\"\n",
    "    Compute the sum S_m(t) = \\sum_{k=0}^{m-1} (t^k / k!)\n",
    "    \"\"\"\n",
    "    return sum((t**k) / factorial(k) for k in range(m))\n",
    "\n",
    "def integrand(t, N, m):\n",
    "    \"\"\"\n",
    "    The integrand for the expected number of draws.\n",
    "    \"\"\"\n",
    "    sm_t = Sm(t, m)\n",
    "    return 1 - (1 - sm_t * exp(-t))**N\n",
    "\n",
    "def expected_draws(N, m):\n",
    "    \"\"\"\n",
    "    Calculate the expected number of draws to complete m collections of N items.\n",
    "\n",
    "    Parameters:\n",
    "        N: int - Number of unique items (coupons).\n",
    "        m: int - Number of collections.\n",
    "\n",
    "    Returns:\n",
    "        float - Expected number of draws.\n",
    "    \"\"\"\n",
    "    result, _ = quad(lambda t: integrand(t, N, m), 0, np.inf)\n",
    "    return N * result\n",
    "\n",
    "# Example usage\n",
    "N = 7  # Number of unique items\n",
    "m = 3  # Number of times each item is needed\n",
    "\n",
    "expected = expected_draws(N, m)\n",
    "print(f\"Expected number of draws to complete {m} collections of {N} items: {expected:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[X] for N=3, m=2 is approximately 9.63888888888889\n"
     ]
    }
   ],
   "source": [
    "import mpmath as mp\n",
    "\n",
    "def S_m(t, m):\n",
    "    \"\"\"\n",
    "    Compute S_m(t) = sum_{k=0}^{m-1} [t^k / k!].\n",
    "    \"\"\"\n",
    "    s = mp.mpf('0')\n",
    "    for k in range(m):\n",
    "        s += (t**k) / mp.factorial(k)\n",
    "    return s\n",
    "\n",
    "def expected_time_m_collections(N, m):\n",
    "    \"\"\"\n",
    "    Return the expected number of coupons needed to collect\n",
    "    m complete sets of N equally likely coupon types.\n",
    "\n",
    "    Formula:\n",
    "      E[X] = N * ∫_{0 to ∞} [ 1 - (1 - S_m(t)*e^-t)^N ] dt\n",
    "    \"\"\"\n",
    "    # Define the integrand for the integral\n",
    "    def integrand(t):\n",
    "        val = S_m(t, m) * mp.e**(-t)\n",
    "        return 1 - (1 - val)**N\n",
    "\n",
    "    # Perform the integral from 0 to ∞\n",
    "    result = N * mp.quad(integrand, [0, mp.inf])\n",
    "    return result\n",
    "\n",
    "# Example usage for small values of N, m:\n",
    "m_example = 2\n",
    "N_example = 3\n",
    "est = expected_time_m_collections(N_example, m_example)\n",
    "print(f\"E[X] for N={N_example}, m={m_example} is approximately {est}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
