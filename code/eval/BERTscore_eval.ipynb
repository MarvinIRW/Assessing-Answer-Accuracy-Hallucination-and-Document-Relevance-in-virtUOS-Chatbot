{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marvin/venv/bertscore_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:41<00:00, 20.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 41.30 seconds, 0.80 sentences/sec\n",
      "[DE] System-level BERTScore F1: 0.667\n",
      "[DE] BERTScore hash code: bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.47.1)\n",
      "\n",
      "BERTScore results saved to: /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-virtUOS-Chatbot/code/eval/../../data/eval/bertscore_evaluation_de.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:46<00:00, 53.09s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 104.98 seconds, 0.31 sentences/sec\n",
      "[EN] System-level BERTScore F1: 0.845\n",
      "[EN] BERTScore hash code: roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.47.1)\n",
      "\n",
      "BERTScore results saved to: /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-virtUOS-Chatbot/code/eval/../../data/eval/bertscore_evaluation_en.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bert_score import score\n",
    "\n",
    "def compute_bertscore(\n",
    "    df: pd.DataFrame,\n",
    "    reference_col: str,\n",
    "    hypothesis_col: str,\n",
    "    question_id_col: str,\n",
    "    language: str,\n",
    "    output_csv_path: str,\n",
    "    mean_csv_path=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes BERTScore for each row in `df`, comparing `hypothesis_col` to\n",
    "    `reference_col` in the specified `language`. Stores the row-level \n",
    "    precision, recall, and F1 in a DataFrame, and writes it to `output_csv_path`.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing references & hypotheses\n",
    "        reference_col (str): Name of the column with reference/human texts\n",
    "        hypothesis_col (str): Name of the column with system/hypothesis texts\n",
    "        question_id_col (str): Name of the column with question IDs\n",
    "        language (str): Language code for BERTScore (\"en\", \"de\", etc.)\n",
    "        output_csv_path (str): Path to save the resulting DataFrame\n",
    "        mean_eval_path (str): Path to save the mean evaluation results\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing row-level BERTScore P, R, F1\n",
    "    \"\"\"\n",
    "    # Convert references/hypotheses to lists\n",
    "    references = df[reference_col].astype(str).tolist()\n",
    "    hypotheses = df[hypothesis_col].astype(str).tolist()\n",
    "    \n",
    "    # Ensure matching lengths\n",
    "    assert len(references) == len(hypotheses), \"Mismatch in # of references vs. hypotheses\"\n",
    "    \n",
    "    # Run BERTScore\n",
    "    (P, R, F1), bert_hash = score(\n",
    "        cands=hypotheses, \n",
    "        refs=references, \n",
    "        lang=language,\n",
    "        verbose=True,\n",
    "        return_hash=True\n",
    "    )\n",
    "    \n",
    "    # Build results DataFrame\n",
    "    bert_df = pd.DataFrame()\n",
    "    bert_df[question_id_col] = df[question_id_col].values\n",
    "    bert_df['BERTScore_P'] = P.tolist()\n",
    "    bert_df['BERTScore_R'] = R.tolist()\n",
    "    bert_df['BERTScore_F1'] = F1.tolist()\n",
    "\n",
    "    # Print system-level (macro) F1\n",
    "    system_f1_mean = bert_df['BERTScore_F1'].mean()\n",
    "    print(f\"[{language.upper()}] System-level BERTScore F1: {system_f1_mean:.3f}\")\n",
    "    print(f\"[{language.upper()}] BERTScore hash code: {bert_hash}\\n\")\n",
    "\n",
    "    # Save to CSV\n",
    "    bert_df.to_csv(output_csv_path, index=False, quoting=1)\n",
    "    print(f\"BERTScore results saved to: {output_csv_path}\")\n",
    "\n",
    "    if mean_csv_path is not None and os.path.exists(mean_csv_path) and language is not None:\n",
    "        # save the mean evaluation scores\n",
    "        mean_eval = pd.read_csv(mean_csv_path)\n",
    "        # add row to the mean_eval df\n",
    "        if f\"BERTScore_F1_{language}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"BERTScore_F1_{language}\", \"value\": system_f1_mean}])], ignore_index=True)\n",
    "        mean_eval.to_csv(mean_csv_path, index=False)\n",
    "\n",
    "    return bert_df\n",
    "\n",
    "# 1. Load the data\n",
    "cwd = os.getcwd()\n",
    "    \n",
    "csv_path_de = os.path.join(cwd, '../../data/short_dataset_de.csv')\n",
    "csv_path_en = os.path.join(cwd, '../../data/short_dataset_en.csv')\n",
    "mean_csv_path = os.path.join(cwd, '../../data/eval/mean_eval.csv')\n",
    "    \n",
    "df_de = pd.read_csv(csv_path_de)\n",
    "df_en = pd.read_csv(csv_path_en)\n",
    "\n",
    "# # 2. (Optional) limit for demonstration\n",
    "# df_de = df_de.head(18).copy()\n",
    "# df_en = df_en.head(18).copy()\n",
    "\n",
    "# 3. Compute BERTScore for German\n",
    "output_csv_de = os.path.join(cwd, '../../data/eval/bertscore_evaluation_de.csv')\n",
    "bert_df_de = compute_bertscore(\n",
    "    df=df_de,\n",
    "    reference_col='human_answer_de',\n",
    "    hypothesis_col='chatbot_answer_de',\n",
    "    question_id_col='question_id_q',\n",
    "    language='de',  # BERTScore language code\n",
    "    output_csv_path=output_csv_de,\n",
    "    mean_csv_path=mean_csv_path\n",
    ")\n",
    "\n",
    "# 4. Compute BERTScore for English\n",
    "output_csv_en = os.path.join(cwd, '../../data/eval/bertscore_evaluation_en.csv')\n",
    "bert_df_en = compute_bertscore(\n",
    "    df=df_en,\n",
    "    reference_col='human_answer_en',\n",
    "    hypothesis_col='chatbot_answer_en',\n",
    "    question_id_col='question_id_q',\n",
    "    language='en',\n",
    "    output_csv_path=output_csv_en,\n",
    "    mean_csv_path=mean_csv_path\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertscore_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
