{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# Create a new df for saving the mean evaluation scores\n",
    "cwd = os.getcwd()\n",
    "csv_path = os.path.join(cwd, '../../data/eval/mean_eval.csv')\n",
    "df = pd.DataFrame(columns=['metric', 'value'])\n",
    "df.to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence-level BLEU for /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-a-RAG-Based-Chatbot/code/eval/../../data/eval/bleu_sentence_evaluation_de.csv: 5.62\n",
      "BLEU signature: nrefs:1|case:mixed|eff:yes|tok:13a|smooth:exp|version:2.4.3\n",
      "Saved BLEU results to: /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-a-RAG-Based-Chatbot/code/eval/../../data/eval/bleu_sentence_evaluation_de.csv\n",
      "\n",
      "Average sentence-level BLEU for /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-a-RAG-Based-Chatbot/code/eval/../../data/eval/bleu_sentence_evaluation_en.csv: 6.92\n",
      "BLEU signature: nrefs:1|case:mixed|eff:yes|tok:13a|smooth:exp|version:2.4.3\n",
      "Saved BLEU results to: /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-a-RAG-Based-Chatbot/code/eval/../../data/eval/bleu_sentence_evaluation_en.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2627/3371442123.py:84: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"BLEU_{dataset_lang}\", \"value\": avg_bleu}])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "def compute_sentence_bleu(\n",
    "    df: pd.DataFrame,\n",
    "    reference_col: str,\n",
    "    hypothesis_col: str,\n",
    "    question_id_col: str,\n",
    "    output_csv_path: str,\n",
    "    mean_csv_path=None,\n",
    "    dataset_lang=None,\n",
    "    bleu_metric=None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute sentence-level BLEU for each row in the given DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The data containing references & hypotheses.\n",
    "        reference_col (str): Name of the column with reference/human texts.\n",
    "        hypothesis_col (str): Name of the column with system/hypothesis texts.\n",
    "        question_id_col (str): Name of the column with question IDs (for alignment).\n",
    "        output_csv_path (str): Where to save the resulting DataFrame.\n",
    "        mean_csv_path (str): Where to find the mean evaluation scores.\n",
    "        dataset_lang (str): The language of the dataset (de or en).\n",
    "        bleu_metric (sacrebleu.metrics.BLEU, optional): \n",
    "            BLEU object to use. If None, create a new one with effective_order=True.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame containing sentence-level BLEU results.\n",
    "    \"\"\"\n",
    "    if bleu_metric is None:\n",
    "        bleu_metric = BLEU(effective_order=True)\n",
    "    \n",
    "    references = df[reference_col].astype(str).tolist()\n",
    "    hypotheses = df[hypothesis_col].astype(str).tolist()\n",
    "    \n",
    "    bleu_scores = []\n",
    "    bleu_1gram_precision = []\n",
    "    bleu_2gram_precision = []\n",
    "    bleu_3gram_precision = []\n",
    "    bleu_4gram_precision = []\n",
    "    bleu_bp = []\n",
    "    bleu_sys_len = []\n",
    "    bleu_ref_len = []\n",
    "\n",
    "    for hyp, ref in zip(hypotheses, references):\n",
    "        result = bleu_metric.sentence_score(hyp, [ref])\n",
    "        bleu_scores.append(result.score)  # BLEU on a 0â€“100 scale\n",
    "        bleu_1gram_precision.append(result.precisions[0])\n",
    "        bleu_2gram_precision.append(result.precisions[1])\n",
    "        bleu_3gram_precision.append(result.precisions[2])\n",
    "        bleu_4gram_precision.append(result.precisions[3])\n",
    "        bleu_bp.append(result.bp)\n",
    "        bleu_sys_len.append(result.sys_len)\n",
    "        bleu_ref_len.append(result.ref_len)\n",
    "\n",
    "    # Build a new DataFrame of BLEU results\n",
    "    bleu_df = pd.DataFrame()\n",
    "    bleu_df[question_id_col] = df[question_id_col].values\n",
    "    bleu_df['BLEU'] = bleu_scores\n",
    "    bleu_df['BLEU_1gram_prec'] = bleu_1gram_precision\n",
    "    bleu_df['BLEU_2gram_prec'] = bleu_2gram_precision\n",
    "    bleu_df['BLEU_3gram_prec'] = bleu_3gram_precision\n",
    "    bleu_df['BLEU_4gram_prec'] = bleu_4gram_precision\n",
    "    bleu_df['BLEU_BP'] = bleu_bp\n",
    "    bleu_df['BLEU_sys_len'] = bleu_sys_len\n",
    "    bleu_df['BLEU_ref_len'] = bleu_ref_len\n",
    "    \n",
    "    # Compute the average (macro) of sentence-level BLEU\n",
    "    avg_bleu = bleu_df['BLEU'].mean()\n",
    "    print(f\"Average sentence-level BLEU for {output_csv_path}: {avg_bleu:.2f}\")\n",
    "    print(f\"BLEU signature: {bleu_metric.get_signature()}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    bleu_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Saved BLEU results to: {output_csv_path}\\n\")\n",
    "\n",
    "    if mean_csv_path is not None and os.path.exists(mean_csv_path) and dataset_lang is not None:\n",
    "        # save the mean evaluation scores\n",
    "        mean_eval = pd.read_csv(mean_csv_path)\n",
    "        # add row to the mean_eval df\n",
    "        if f\"BLEU_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"BLEU_{dataset_lang}\", \"value\": avg_bleu}])], ignore_index=True)\n",
    "        mean_eval.to_csv(mean_csv_path, index=False)\n",
    "    \n",
    "    return bleu_df\n",
    "\n",
    "# 1. Load CSVs\n",
    "cwd = os.getcwd()\n",
    "csv_path_de = os.path.join(cwd, '../../data/final_merged_dataset_short_de.csv')\n",
    "csv_path_en = os.path.join(cwd, '../../data/final_merged_dataset_short_en.csv')\n",
    "csv_path_mean = os.path.join(cwd, '../../data/eval/mean_eval.csv')\n",
    "df_original_de = pd.read_csv(csv_path_de)\n",
    "df_original_en = pd.read_csv(csv_path_en)\n",
    "# 2. (Optional) Subset for demonstration\n",
    "df_de = df_original_de.head(18).copy()\n",
    "df_en = df_original_en.head(18).copy()\n",
    "# 3. Evaluate German\n",
    "output_csv_de = os.path.join(cwd, '../../data/eval/bleu_sentence_evaluation_de.csv')\n",
    "bleu_df_de = compute_sentence_bleu(\n",
    "    df=df_de,\n",
    "    reference_col='human_answer_de',\n",
    "    hypothesis_col='chatbot_answer_de',\n",
    "    question_id_col='question_id_q',\n",
    "    output_csv_path=output_csv_de,\n",
    "    mean_csv_path=csv_path_mean,\n",
    "    dataset_lang='de'\n",
    ")\n",
    "# 4. Evaluate English\n",
    "output_csv_en = os.path.join(cwd, '../../data/eval/bleu_sentence_evaluation_en.csv')\n",
    "bleu_df_en = compute_sentence_bleu(\n",
    "    df=df_en,\n",
    "    reference_col='human_answer_en',\n",
    "    hypothesis_col='chatbot_answer_en',\n",
    "    question_id_col='question_id_q',\n",
    "    output_csv_path=output_csv_en,\n",
    "    mean_csv_path=csv_path_mean,\n",
    "    dataset_lang='en'\n",
    ")\n",
    "# Now you have two separate CSVs with detailed BLEU metrics for DE and EN.\n",
    "# If you prefer, you can also merge `bleu_df_de` and `bleu_df_en` into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score is: BLEU = 25.58 53.2/27.6/19.3/15.1 (BP = 1.000 ratio = 1.442 hyp_len = 421 ref_len = 292)\n",
      "The BLEU signature is: nrefs:18|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.3\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Load CSV\n",
    "cwd = os.getcwd()\n",
    "csv_path = os.path.join(cwd, '../../data/final_merged_dataset_short_de.csv')\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# If you only want a subset of rows, make a real copy:\n",
    "data_short = data.head(18).copy()\n",
    "\n",
    "# 2. Get the reference and system lists\n",
    "human_answers_refs = data_short['human_answer_de'].astype(str).tolist()\n",
    "# 'human_answers_refs' needs to be list of lists\n",
    "human_answers_refs = [[x] for x in human_answers_refs]\n",
    "chatbot_answers_sys = data_short['chatbot_answer_de'].astype(str).tolist()\n",
    "\n",
    "# 3. Calculate BLEU\n",
    "bleu = BLEU()\n",
    "score = bleu.corpus_score(chatbot_answers_sys, human_answers_refs)\n",
    "\n",
    "# 4. Print the score\n",
    "print(f\"The BLEU score is: {score}\")\n",
    "print(f\"The BLEU signature is: {bleu.get_signature()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
