{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System-level average (macro) F1 scores for /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-a-RAG-Based-Chatbot/code/eval/../../data/eval/rouge_evaluation_de.csv:\n",
      "  ROUGE-1: 27.424\n",
      "  ROUGE-2: 11.736\n",
      "  ROUGE-3: 6.745\n",
      "  ROUGE-4: 4.330\n",
      "  ROUGE-L: 24.162\n",
      "  ROUGE-SU4: 11.275\n",
      "  ROUGE-W-1.2: 11.607\n",
      "\n",
      "Saved ROUGE metrics to: /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-a-RAG-Based-Chatbot/code/eval/../../data/eval/rouge_evaluation_de.csv\n",
      "System-level average (macro) F1 scores for /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-a-RAG-Based-Chatbot/code/eval/../../data/eval/rouge_evaluation_en.csv:\n",
      "  ROUGE-1: 34.282\n",
      "  ROUGE-2: 12.627\n",
      "  ROUGE-3: 6.572\n",
      "  ROUGE-4: 4.048\n",
      "  ROUGE-L: 28.490\n",
      "  ROUGE-SU4: 14.230\n",
      "  ROUGE-W-1.2: 13.505\n",
      "\n",
      "Saved ROUGE metrics to: /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-a-RAG-Based-Chatbot/code/eval/../../data/eval/rouge_evaluation_en.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sacrerouge.metrics import Rouge\n",
    "\n",
    "def compute_sentence_rouge(\n",
    "    df: pd.DataFrame,\n",
    "    reference_col: str,\n",
    "    hypothesis_col: str,\n",
    "    question_id_col: str,\n",
    "    output_csv_path: str,\n",
    "    mean_csv_path=None,\n",
    "    dataset_lang=None,\n",
    "    rouge_metric: Rouge = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute sentence-level ROUGE for each row in `df`, comparing `hypothesis_col` \n",
    "    to `reference_col`. Saves the results to `output_csv_path` and returns a \n",
    "    DataFrame with the ROUGE results for each row.\n",
    "    \"\"\"\n",
    "    # If no custom ROUGE object is provided, create a default one\n",
    "    if rouge_metric is None:\n",
    "        rouge_metric = Rouge(\n",
    "            max_ngram=4,\n",
    "            use_porter_stemmer=False,\n",
    "            remove_stopwords=False,\n",
    "            compute_rouge_l=True,\n",
    "            skip_bigram_gap_length=4,\n",
    "            wlcs_weight=1.2\n",
    "        )\n",
    "    \n",
    "    # Lists to store each variant's F1, precision, and recall\n",
    "    r1_f, r1_p, r1_r = [], [], []\n",
    "    r2_f, r2_p, r2_r = [], [], []\n",
    "    r3_f, r3_p, r3_r = [], [], []\n",
    "    r4_f, r4_p, r4_r = [], [], []\n",
    "    rl_f, rl_p, rl_r = [], [], []\n",
    "    rsu4_f, rsu4_p, rsu4_r = [], [], []\n",
    "    rw12_f, rw12_p, rw12_r = [], [], []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        hypothesis = str(row[hypothesis_col])\n",
    "        reference  = str(row[reference_col])\n",
    "        \n",
    "        # Each row can have multiple references, but here we just pass a single-element list\n",
    "        scores = rouge_metric.score(hypothesis, [reference])\n",
    "\n",
    "        # R-1\n",
    "        r1_f.append(scores['rouge-1']['f1'])\n",
    "        r1_p.append(scores['rouge-1']['precision'])\n",
    "        r1_r.append(scores['rouge-1']['recall'])\n",
    "        # R-2\n",
    "        r2_f.append(scores['rouge-2']['f1'])\n",
    "        r2_p.append(scores['rouge-2']['precision'])\n",
    "        r2_r.append(scores['rouge-2']['recall'])\n",
    "        # R-3\n",
    "        r3_f.append(scores['rouge-3']['f1'])\n",
    "        r3_p.append(scores['rouge-3']['precision'])\n",
    "        r3_r.append(scores['rouge-3']['recall'])\n",
    "        # R-4\n",
    "        r4_f.append(scores['rouge-4']['f1'])\n",
    "        r4_p.append(scores['rouge-4']['precision'])\n",
    "        r4_r.append(scores['rouge-4']['recall'])\n",
    "        # R-L\n",
    "        rl_f.append(scores['rouge-l']['f1'])\n",
    "        rl_p.append(scores['rouge-l']['precision'])\n",
    "        rl_r.append(scores['rouge-l']['recall'])\n",
    "        # R-SU4\n",
    "        rsu4_f.append(scores['rouge-su4']['f1'])\n",
    "        rsu4_p.append(scores['rouge-su4']['precision'])\n",
    "        rsu4_r.append(scores['rouge-su4']['recall'])\n",
    "        # R-W-1.2\n",
    "        rw12_f.append(scores['rouge-w-1.2']['f1'])\n",
    "        rw12_p.append(scores['rouge-w-1.2']['precision'])\n",
    "        rw12_r.append(scores['rouge-w-1.2']['recall'])\n",
    "\n",
    "    # Build a new DataFrame\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[question_id_col] = df[question_id_col].values\n",
    "\n",
    "    result_df['ROUGE-1_f'] = r1_f\n",
    "    result_df['ROUGE-1_p'] = r1_p\n",
    "    result_df['ROUGE-1_r'] = r1_r\n",
    "\n",
    "    result_df['ROUGE-2_f'] = r2_f\n",
    "    result_df['ROUGE-2_p'] = r2_p\n",
    "    result_df['ROUGE-2_r'] = r2_r\n",
    "\n",
    "    result_df['ROUGE-3_f'] = r3_f\n",
    "    result_df['ROUGE-3_p'] = r3_p\n",
    "    result_df['ROUGE-3_r'] = r3_r\n",
    "\n",
    "    result_df['ROUGE-4_f'] = r4_f\n",
    "    result_df['ROUGE-4_p'] = r4_p\n",
    "    result_df['ROUGE-4_r'] = r4_r\n",
    "\n",
    "    result_df['ROUGE-L_f'] = rl_f\n",
    "    result_df['ROUGE-L_p'] = rl_p\n",
    "    result_df['ROUGE-L_r'] = rl_r\n",
    "\n",
    "    result_df['ROUGE-SU4_f'] = rsu4_f\n",
    "    result_df['ROUGE-SU4_p'] = rsu4_p\n",
    "    result_df['ROUGE-SU4_r'] = rsu4_r\n",
    "\n",
    "    result_df['ROUGE-W-1.2_f'] = rw12_f\n",
    "    result_df['ROUGE-W-1.2_p'] = rw12_p\n",
    "    result_df['ROUGE-W-1.2_r'] = rw12_r\n",
    "\n",
    "\n",
    "    # Compute macro averages for F1\n",
    "    r1_f_mean   = result_df['ROUGE-1_f'].mean()\n",
    "    r2_f_mean   = result_df['ROUGE-2_f'].mean()\n",
    "    r3_f_mean   = result_df['ROUGE-3_f'].mean()\n",
    "    r4_f_mean   = result_df['ROUGE-4_f'].mean()\n",
    "    rl_f_mean   = result_df['ROUGE-L_f'].mean()\n",
    "    rsu4_f_mean = result_df['ROUGE-SU4_f'].mean()\n",
    "    rw12_f_mean = result_df['ROUGE-W-1.2_f'].mean()\n",
    "\n",
    "    print(f\"System-level average (macro) F1 scores for {output_csv_path}:\")\n",
    "    print(f\"  ROUGE-1: {r1_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-2: {r2_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-3: {r3_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-4: {r4_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-L: {rl_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-SU4: {rsu4_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-W-1.2: {rw12_f_mean:.3f}\\n\")\n",
    "\n",
    "    if mean_csv_path is not None and os.path.exists(mean_csv_path) and dataset_lang is not None:\n",
    "        # save the mean evaluation scores\n",
    "        mean_eval = pd.read_csv(mean_csv_path)\n",
    "        # add row to the mean_eval df\n",
    "        if f\"ROUGE-1_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-1_f_{dataset_lang}\", \"value\": r1_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-2_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-2_f_{dataset_lang}\", \"value\": r2_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-3_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-3_f_{dataset_lang}\", \"value\": r3_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-4_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-4_f_{dataset_lang}\", \"value\": r4_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-L_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-L_f_{dataset_lang}\", \"value\": rl_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-SU4_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-SU4_f_{dataset_lang}\", \"value\": rsu4_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-W-1.2_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-W-1.2_f_{dataset_lang}\", \"value\": rw12_f_mean}])], ignore_index=True)\n",
    "        mean_eval.to_csv(mean_csv_path, index=False)\n",
    "\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(output_csv_path, index=False, quoting=1)\n",
    "    print(\"Saved ROUGE metrics to:\", output_csv_path)\n",
    "    return result_df\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1) Load the CSVs\n",
    "cwd = os.getcwd()\n",
    "# - If you have separate DE and EN data:\n",
    "csv_path_de = os.path.join(cwd, '../../data/final_merged_dataset_short_de.csv')\n",
    "csv_path_en = os.path.join(cwd, '../../data/final_merged_dataset_short_en.csv')\n",
    "mean_csv_path = os.path.join(cwd, '../../data/eval/mean_eval.csv')\n",
    "df_de = pd.read_csv(csv_path_de)\n",
    "df_en = pd.read_csv(csv_path_en)\n",
    "\n",
    "# 2) (Optional) limit to smaller subset for demonstration\n",
    "df_de = df_de.head(18).copy()\n",
    "df_en = df_en.head(18).copy()\n",
    "\n",
    "# 3) Evaluate ROUGE for German\n",
    "output_csv_de = os.path.join(cwd, '../../data/eval/rouge_evaluation_de.csv')\n",
    "rouge_df_de = compute_sentence_rouge(\n",
    "    df=df_de,\n",
    "    reference_col='human_answer_de',\n",
    "    hypothesis_col='chatbot_answer_de',\n",
    "    question_id_col='question_id_q',\n",
    "    output_csv_path=output_csv_de,\n",
    "    mean_csv_path=mean_csv_path,\n",
    "    dataset_lang='de'\n",
    ")\n",
    "\n",
    "# 4) Evaluate ROUGE for English\n",
    "output_csv_en = os.path.join(cwd, '../../data/eval/rouge_evaluation_en.csv')\n",
    "rouge_df_en = compute_sentence_rouge(\n",
    "    df=df_en,\n",
    "    reference_col='human_answer_en',\n",
    "    hypothesis_col='chatbot_answer_en',\n",
    "    question_id_col='question_id_q',\n",
    "    output_csv_path=output_csv_en,\n",
    "    mean_csv_path=mean_csv_path,\n",
    "    dataset_lang='en'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System-level average (macro) F1 scores:\n",
      "  ROUGE-1: 30.829\n",
      "  ROUGE-2: 13.264\n",
      "  ROUGE-3: 7.671\n",
      "  ROUGE-4: 5.136\n",
      "  ROUGE-L: 26.867\n",
      "  ROUGE-SU4: 12.637\n",
      "  ROUGE-W-1.2: 13.246\n",
      "Saved ROUGE metrics to: /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-a-RAG-Based-Chatbot/data/eval/rouge_evaluation_de.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sacrerouge.metrics import Rouge\n",
    "\n",
    "# 1. Load CSV\n",
    "cwd = os.getcwd()\n",
    "csv_path = os.path.join(cwd, 'data/final_merged_dataset_short.csv')\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# If you only want a subset of rows, make a real copy:\n",
    "data_short = data.head(18).copy()\n",
    "\n",
    "# 2. Initialize ROUGE\n",
    "rouge = Rouge(\n",
    "    max_ngram=4,\n",
    "    use_porter_stemmer=False,\n",
    "    remove_stopwords=False,\n",
    "    compute_rouge_l=True,\n",
    "    skip_bigram_gap_length=4,  # for ROUGE-SU4\n",
    "    wlcs_weight=1.2            # for ROUGE-W-1.2\n",
    ")\n",
    "\n",
    "# Prepare lists for metrics\n",
    "rouge_1_f, rouge_1_p, rouge_1_r = [], [], []\n",
    "rouge_2_f, rouge_2_p, rouge_2_r = [], [], []\n",
    "rouge_3_f, rouge_3_p, rouge_3_r = [], [], []\n",
    "rouge_4_f, rouge_4_p, rouge_4_r = [], [], []\n",
    "rouge_l_f, rouge_l_p, rouge_l_r = [], [], []\n",
    "rouge_su4_f, rouge_su4_p, rouge_su4_r = [], [], []\n",
    "rouge_w12_f, rouge_w12_p, rouge_w12_r = [], [], []\n",
    "\n",
    "# 3. Iterate over rows and compute ROUGE\n",
    "for idx, row in data_short.iterrows():\n",
    "    chatbot_answer = str(row['chatbot_answer'])\n",
    "    human_answer   = str(row['human_answer'])\n",
    "    \n",
    "    scores = rouge.score(chatbot_answer, [human_answer])\n",
    "\n",
    "    # R-1\n",
    "    rouge_1_f.append(scores['rouge-1']['f1'])\n",
    "    rouge_1_p.append(scores['rouge-1']['precision'])\n",
    "    rouge_1_r.append(scores['rouge-1']['recall'])\n",
    "    # R-2\n",
    "    rouge_2_f.append(scores['rouge-2']['f1'])\n",
    "    rouge_2_p.append(scores['rouge-2']['precision'])\n",
    "    rouge_2_r.append(scores['rouge-2']['recall'])\n",
    "    # R-3\n",
    "    rouge_3_f.append(scores['rouge-3']['f1'])\n",
    "    rouge_3_p.append(scores['rouge-3']['precision'])\n",
    "    rouge_3_r.append(scores['rouge-3']['recall'])\n",
    "    # R-4\n",
    "    rouge_4_f.append(scores['rouge-4']['f1'])\n",
    "    rouge_4_p.append(scores['rouge-4']['precision'])\n",
    "    rouge_4_r.append(scores['rouge-4']['recall'])\n",
    "    # R-L\n",
    "    rouge_l_f.append(scores['rouge-l']['f1'])\n",
    "    rouge_l_p.append(scores['rouge-l']['precision'])\n",
    "    rouge_l_r.append(scores['rouge-l']['recall'])\n",
    "    # R-SU4\n",
    "    rouge_su4_f.append(scores['rouge-su4']['f1'])\n",
    "    rouge_su4_p.append(scores['rouge-su4']['precision'])\n",
    "    rouge_su4_r.append(scores['rouge-su4']['recall'])\n",
    "    # R-W-1.2\n",
    "    rouge_w12_f.append(scores['rouge-w-1.2']['f1'])\n",
    "    rouge_w12_p.append(scores['rouge-w-1.2']['precision'])\n",
    "    rouge_w12_r.append(scores['rouge-w-1.2']['recall'])\n",
    "\n",
    "# 4. Create a new DataFrame to store only the ROUGE results\n",
    "rouge_df = pd.DataFrame()\n",
    "\n",
    "# We also want to preserve 'question_id_q' from the original data\n",
    "rouge_df['question_id_q'] = data_short['question_id_q'].values\n",
    "\n",
    "# Add all the ROUGE metric columns\n",
    "rouge_df['ROUGE-1_f'] = rouge_1_f\n",
    "rouge_df['ROUGE-1_p'] = rouge_1_p\n",
    "rouge_df['ROUGE-1_r'] = rouge_1_r\n",
    "\n",
    "rouge_df['ROUGE-2_f'] = rouge_2_f\n",
    "rouge_df['ROUGE-2_p'] = rouge_2_p\n",
    "rouge_df['ROUGE-2_r'] = rouge_2_r\n",
    "\n",
    "rouge_df['ROUGE-3_f'] = rouge_3_f\n",
    "rouge_df['ROUGE-3_p'] = rouge_3_p\n",
    "rouge_df['ROUGE-3_r'] = rouge_3_r\n",
    "\n",
    "rouge_df['ROUGE-4_f'] = rouge_4_f\n",
    "rouge_df['ROUGE-4_p'] = rouge_4_p\n",
    "rouge_df['ROUGE-4_r'] = rouge_4_r\n",
    "\n",
    "rouge_df['ROUGE-L_f'] = rouge_l_f\n",
    "rouge_df['ROUGE-L_p'] = rouge_l_p\n",
    "rouge_df['ROUGE-L_r'] = rouge_l_r\n",
    "\n",
    "rouge_df['ROUGE-SU4_f'] = rouge_su4_f\n",
    "rouge_df['ROUGE-SU4_p'] = rouge_su4_p\n",
    "rouge_df['ROUGE-SU4_r'] = rouge_su4_r\n",
    "\n",
    "rouge_df['ROUGE-W-1.2_f'] = rouge_w12_f\n",
    "rouge_df['ROUGE-W-1.2_p'] = rouge_w12_p\n",
    "rouge_df['ROUGE-W-1.2_r'] = rouge_w12_r\n",
    "\n",
    "# 5. [Optional] Compute system-level averages (macro) for ROUGE F-scores\n",
    "r1_f_mean   = rouge_df['ROUGE-1_f'].mean()\n",
    "r2_f_mean   = rouge_df['ROUGE-2_f'].mean()\n",
    "r3_f_mean   = rouge_df['ROUGE-3_f'].mean()\n",
    "r4_f_mean   = rouge_df['ROUGE-4_f'].mean()\n",
    "rl_f_mean   = rouge_df['ROUGE-L_f'].mean()\n",
    "rsu4_f_mean = rouge_df['ROUGE-SU4_f'].mean()\n",
    "rw12_f_mean = rouge_df['ROUGE-W-1.2_f'].mean()\n",
    "\n",
    "print(\"System-level average (macro) F1 scores:\")\n",
    "print(f\"  ROUGE-1: {r1_f_mean:.3f}\")\n",
    "print(f\"  ROUGE-2: {r2_f_mean:.3f}\")\n",
    "print(f\"  ROUGE-3: {r3_f_mean:.3f}\")\n",
    "print(f\"  ROUGE-4: {r4_f_mean:.3f}\")\n",
    "print(f\"  ROUGE-L: {rl_f_mean:.3f}\")\n",
    "print(f\"  ROUGE-SU4: {rsu4_f_mean:.3f}\")\n",
    "print(f\"  ROUGE-W-1.2: {rw12_f_mean:.3f}\")\n",
    "\n",
    "# 6. Save the new ROUGE-only DataFrame\n",
    "rouge_csv_path = os.path.join(cwd, 'data/eval/rouge_evaluation_de.csv')\n",
    "rouge_df.to_csv(rouge_csv_path, index=False, quoting=1)\n",
    "print(\"Saved ROUGE metrics to:\", rouge_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System-level (macro) ROUGE scores:\n",
      "defaultdict(<class 'sacrerouge.data.metrics_dict.MetricsDict'>, {'rouge-1': {'recall': 96.69200000000001, 'precision': 96.255, 'f1': 96.326}, 'rouge-2': {'recall': 0.9249999999999999, 'precision': 0.641, 'f1': 0.7020000000000001}, 'rouge-3': {'recall': 0.573, 'precision': 0.357, 'f1': 0.406}, 'rouge-4': {'recall': 0.40800000000000003, 'precision': 0.22999999999999998, 'f1': 0.271}, 'rouge-l': {'recall': 96.453, 'precision': 96.048, 'f1': 96.116}, 'rouge-w-1.2': {'recall': 95.36200000000001, 'precision': 95.6, 'f1': 95.393}, 'rouge-su4': {'recall': 0.882, 'precision': 0.617, 'f1': 0.67}})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sacrerouge.metrics import Rouge\n",
    "\n",
    "# 1. Load your CSV\n",
    "cwd = os.getcwd()\n",
    "csv_path = os.path.join(cwd, 'data/final_merged_dataset_short.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2. Prepare lists of summaries and references\n",
    "#    Each summary is just a string, each reference is in a list (even if there is only one reference).\n",
    "summaries = [str(ans) for ans in df['chatbot_answer']]\n",
    "references_list = [[str(ref)] for ref in df['human_answer']]\n",
    "\n",
    "# 3. Initialize the ROUGE metric with your chosen parameters\n",
    "#    This includes R-1, R-2, R-3, R-L, R-SU4, R-W-1.2, etc.\n",
    "rouge = Rouge(\n",
    "    max_ngram=4,               # includes R-1, R-2, R-3, and R-4\n",
    "    use_porter_stemmer=False,  # not useing Porter stemmer to strip word suffixes\n",
    "    compute_rouge_l=True,\n",
    "    skip_bigram_gap_length=4,  # needed for R-SU4\n",
    "    wlcs_weight=1.2            # needed for R-W-1.2\n",
    ")\n",
    "\n",
    "# 4. Run 'evaluate'\n",
    "#    This returns:\n",
    "#       - macro_metrics: one \"system-level\" dictionary with average scores across all summaries\n",
    "#       - micro_metrics_list: a list of dictionaries, one for each summary\n",
    "macro_metrics, micro_metrics_list = rouge.evaluate(summaries, references_list)\n",
    "\n",
    "print(\"System-level (macro) ROUGE scores:\")\n",
    "print(macro_metrics)  # e.g. {'rouge-1': {'recall': 40.123, 'precision': ...}, 'rouge-2': {...}, ...}\n",
    "\n",
    "# 5. Append the per-summary (micro) ROUGE scores to your DataFrame\n",
    "rouge_1_f = []\n",
    "rouge_2_f = []\n",
    "rouge_3_f = []\n",
    "rouge_l_f = []\n",
    "rouge_su4_f = []\n",
    "rouge_w12_f = []\n",
    "\n",
    "for metrics_dict in micro_metrics_list:\n",
    "    # metrics_dict looks like:\n",
    "    # {\n",
    "    #   'rouge-1':  {'recall': X, 'precision': Y, 'f1': Z},\n",
    "    #   'rouge-2':  {...},\n",
    "    #   'rouge-3':  {...},\n",
    "    #   'rouge-4':  {...},\n",
    "    #   'rouge-l':  {...},\n",
    "    #   'rouge-su4': {...},\n",
    "    #   'rouge-w-1.2': {...},\n",
    "    # }\n",
    "\n",
    "    rouge_1_f.append(metrics_dict['rouge-1']['f1'])\n",
    "    rouge_2_f.append(metrics_dict['rouge-2']['f1'])\n",
    "    rouge_3_f.append(metrics_dict['rouge-3']['f1'])\n",
    "    rouge_l_f.append(metrics_dict['rouge-l']['f1'])\n",
    "    rouge_su4_f.append(metrics_dict['rouge-su4']['f1'])\n",
    "    rouge_w12_f.append(metrics_dict['rouge-w-1.2']['f1'])\n",
    "\n",
    "df['ROUGE-1_f'] = rouge_1_f\n",
    "df['ROUGE-2_f'] = rouge_2_f\n",
    "df['ROUGE-3_f'] = rouge_3_f\n",
    "df['ROUGE-L_f'] = rouge_l_f\n",
    "df['ROUGE-SU4_f'] = rouge_su4_f\n",
    "df['ROUGE-W-1.2_f'] = rouge_w12_f\n",
    "\n",
    "# # 6. Save the results back to CSV\n",
    "# output_csv = os.path.join(cwd, 'data/final_merged_dataset_with_rouge_evaluate.csv')\n",
    "# df.to_csv(output_csv, index=False)\n",
    "\n",
    "# print(\"Saved per-summary and system-level ROUGE scores to:\", output_csv)\n",
    "# #print(micro_metrics_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sacrerouge-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
