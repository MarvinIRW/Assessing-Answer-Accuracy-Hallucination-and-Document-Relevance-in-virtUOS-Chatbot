{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System-level average (macro) F1 scores for /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-virtUOS-Chatbot/code/eval/../../data/eval/rouge_evaluation_de.csv:\n",
      "  ROUGE-1: 28.290\n",
      "  ROUGE-2: 12.419\n",
      "  ROUGE-3: 7.133\n",
      "  ROUGE-4: 4.636\n",
      "  ROUGE-L: 24.710\n",
      "  ROUGE-SU4: 11.748\n",
      "  ROUGE-W-1.2: 11.872\n",
      "\n",
      "Saved ROUGE metrics to: /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-virtUOS-Chatbot/code/eval/../../data/eval/rouge_evaluation_de.csv\n",
      "System-level average (macro) F1 scores for /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-virtUOS-Chatbot/code/eval/../../data/eval/rouge_evaluation_en.csv:\n",
      "  ROUGE-1: 31.612\n",
      "  ROUGE-2: 12.896\n",
      "  ROUGE-3: 7.018\n",
      "  ROUGE-4: 4.708\n",
      "  ROUGE-L: 26.889\n",
      "  ROUGE-SU4: 13.673\n",
      "  ROUGE-W-1.2: 12.311\n",
      "\n",
      "Saved ROUGE metrics to: /mnt/c/Users/wurch/Documents/_STUDIUM/Cognitive_Science_Studium/_thesis/Assessing-Answer-Accuracy-Hallucination-and-Document-Relevance-in-virtUOS-Chatbot/code/eval/../../data/eval/rouge_evaluation_en.csv\n"
     ]
    }
   ],
   "source": [
    "# run in python 3.9.21 environment - newer version had issues with sacrerouge\n",
    "import os\n",
    "import pandas as pd\n",
    "from sacrerouge.metrics import Rouge\n",
    "\n",
    "def compute_sentence_rouge(\n",
    "    df: pd.DataFrame,\n",
    "    reference_col: str,\n",
    "    hypothesis_col: str,\n",
    "    question_id_col: str,\n",
    "    output_csv_path: str,\n",
    "    mean_csv_path=None,\n",
    "    dataset_lang=None,\n",
    "    rouge_metric: Rouge = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute sentence-level ROUGE for each row in `df`, comparing `hypothesis_col` \n",
    "    to `reference_col`. Saves the results to `output_csv_path` and returns a \n",
    "    DataFrame with the ROUGE results for each row.\n",
    "    \"\"\"\n",
    "    # If no custom ROUGE object is provided, create a default one\n",
    "    if rouge_metric is None:\n",
    "        rouge_metric = Rouge(\n",
    "            max_ngram=4,\n",
    "            use_porter_stemmer=False,\n",
    "            remove_stopwords=False,\n",
    "            compute_rouge_l=True,\n",
    "            skip_bigram_gap_length=4,\n",
    "            wlcs_weight=1.2\n",
    "        )\n",
    "    \n",
    "    # Lists to store each variant's F1, precision, and recall\n",
    "    r1_f, r1_p, r1_r = [], [], []\n",
    "    r2_f, r2_p, r2_r = [], [], []\n",
    "    r3_f, r3_p, r3_r = [], [], []\n",
    "    r4_f, r4_p, r4_r = [], [], []\n",
    "    rl_f, rl_p, rl_r = [], [], []\n",
    "    rsu4_f, rsu4_p, rsu4_r = [], [], []\n",
    "    rw12_f, rw12_p, rw12_r = [], [], []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        hypothesis = str(row[hypothesis_col])\n",
    "        reference  = str(row[reference_col])\n",
    "        \n",
    "        # Each row can have multiple references, but here we just pass a single-element list\n",
    "        scores = rouge_metric.score(hypothesis, [reference])\n",
    "\n",
    "        # R-1\n",
    "        r1_f.append(scores['rouge-1']['f1'])\n",
    "        r1_p.append(scores['rouge-1']['precision'])\n",
    "        r1_r.append(scores['rouge-1']['recall'])\n",
    "        # R-2\n",
    "        r2_f.append(scores['rouge-2']['f1'])\n",
    "        r2_p.append(scores['rouge-2']['precision'])\n",
    "        r2_r.append(scores['rouge-2']['recall'])\n",
    "        # R-3\n",
    "        r3_f.append(scores['rouge-3']['f1'])\n",
    "        r3_p.append(scores['rouge-3']['precision'])\n",
    "        r3_r.append(scores['rouge-3']['recall'])\n",
    "        # R-4\n",
    "        r4_f.append(scores['rouge-4']['f1'])\n",
    "        r4_p.append(scores['rouge-4']['precision'])\n",
    "        r4_r.append(scores['rouge-4']['recall'])\n",
    "        # R-L\n",
    "        rl_f.append(scores['rouge-l']['f1'])\n",
    "        rl_p.append(scores['rouge-l']['precision'])\n",
    "        rl_r.append(scores['rouge-l']['recall'])\n",
    "        # R-SU4\n",
    "        rsu4_f.append(scores['rouge-su4']['f1'])\n",
    "        rsu4_p.append(scores['rouge-su4']['precision'])\n",
    "        rsu4_r.append(scores['rouge-su4']['recall'])\n",
    "        # R-W-1.2\n",
    "        rw12_f.append(scores['rouge-w-1.2']['f1'])\n",
    "        rw12_p.append(scores['rouge-w-1.2']['precision'])\n",
    "        rw12_r.append(scores['rouge-w-1.2']['recall'])\n",
    "\n",
    "    # Build a new DataFrame\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[question_id_col] = df[question_id_col].values\n",
    "\n",
    "    result_df['ROUGE-1_f'] = r1_f\n",
    "    result_df['ROUGE-1_p'] = r1_p\n",
    "    result_df['ROUGE-1_r'] = r1_r\n",
    "\n",
    "    result_df['ROUGE-2_f'] = r2_f\n",
    "    result_df['ROUGE-2_p'] = r2_p\n",
    "    result_df['ROUGE-2_r'] = r2_r\n",
    "\n",
    "    result_df['ROUGE-3_f'] = r3_f\n",
    "    result_df['ROUGE-3_p'] = r3_p\n",
    "    result_df['ROUGE-3_r'] = r3_r\n",
    "\n",
    "    result_df['ROUGE-4_f'] = r4_f\n",
    "    result_df['ROUGE-4_p'] = r4_p\n",
    "    result_df['ROUGE-4_r'] = r4_r\n",
    "\n",
    "    result_df['ROUGE-L_f'] = rl_f\n",
    "    result_df['ROUGE-L_p'] = rl_p\n",
    "    result_df['ROUGE-L_r'] = rl_r\n",
    "\n",
    "    result_df['ROUGE-SU4_f'] = rsu4_f\n",
    "    result_df['ROUGE-SU4_p'] = rsu4_p\n",
    "    result_df['ROUGE-SU4_r'] = rsu4_r\n",
    "\n",
    "    result_df['ROUGE-W-1.2_f'] = rw12_f\n",
    "    result_df['ROUGE-W-1.2_p'] = rw12_p\n",
    "    result_df['ROUGE-W-1.2_r'] = rw12_r\n",
    "\n",
    "\n",
    "    # Compute macro averages for F1\n",
    "    r1_f_mean   = result_df['ROUGE-1_f'].mean()\n",
    "    r2_f_mean   = result_df['ROUGE-2_f'].mean()\n",
    "    r3_f_mean   = result_df['ROUGE-3_f'].mean()\n",
    "    r4_f_mean   = result_df['ROUGE-4_f'].mean()\n",
    "    rl_f_mean   = result_df['ROUGE-L_f'].mean()\n",
    "    rsu4_f_mean = result_df['ROUGE-SU4_f'].mean()\n",
    "    rw12_f_mean = result_df['ROUGE-W-1.2_f'].mean()\n",
    "\n",
    "    print(f\"System-level average (macro) F1 scores for {output_csv_path}:\")\n",
    "    print(f\"  ROUGE-1: {r1_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-2: {r2_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-3: {r3_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-4: {r4_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-L: {rl_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-SU4: {rsu4_f_mean:.3f}\")\n",
    "    print(f\"  ROUGE-W-1.2: {rw12_f_mean:.3f}\\n\")\n",
    "\n",
    "    if mean_csv_path is not None and os.path.exists(mean_csv_path) and dataset_lang is not None:\n",
    "        # save the mean evaluation scores\n",
    "        mean_eval = pd.read_csv(mean_csv_path)\n",
    "        # add row to the mean_eval df\n",
    "        if f\"ROUGE-1_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-1_f_{dataset_lang}\", \"value\": r1_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-2_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-2_f_{dataset_lang}\", \"value\": r2_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-3_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-3_f_{dataset_lang}\", \"value\": r3_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-4_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-4_f_{dataset_lang}\", \"value\": r4_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-L_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-L_f_{dataset_lang}\", \"value\": rl_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-SU4_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-SU4_f_{dataset_lang}\", \"value\": rsu4_f_mean}])], ignore_index=True)\n",
    "        if f\"ROUGE-W-1.2_f_{dataset_lang}\" not in mean_eval[\"metric\"].values:\n",
    "            mean_eval = pd.concat([mean_eval, pd.DataFrame([{\"metric\": f\"ROUGE-W-1.2_f_{dataset_lang}\", \"value\": rw12_f_mean}])], ignore_index=True)\n",
    "        mean_eval.to_csv(mean_csv_path, index=False)\n",
    "\n",
    "    # Save to CSV\n",
    "    result_df.to_csv(output_csv_path, index=False, quoting=1)\n",
    "    print(\"Saved ROUGE metrics to:\", output_csv_path)\n",
    "    return result_df\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1) Load the CSVs\n",
    "cwd = os.getcwd()\n",
    "# - If you have separate DE and EN data:\n",
    "csv_path_de = os.path.join(cwd, '../../data/short_dataset_de.csv')\n",
    "csv_path_en = os.path.join(cwd, '../../data/short_dataset_en.csv')\n",
    "mean_csv_path = os.path.join(cwd, '../../data/eval/mean_eval.csv')\n",
    "df_de = pd.read_csv(csv_path_de)\n",
    "df_en = pd.read_csv(csv_path_en)\n",
    "\n",
    "# # 2) (Optional) limit to smaller subset for demonstration\n",
    "# df_de = df_de.head(18).copy()\n",
    "# df_en = df_en.head(18).copy()\n",
    "\n",
    "# 3) Evaluate ROUGE for German\n",
    "output_csv_de = os.path.join(cwd, '../../data/eval/rouge_evaluation_de.csv')\n",
    "rouge_df_de = compute_sentence_rouge(\n",
    "    df=df_de,\n",
    "    reference_col='human_answer_de',\n",
    "    hypothesis_col='chatbot_answer_de',\n",
    "    question_id_col='question_id_q',\n",
    "    output_csv_path=output_csv_de,\n",
    "    mean_csv_path=mean_csv_path,\n",
    "    dataset_lang='de'\n",
    ")\n",
    "\n",
    "# 4) Evaluate ROUGE for English\n",
    "output_csv_en = os.path.join(cwd, '../../data/eval/rouge_evaluation_en.csv')\n",
    "rouge_df_en = compute_sentence_rouge(\n",
    "    df=df_en,\n",
    "    reference_col='human_answer_en',\n",
    "    hypothesis_col='chatbot_answer_en',\n",
    "    question_id_col='question_id_q',\n",
    "    output_csv_path=output_csv_en,\n",
    "    mean_csv_path=mean_csv_path,\n",
    "    dataset_lang='en'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sacrerouge-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
