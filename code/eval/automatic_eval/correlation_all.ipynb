{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_spearman_correlations_by_language(\n",
    "    human_eval_path,\n",
    "    metrics_folder,\n",
    "    metrics_files,\n",
    "    question_id_human=\"qid\",\n",
    "    question_id_metric=\"question_id_q\",\n",
    "    human_eval_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes Spearman correlations between human evaluation columns and \n",
    "    multiple automatic metric CSVs, separately for German (langq='de') and \n",
    "    English (langq='en') question-answer pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    human_eval_path : str\n",
    "        Path to the 'human_eval.csv' file.\n",
    "    metrics_folder : str\n",
    "        Folder containing your automatic metric CSV files.\n",
    "    metrics_files : dict\n",
    "        A dict that maps a CSV filename (e.g., 'bartscore_de_cnn.csv') \n",
    "        to a list of columns in that file to correlate.\n",
    "        Example:\n",
    "            {\n",
    "                \"bartscore_de_cnn.csv\": [\"BARTScore_paper_avg\", \"BARTScore_paper_harm\"],\n",
    "                \"bleu_evaluation_en.csv\": [\"BLEU\"],\n",
    "                ...\n",
    "            }\n",
    "    question_id_human : str, optional\n",
    "        The column name in the human eval CSV used to identify the question ID\n",
    "        (default \"qid\").\n",
    "    question_id_metric : str, optional\n",
    "        The column name in the metric CSV used to identify the question ID\n",
    "        (default \"question_id_q\").\n",
    "    human_eval_cols : list of str, optional\n",
    "        Which human-eval columns to compare against each metric.\n",
    "        Defaults to the columns in your provided CSV example.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : pd.DataFrame\n",
    "        A DataFrame with columns:\n",
    "        [\"metric_file\", \"metric_column\", \"human_column\", \"language\", \n",
    "         \"spearman_corr\", \"p_value\"]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function automatically splits the human-eval data into two subsets:\n",
    "      one for 'langq' == 'de' and one for 'langq' == 'en'.\n",
    "    - It detects whether a metric file is meant for German or English by\n",
    "      checking if the filename contains '_de' or '_en'.\n",
    "    - Merges are done on question ID columns. Only rows present in both\n",
    "      data sets are used in the correlation.\n",
    "    \"\"\"\n",
    "    # 1) Read the human evaluation CSV\n",
    "    human_eval_df = pd.read_csv(human_eval_path)\n",
    "\n",
    "    # 2) Split the human-eval data into DE and EN subsets\n",
    "    human_eval_de = human_eval_df[human_eval_df[\"langq\"] == \"de\"].copy()\n",
    "    human_eval_en = human_eval_df[human_eval_df[\"langq\"] == \"en\"].copy()\n",
    "\n",
    "    # Default columns if none are specified\n",
    "    if human_eval_cols is None:\n",
    "        human_eval_cols = [\n",
    "            \"avg_hallucination\",\n",
    "            \"avg_answer_acc\",\n",
    "            \"avg_user_sat\",\n",
    "            \"avg_coherence\",\n",
    "            \"avg_context_qual\",\n",
    "            \"avg_overall\",\n",
    "            \"overall_mean\"\n",
    "        ]\n",
    "    \n",
    "    # Prepare a list to accumulate correlation records\n",
    "    correlation_records = []\n",
    "\n",
    "    # 3) Loop over each metric file and correlate with the matching language subset\n",
    "    for metric_filename, metric_cols in metrics_files.items():\n",
    "        metric_path = os.path.join(metrics_folder, metric_filename)\n",
    "        \n",
    "        # Check if it's a DE file or an EN file\n",
    "        if \"_de\" in metric_filename.lower():\n",
    "            relevant_human_eval = human_eval_de\n",
    "            language = \"de\"\n",
    "        elif \"_en\" in metric_filename.lower():\n",
    "            relevant_human_eval = human_eval_en\n",
    "            language = \"en\"\n",
    "        else:\n",
    "            # If neither _de nor _en is found, skip or raise a warning\n",
    "            print(f\"Warning: '{metric_filename}' does not indicate 'de' or 'en'; skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Read the metric CSV\n",
    "        if not os.path.exists(metric_path):\n",
    "            print(f\"File not found: {metric_path}\")\n",
    "            continue\n",
    "        \n",
    "        metric_df = pd.read_csv(metric_path)\n",
    "        \n",
    "        # Merge on the question IDs\n",
    "        merged = pd.merge(\n",
    "            relevant_human_eval,\n",
    "            metric_df,\n",
    "            how=\"inner\",\n",
    "            left_on=question_id_human,\n",
    "            right_on=question_id_metric\n",
    "        )\n",
    "\n",
    "        # 4) For each metric column, compute correlation with each human_eval_col\n",
    "        for metric_col in metric_cols:\n",
    "            if metric_col not in merged.columns:\n",
    "                print(f\"Column '{metric_col}' not found in '{metric_filename}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Drop rows with missing data to avoid correlation errors\n",
    "            \n",
    "            valid_data = merged.dropna(subset=human_eval_cols + [metric_col])\n",
    "\n",
    "            for human_col in human_eval_cols:\n",
    "                x = valid_data[human_col]\n",
    "                y = valid_data[metric_col]\n",
    "\n",
    "                # Check if x or y is constant (zero variance)\n",
    "                if x.nunique() <= 1 or y.nunique() <= 1:\n",
    "                    print(\n",
    "                        f\"Warning: Constant input array encountered in file '{metric_filename}' \"\n",
    "                        f\"(metric_col='{metric_col}', human_col='{human_col}', lang='{language}'). \"\n",
    "                        \"Spearman's r is not defined for constant data.\"\n",
    "                    )\n",
    "                    # We can still compute it for completeness (will be NaN),\n",
    "                    # or skip. Let's compute and store the results (NaN).\n",
    "                    r, pval = spearmanr(x, y)\n",
    "                else:\n",
    "                    # Regular correlation\n",
    "                    r, pval = spearmanr(x, y)\n",
    "                \n",
    "                correlation_records.append({\n",
    "                    \"metric_file\": metric_filename,\n",
    "                    \"metric_column\": metric_col,\n",
    "                    \"human_column\": human_col,\n",
    "                    \"language\": language,\n",
    "                    \"spearman_corr\": r,\n",
    "                    \"p_value\": pval\n",
    "                })\n",
    "\n",
    "    # 5) Convert list of records to a DataFrame\n",
    "    results_df = pd.DataFrame(correlation_records)\n",
    "    #results_df = results_df.sort_values(\"spearman_corr\", ascending=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_hallucination', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_answer_acc', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_user_sat', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_coherence', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_context_qual', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_overall', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='overall_mean', lang='en'). Spearman's r is not defined for constant data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wurch\\AppData\\Local\\Temp\\ipykernel_15940\\3396539373.py:136: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, pval = spearmanr(x, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532, 6)\n"
     ]
    }
   ],
   "source": [
    "human_eval_csv = \"../../../data//human_eval_avg.csv\"\n",
    "metrics_dir = \"../../../data/eval\"\n",
    "\n",
    "# Each CSV file plus the columns you want to correlate\n",
    "metrics_config = {\n",
    "    # ROUGE\n",
    "    \"rouge_evaluation_de.csv\": [\n",
    "        \"ROUGE-1_f\",\n",
    "        \"ROUGE-2_f\",\n",
    "        \"ROUGE-3_f\",\n",
    "        \"ROUGE-4_f\",\n",
    "        \"ROUGE-L_f\",\n",
    "        \"ROUGE-SU4_f\",\n",
    "        \"ROUGE-W-1.2_f\"],\n",
    "    \"rouge_evaluation_en.csv\": [\n",
    "        \"ROUGE-1_f\",\n",
    "        \"ROUGE-2_f\",\n",
    "        \"ROUGE-3_f\",\n",
    "        \"ROUGE-4_f\",\n",
    "        \"ROUGE-L_f\",\n",
    "        \"ROUGE-SU4_f\",\n",
    "        \"ROUGE-W-1.2_f\"],\n",
    "    #BLEU\n",
    "    \"bleu_evaluation_de.csv\": [\n",
    "        \"BLEU\"\n",
    "    ],\n",
    "    \"bleu_evaluation_en.csv\": [\n",
    "        \"BLEU\"\n",
    "    ],\n",
    "    # BERTScore\n",
    "    \"bertscore_evaluation_de.csv\": [\n",
    "        \"BERTScore_F1\",\n",
    "    ],\n",
    "    \"bertscore_evaluation_en.csv\": [\n",
    "        \"BERTScore_F1\",\n",
    "    ],\n",
    "    # BARTScore\n",
    "    \"bartscore_cnn_de.csv\": [\n",
    "        \"BARTScore_paper_avg\", \n",
    "        \"BARTScore_paper_harm\"\n",
    "    ],\n",
    "    \"bartscore_cnn_en.csv\": [\n",
    "        \"BARTScore_paper_avg\", \n",
    "        \"BARTScore_paper_harm\"\n",
    "    ],\n",
    "    \"bartscore_multi_de.csv\": [\n",
    "        \"BARTScore_multilang_avg\", \n",
    "        \"BARTScore_multilang_harm\"\n",
    "    ],\n",
    "    \"bartscore_multi_en.csv\": [\n",
    "        \"BARTScore_multilang_avg\", \n",
    "        \"BARTScore_multilang_harm\"\n",
    "    ],\n",
    "    # BLEURT\n",
    "    \"bleurt_evaluation_de.csv\": [\n",
    "        \"BLEURT\"\n",
    "    ],\n",
    "    \"bleurt_evaluation_en.csv\": [\n",
    "        \"BLEURT\"\n",
    "    ],\n",
    "    # LLM Judge\n",
    "    # together no ref\n",
    "    \"llm_judge_together_no_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_together_no_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    # together with ref\n",
    "    \"llm_judge_together_with_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_together_with_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    # seperate no ref\n",
    "    \"llm_judge_seperate_no_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_seperate_no_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    # seperate with ref\n",
    "    \"llm_judge_seperate_with_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_seperate_with_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    \n",
    "}\n",
    "\n",
    "# Compute correlations\n",
    "df_results = compute_spearman_correlations_by_language(\n",
    "    human_eval_path=human_eval_csv,\n",
    "    metrics_folder=metrics_dir,\n",
    "    metrics_files=metrics_config,\n",
    "    question_id_human=\"qid\",          \n",
    "    question_id_metric=\"question_id_q\"\n",
    ")\n",
    "\n",
    "print(df_results.shape)\n",
    "# save to CSV\n",
    "df_results.to_csv(\"../../../data/eval/correlation/correlation_splits_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 6)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the mapping to allow multiple mappings per LLM metric\n",
    "llm_human_mapping = {\n",
    "    \"hallucination_score\": [\"avg_hallucination\"],\n",
    "    \"answer_accuracy_score\": [\"avg_answer_acc\"],\n",
    "    \"user_satisfaction_score\": [\"avg_user_sat\"],\n",
    "    \"coherence_clarity_fluency_score\": [\"avg_coherence\"],\n",
    "    \"context_quality_score\": [\"avg_context_qual\"],\n",
    "    \"overall_score\": [\"avg_overall\", \"overall_mean\"]  # Multiple mappings for overall_score\n",
    "}\n",
    "# Filter for LLM-as-a-judge metrics\n",
    "llm_files = [\n",
    "    'llm_judge_together_no_ref_de.csv',\n",
    "    'llm_judge_together_no_ref_en.csv',\n",
    "    'llm_judge_together_with_ref_de.csv',\n",
    "    'llm_judge_together_with_ref_en.csv',\n",
    "    'llm_judge_seperate_no_ref_de.csv',\n",
    "    'llm_judge_seperate_no_ref_en.csv',\n",
    "    'llm_judge_seperate_with_ref_de.csv',\n",
    "    'llm_judge_seperate_with_ref_en.csv'\n",
    "]\n",
    "# Filter DataFrame for LLM-as-a-judge rows\n",
    "filtered_llm_df = df_results[\n",
    "    df_results['metric_file'].isin(llm_files)\n",
    "]\n",
    "\n",
    "# Apply the updated mapping for LLM-as-a-judge metrics\n",
    "filtered_llm_df = filtered_llm_df[\n",
    "    filtered_llm_df.apply(\n",
    "        lambda row: row['human_column'] in llm_human_mapping.get(row['metric_column'], []),\n",
    "        axis=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Combine with the other metrics (no filtering needed for these)\n",
    "other_files = [file for file in df_results['metric_file'].unique() if file not in llm_files]\n",
    "\n",
    "filtered_other_df = df_results[\n",
    "    df_results['metric_file'].isin(other_files)\n",
    "]\n",
    "\n",
    "# Concatenate the filtered LLM-as-a-judge DataFrame with other metrics\n",
    "final_filtered_df = pd.concat([filtered_llm_df, filtered_other_df], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "final_filtered_df.to_csv(\"../../../data/eval/correlation/correlation_splits_filtered_llm.csv\", index=False)\n",
    "\n",
    "final_filtered_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out llm_judge results not alligned with human eval\n",
    "# Define the mapping of LLM-as-a-judge metrics to human evaluation columns\n",
    "llm_human_mapping = {\n",
    "    \"hallucination_score\": \"avg_hallucination\",\n",
    "    \"answer_accuracy_score\": \"avg_answer_acc\",\n",
    "    \"user_satisfaction_score\": \"avg_user_sat\",\n",
    "    \"coherence_clarity_fluency_score\": \"avg_coherence\",\n",
    "    \"context_quality_score\": \"avg_context_qual\",\n",
    "    \"overall_score\": \"avg_overall\",\n",
    "    \"overall_score\": \"overall_mean\" \n",
    "\n",
    "}\n",
    "\n",
    "# Filter for LLM-as-a-judge metrics\n",
    "llm_files = [\n",
    "    'llm_judge_together_no_ref_de.csv',\n",
    "    'llm_judge_together_no_ref_en.csv',\n",
    "    'llm_judge_together_with_ref_de.csv',\n",
    "    'llm_judge_together_with_ref_en.csv',\n",
    "    'llm_judge_seperate_no_ref_de.csv',\n",
    "    'llm_judge_seperate_no_ref_en.csv',\n",
    "    'llm_judge_seperate_with_ref_de.csv',\n",
    "    'llm_judge_seperate_with_ref_en.csv'\n",
    "]\n",
    "\n",
    "# Filter DataFrame for LLM-as-a-judge rows\n",
    "filtered_llm_df = df_results[\n",
    "    df_results['metric_file'].isin(llm_files)\n",
    "]\n",
    "\n",
    "# Apply the one-to-one mapping for LLM-as-a-judge metrics\n",
    "filtered_llm_df = filtered_llm_df[\n",
    "    filtered_llm_df.apply(\n",
    "        lambda row: llm_human_mapping.get(row['metric_column']) == row['human_column'], axis=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Combine with the other metrics (no filtering needed for these)\n",
    "other_files = [file for file in df_results['metric_file'].unique() if file not in llm_files]\n",
    "\n",
    "filtered_other_df = df_results[\n",
    "    df_results['metric_file'].isin(other_files)\n",
    "]\n",
    "\n",
    "# Concatenate the filtered LLM-as-a-judge DataFrame with other metrics\n",
    "final_filtered_df = pd.concat([filtered_llm_df, filtered_other_df], ignore_index=True)\n",
    "\n",
    "# save to CSV\n",
    "final_filtered_df.to_csv(\"../../../data/eval/correlation/correlation_splits_filtered_llm.csv\", index=False)\n",
    "\n",
    "final_filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_file</th>\n",
       "      <th>metric_column</th>\n",
       "      <th>human_column</th>\n",
       "      <th>language</th>\n",
       "      <th>spearman_corr</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>hallucination_score</td>\n",
       "      <td>avg_hallucination</td>\n",
       "      <td>de</td>\n",
       "      <td>0.123084</td>\n",
       "      <td>0.494985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>answer_accuracy_score</td>\n",
       "      <td>avg_answer_acc</td>\n",
       "      <td>de</td>\n",
       "      <td>0.139438</td>\n",
       "      <td>0.438981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>user_satisfaction_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.457298</td>\n",
       "      <td>0.007459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>coherence_clarity_fluency_score</td>\n",
       "      <td>avg_coherence</td>\n",
       "      <td>de</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.430143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>context_quality_score</td>\n",
       "      <td>avg_context_qual</td>\n",
       "      <td>de</td>\n",
       "      <td>0.245507</td>\n",
       "      <td>0.168472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        metric_file                    metric_column  \\\n",
       "0  llm_judge_together_no_ref_de.csv              hallucination_score   \n",
       "1  llm_judge_together_no_ref_de.csv            answer_accuracy_score   \n",
       "2  llm_judge_together_no_ref_de.csv          user_satisfaction_score   \n",
       "3  llm_judge_together_no_ref_de.csv  coherence_clarity_fluency_score   \n",
       "4  llm_judge_together_no_ref_de.csv            context_quality_score   \n",
       "\n",
       "        human_column language  spearman_corr   p_value  \n",
       "0  avg_hallucination       de       0.123084  0.494985  \n",
       "1     avg_answer_acc       de       0.139438  0.438981  \n",
       "2       avg_user_sat       de       0.457298  0.007459  \n",
       "3      avg_coherence       de       0.142119  0.430143  \n",
       "4   avg_context_qual       de       0.245507  0.168472  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the best rouge metric\n",
    "# Filter the DataFrame to only include ROUGE metrics\n",
    "rouge_files = ['rouge_evaluation_de.csv', 'rouge_evaluation_en.csv']\n",
    "\n",
    "# Filter for ROUGE metrics\n",
    "rouge_df = final_filtered_df[final_filtered_df['metric_file'].isin(rouge_files)]\n",
    "\n",
    "# Find the best ROUGE metric per human column and language\n",
    "best_rouge_df = (\n",
    "    rouge_df.loc[\n",
    "        rouge_df.groupby(['metric_file', 'human_column'])['spearman_corr'].idxmax()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine the best ROUGE metrics with the rest of the DataFrame\n",
    "non_rouge_df = final_filtered_df[~final_filtered_df['metric_file'].isin(rouge_files)]\n",
    "final_filtered_df_with_best_rouge = pd.concat([non_rouge_df, best_rouge_df], ignore_index=True)\n",
    "\n",
    "# save to CSV\n",
    "final_filtered_df_with_best_rouge.to_csv(\"../../testing/correlation_splits_filtered_best_rouge.csv\", index=False)\n",
    "\n",
    "# Display some rows for inspection\n",
    "final_filtered_df_with_best_rouge.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_file</th>\n",
       "      <th>metric_column</th>\n",
       "      <th>human_column</th>\n",
       "      <th>language</th>\n",
       "      <th>spearman_corr</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>hallucination_score</td>\n",
       "      <td>avg_hallucination</td>\n",
       "      <td>de</td>\n",
       "      <td>0.123084</td>\n",
       "      <td>0.494985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>answer_accuracy_score</td>\n",
       "      <td>avg_answer_acc</td>\n",
       "      <td>de</td>\n",
       "      <td>0.139438</td>\n",
       "      <td>0.438981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>user_satisfaction_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.457298</td>\n",
       "      <td>0.007459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>coherence_clarity_fluency_score</td>\n",
       "      <td>avg_coherence</td>\n",
       "      <td>de</td>\n",
       "      <td>0.142119</td>\n",
       "      <td>0.430143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>context_quality_score</td>\n",
       "      <td>avg_context_qual</td>\n",
       "      <td>de</td>\n",
       "      <td>0.245507</td>\n",
       "      <td>0.168472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        metric_file                    metric_column  \\\n",
       "0  llm_judge_together_no_ref_de.csv              hallucination_score   \n",
       "1  llm_judge_together_no_ref_de.csv            answer_accuracy_score   \n",
       "2  llm_judge_together_no_ref_de.csv          user_satisfaction_score   \n",
       "3  llm_judge_together_no_ref_de.csv  coherence_clarity_fluency_score   \n",
       "4  llm_judge_together_no_ref_de.csv            context_quality_score   \n",
       "\n",
       "        human_column language  spearman_corr   p_value  \n",
       "0  avg_hallucination       de       0.123084  0.494985  \n",
       "1     avg_answer_acc       de       0.139438  0.438981  \n",
       "2       avg_user_sat       de       0.457298  0.007459  \n",
       "3      avg_coherence       de       0.142119  0.430143  \n",
       "4   avg_context_qual       de       0.245507  0.168472  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the best bartscore metric\n",
    "# Filter the DataFrame to only include BARTScore metrics\n",
    "bartscore_files = [\n",
    "    'bartscore_cnn_de.csv', 'bartscore_cnn_en.csv',\n",
    "    'bartscore_multi_de.csv', 'bartscore_multi_en.csv'\n",
    "]\n",
    "\n",
    "# Filter for BARTScore metrics\n",
    "bartscore_df = final_filtered_df_with_best_rouge[\n",
    "    final_filtered_df_with_best_rouge['metric_file'].isin(bartscore_files)\n",
    "]\n",
    "\n",
    "# Find the best BARTScore metric per human column and language\n",
    "best_bartscore_df = (\n",
    "    bartscore_df.loc[\n",
    "        bartscore_df.groupby(['metric_file', 'human_column'])['spearman_corr'].idxmax()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine the best BARTScore metrics with the rest of the DataFrame\n",
    "non_bartscore_df = final_filtered_df_with_best_rouge[\n",
    "    ~final_filtered_df_with_best_rouge['metric_file'].isin(bartscore_files)\n",
    "]\n",
    "final_filtered_df_with_best_bartscore = pd.concat([non_bartscore_df, best_bartscore_df], ignore_index=True)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "final_filtered_df_with_best_bartscore.to_csv(\"../../testing/filtered_results_best_bartscore.csv\", index=False)\n",
    "\n",
    "# Display some rows for inspection\n",
    "final_filtered_df_with_best_bartscore.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_file</th>\n",
       "      <th>metric_column</th>\n",
       "      <th>human_column</th>\n",
       "      <th>language</th>\n",
       "      <th>spearman_corr</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>user_satisfaction_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.604144</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>llm_judge_together_with_ref_de.csv</td>\n",
       "      <td>user_satisfaction_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.599864</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>llm_judge_together_with_ref_en.csv</td>\n",
       "      <td>user_satisfaction_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>en</td>\n",
       "      <td>0.586826</td>\n",
       "      <td>0.000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>llm_judge_seperate_no_ref_de.csv</td>\n",
       "      <td>user_satisfaction_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.565073</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>llm_judge_seperate_with_ref_en.csv</td>\n",
       "      <td>user_satisfaction_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>en</td>\n",
       "      <td>0.562936</td>\n",
       "      <td>0.000648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>weighted_overall_score</td>\n",
       "      <td>avg_overall</td>\n",
       "      <td>de</td>\n",
       "      <td>0.482070</td>\n",
       "      <td>0.004499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>user_satisfaction_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.457298</td>\n",
       "      <td>0.007459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>context_quality_score</td>\n",
       "      <td>avg_context_qual</td>\n",
       "      <td>de</td>\n",
       "      <td>0.454054</td>\n",
       "      <td>0.007949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>llm_judge_together_with_ref_de.csv</td>\n",
       "      <td>weighted_overall_score</td>\n",
       "      <td>avg_overall</td>\n",
       "      <td>de</td>\n",
       "      <td>0.447441</td>\n",
       "      <td>0.009031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>bartscore_multi_de.csv</td>\n",
       "      <td>BARTScore_multilang_harm</td>\n",
       "      <td>avg_coherence</td>\n",
       "      <td>de</td>\n",
       "      <td>0.446717</td>\n",
       "      <td>0.009157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>bartscore_multi_de.csv</td>\n",
       "      <td>BARTScore_multilang_harm</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.439047</td>\n",
       "      <td>0.010583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>llm_judge_together_with_ref_de.csv</td>\n",
       "      <td>context_quality_score</td>\n",
       "      <td>avg_context_qual</td>\n",
       "      <td>de</td>\n",
       "      <td>0.426990</td>\n",
       "      <td>0.013203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>answer_accuracy_score</td>\n",
       "      <td>avg_answer_acc</td>\n",
       "      <td>de</td>\n",
       "      <td>0.415286</td>\n",
       "      <td>0.016247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>bartscore_multi_de.csv</td>\n",
       "      <td>BARTScore_multilang_harm</td>\n",
       "      <td>avg_overall</td>\n",
       "      <td>de</td>\n",
       "      <td>0.409571</td>\n",
       "      <td>0.017934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>coherence_clarity_fluency_score</td>\n",
       "      <td>avg_coherence</td>\n",
       "      <td>de</td>\n",
       "      <td>0.387445</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>llm_judge_seperate_with_ref_en.csv</td>\n",
       "      <td>weighted_overall_score</td>\n",
       "      <td>avg_overall</td>\n",
       "      <td>en</td>\n",
       "      <td>0.373272</td>\n",
       "      <td>0.032386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>bartscore_multi_de.csv</td>\n",
       "      <td>BARTScore_multilang_harm</td>\n",
       "      <td>avg_context_qual</td>\n",
       "      <td>de</td>\n",
       "      <td>0.352304</td>\n",
       "      <td>0.044348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            metric_file                    metric_column  \\\n",
       "38   llm_judge_seperate_with_ref_de.csv          user_satisfaction_score   \n",
       "14   llm_judge_together_with_ref_de.csv          user_satisfaction_score   \n",
       "20   llm_judge_together_with_ref_en.csv          user_satisfaction_score   \n",
       "26     llm_judge_seperate_no_ref_de.csv          user_satisfaction_score   \n",
       "44   llm_judge_seperate_with_ref_en.csv          user_satisfaction_score   \n",
       "41   llm_judge_seperate_with_ref_de.csv           weighted_overall_score   \n",
       "2      llm_judge_together_no_ref_de.csv          user_satisfaction_score   \n",
       "40   llm_judge_seperate_with_ref_de.csv            context_quality_score   \n",
       "17   llm_judge_together_with_ref_de.csv           weighted_overall_score   \n",
       "109              bartscore_multi_de.csv         BARTScore_multilang_harm   \n",
       "113              bartscore_multi_de.csv         BARTScore_multilang_harm   \n",
       "16   llm_judge_together_with_ref_de.csv            context_quality_score   \n",
       "37   llm_judge_seperate_with_ref_de.csv            answer_accuracy_score   \n",
       "112              bartscore_multi_de.csv         BARTScore_multilang_harm   \n",
       "39   llm_judge_seperate_with_ref_de.csv  coherence_clarity_fluency_score   \n",
       "47   llm_judge_seperate_with_ref_en.csv           weighted_overall_score   \n",
       "110              bartscore_multi_de.csv         BARTScore_multilang_harm   \n",
       "\n",
       "         human_column language  spearman_corr   p_value  \n",
       "38       avg_user_sat       de       0.604144  0.000197  \n",
       "14       avg_user_sat       de       0.599864  0.000224  \n",
       "20       avg_user_sat       en       0.586826  0.000331  \n",
       "26       avg_user_sat       de       0.565073  0.000612  \n",
       "44       avg_user_sat       en       0.562936  0.000648  \n",
       "41        avg_overall       de       0.482070  0.004499  \n",
       "2        avg_user_sat       de       0.457298  0.007459  \n",
       "40   avg_context_qual       de       0.454054  0.007949  \n",
       "17        avg_overall       de       0.447441  0.009031  \n",
       "109     avg_coherence       de       0.446717  0.009157  \n",
       "113      avg_user_sat       de       0.439047  0.010583  \n",
       "16   avg_context_qual       de       0.426990  0.013203  \n",
       "37     avg_answer_acc       de       0.415286  0.016247  \n",
       "112       avg_overall       de       0.409571  0.017934  \n",
       "39      avg_coherence       de       0.387445  0.025900  \n",
       "47        avg_overall       en       0.373272  0.032386  \n",
       "110  avg_context_qual       de       0.352304  0.044348  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for p-value < 0.05\n",
    "final_filtered_df_with_best_bartscore_1 = final_filtered_df_with_best_bartscore[final_filtered_df_with_best_bartscore[\"p_value\"] < 0.05]\n",
    "final_filtered_df_with_best_bartscore_1 = final_filtered_df_with_best_bartscore_1.sort_values(\"spearman_corr\", ascending=False)\n",
    "final_filtered_df_with_best_bartscore_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id_q</th>\n",
       "      <th>hallucination_score</th>\n",
       "      <th>hallucination_comment</th>\n",
       "      <th>answer_accuracy_score</th>\n",
       "      <th>answer_accuracy_comment</th>\n",
       "      <th>user_satisfaction_score</th>\n",
       "      <th>user_satisfaction_comment</th>\n",
       "      <th>coherence_clarity_fluency_score</th>\n",
       "      <th>coherence_clarity_fluency_comment</th>\n",
       "      <th>context_quality_score</th>\n",
       "      <th>context_quality_comment</th>\n",
       "      <th>weighted_overall_score</th>\n",
       "      <th>api_call_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer correctly declines to provide information outside the context of Osnabrück University, aligning with the system's task and avoiding hallucination.</td>\n",
       "      <td>3</td>\n",
       "      <td>The system answer accurately addresses the user's question by clarifying its scope and offering relevant assistance related to Osnabrück University, but it could be more direct in requesting specific fields of study.</td>\n",
       "      <td>3</td>\n",
       "      <td>The system answer appropriately redirects the user to focus on Osnabrück University, aligning with its task, but could be more concise.</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer is coherent, clear, and fluently addresses the user's question by specifying its limitations and offering relevant assistance related to Osnabrück University.</td>\n",
       "      <td>4</td>\n",
       "      <td>The absence of context does not impact the quality of the answer, as the system appropriately limits its response to the scope of Osnabrück University.</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.012745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer accurately lists scholarships and financial aid options available for international students at Osnabrück University, with no factual inaccuracies or hallucinations.</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer accurately and comprehensively lists the scholarships and financial aid options available for international students at Osnabrück University, matching the reference answer in detail and relevance.</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer is highly satisfactory as it provides a comprehensive list of scholarships and financial aid options available for international students at Osnabrück University, including relevant links and contact information, closely aligning with the reference answer.</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer is highly coherent, clear, and well-structured, providing a comprehensive list of scholarships and financial aid options with relevant links and contact information.</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer provides a comprehensive list of scholarships and financial aid options for international students at Osnabrück University, supported by relevant links and details, effectively utilizing the provided context.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.083625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>196</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer accurately provides the semester dates for the University of Osnabrück, matching the context information without any hallucinations.</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer accurately provides the semester dates for the University of Osnabrück, including start and end dates for semesters and classes, as well as holiday periods, matching the context and user question comprehensively.</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer provides a detailed and accurate list of semester dates for Osnabrück University, matching the user's request and offering additional context with a link for further information, thus ensuring high user satisfaction.</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer is highly coherent, clear, and well-structured, providing specific semester dates in a logical format and including a link for further information.</td>\n",
       "      <td>4</td>\n",
       "      <td>The provided context is highly relevant and effectively supports the system's answer by detailing the semester dates, aligning perfectly with the user's question.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.063363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>The system answer is severely hallucinated as it discusses admission requirements and facilities access, which are unrelated to the question about the physical accessibility of the university.</td>\n",
       "      <td>0</td>\n",
       "      <td>The system answer is inaccurate as it misinterprets the question about physical accessibility of the university, focusing instead on admission requirements and facilities access, which are irrelevant to the user's query.</td>\n",
       "      <td>0</td>\n",
       "      <td>The system answer is completely off-topic, focusing on admission requirements instead of physical accessibility, making it unhelpful and unsatisfactory.</td>\n",
       "      <td>1</td>\n",
       "      <td>The system answer is mostly irrelevant to the question about physical accessibility, focusing instead on admission requirements and facilities access, which makes it unclear and hard to follow.</td>\n",
       "      <td>0</td>\n",
       "      <td>The system answer focuses on admission requirements and facilities, which are irrelevant to the user's question about physical accessibility to the university. The provided context does not support the answer, as it does not address transportation or location details.</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.088695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>The system answer does not contain any factual claims, thus there is no hallucination present.</td>\n",
       "      <td>0</td>\n",
       "      <td>The system answer fails to provide any information about the master's programs available at Osnabrück University for a Cognitive Science bachelor's degree, which is the user's question.</td>\n",
       "      <td>1</td>\n",
       "      <td>The system answer is unhelpful as it fails to provide any specific information about master's programs at Osnabrück University, unlike the detailed reference answer.</td>\n",
       "      <td>1</td>\n",
       "      <td>The system answer is clear in its intent to provide information about Osnabrück University, but it fails to directly answer the user's question, making it less coherent and useful.</td>\n",
       "      <td>0</td>\n",
       "      <td>The system answer lacks any context or relevant information about master's programs at Osnabrück University, severely impacting the quality.</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.015157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id_q  hallucination_score  \\\n",
       "0            356                    4   \n",
       "1            153                    4   \n",
       "2            196                    4   \n",
       "3             92                    0   \n",
       "4              9                    4   \n",
       "\n",
       "                                                                                                                                                                              hallucination_comment  \\\n",
       "0                              The system answer correctly declines to provide information outside the context of Osnabrück University, aligning with the system's task and avoiding hallucination.   \n",
       "1           The system answer accurately lists scholarships and financial aid options available for international students at Osnabrück University, with no factual inaccuracies or hallucinations.   \n",
       "2                                            The system answer accurately provides the semester dates for the University of Osnabrück, matching the context information without any hallucinations.   \n",
       "3  The system answer is severely hallucinated as it discusses admission requirements and facilities access, which are unrelated to the question about the physical accessibility of the university.   \n",
       "4                                                                                                    The system answer does not contain any factual claims, thus there is no hallucination present.   \n",
       "\n",
       "   answer_accuracy_score  \\\n",
       "0                      3   \n",
       "1                      4   \n",
       "2                      4   \n",
       "3                      0   \n",
       "4                      0   \n",
       "\n",
       "                                                                                                                                                                                                                  answer_accuracy_comment  \\\n",
       "0                The system answer accurately addresses the user's question by clarifying its scope and offering relevant assistance related to Osnabrück University, but it could be more direct in requesting specific fields of study.   \n",
       "1                  The system answer accurately and comprehensively lists the scholarships and financial aid options available for international students at Osnabrück University, matching the reference answer in detail and relevance.   \n",
       "2  The system answer accurately provides the semester dates for the University of Osnabrück, including start and end dates for semesters and classes, as well as holiday periods, matching the context and user question comprehensively.   \n",
       "3            The system answer is inaccurate as it misinterprets the question about physical accessibility of the university, focusing instead on admission requirements and facilities access, which are irrelevant to the user's query.   \n",
       "4                                               The system answer fails to provide any information about the master's programs available at Osnabrück University for a Cognitive Science bachelor's degree, which is the user's question.   \n",
       "\n",
       "   user_satisfaction_score  \\\n",
       "0                        3   \n",
       "1                        4   \n",
       "2                        4   \n",
       "3                        0   \n",
       "4                        1   \n",
       "\n",
       "                                                                                                                                                                                                                                                            user_satisfaction_comment  \\\n",
       "0                                                                                                                                             The system answer appropriately redirects the user to focus on Osnabrück University, aligning with its task, but could be more concise.   \n",
       "1  The system answer is highly satisfactory as it provides a comprehensive list of scholarships and financial aid options available for international students at Osnabrück University, including relevant links and contact information, closely aligning with the reference answer.   \n",
       "2                                          The system answer provides a detailed and accurate list of semester dates for Osnabrück University, matching the user's request and offering additional context with a link for further information, thus ensuring high user satisfaction.   \n",
       "3                                                                                                                            The system answer is completely off-topic, focusing on admission requirements instead of physical accessibility, making it unhelpful and unsatisfactory.   \n",
       "4                                                                                                               The system answer is unhelpful as it fails to provide any specific information about master's programs at Osnabrück University, unlike the detailed reference answer.   \n",
       "\n",
       "   coherence_clarity_fluency_score  \\\n",
       "0                                4   \n",
       "1                                4   \n",
       "2                                4   \n",
       "3                                1   \n",
       "4                                1   \n",
       "\n",
       "                                                                                                                                                                   coherence_clarity_fluency_comment  \\\n",
       "0                   The system answer is coherent, clear, and fluently addresses the user's question by specifying its limitations and offering relevant assistance related to Osnabrück University.   \n",
       "1            The system answer is highly coherent, clear, and well-structured, providing a comprehensive list of scholarships and financial aid options with relevant links and contact information.   \n",
       "2                              The system answer is highly coherent, clear, and well-structured, providing specific semester dates in a logical format and including a link for further information.   \n",
       "3  The system answer is mostly irrelevant to the question about physical accessibility, focusing instead on admission requirements and facilities access, which makes it unclear and hard to follow.   \n",
       "4               The system answer is clear in its intent to provide information about Osnabrück University, but it fails to directly answer the user's question, making it less coherent and useful.   \n",
       "\n",
       "   context_quality_score  \\\n",
       "0                      4   \n",
       "1                      4   \n",
       "2                      4   \n",
       "3                      0   \n",
       "4                      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                        context_quality_comment  \\\n",
       "0                                                                                                                       The absence of context does not impact the quality of the answer, as the system appropriately limits its response to the scope of Osnabrück University.   \n",
       "1                                            The system answer provides a comprehensive list of scholarships and financial aid options for international students at Osnabrück University, supported by relevant links and details, effectively utilizing the provided context.   \n",
       "2                                                                                                            The provided context is highly relevant and effectively supports the system's answer by detailing the semester dates, aligning perfectly with the user's question.   \n",
       "3  The system answer focuses on admission requirements and facilities, which are irrelevant to the user's question about physical accessibility to the university. The provided context does not support the answer, as it does not address transportation or location details.   \n",
       "4                                                                                                                                  The system answer lacks any context or relevant information about master's programs at Osnabrück University, severely impacting the quality.   \n",
       "\n",
       "   weighted_overall_score  api_call_cost  \n",
       "0                     3.6       0.012745  \n",
       "1                     4.0       0.083625  \n",
       "2                     4.0       0.063363  \n",
       "3                     0.1       0.088695  \n",
       "4                     1.4       0.015157  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load llm_judge_seperate_with_ref_en.csv\n",
    "llm_judge_seperate_with_ref_en = pd.read_csv(\"../../data/eval/llm_judge_seperate_with_ref_en.csv\")\n",
    "\n",
    "# show all columns in full\n",
    "pd.set_option('display.max_columns', None)\n",
    "# show content of column in full\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "llm_judge_seperate_with_ref_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "\n",
    "def compute_correlations_by_language_2(\n",
    "    human_eval_path,\n",
    "    metrics_folder,\n",
    "    metrics_files,\n",
    "    question_id_human=\"qid\",\n",
    "    question_id_metric=\"question_id_q\",\n",
    "    human_eval_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes Spearman, Pearson, and Kendall correlations between human \n",
    "    evaluation columns and multiple automatic metric CSVs, separately for \n",
    "    German (langq='de') and English (langq='en') question-answer pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    human_eval_path : str\n",
    "        Path to the 'human_eval.csv' file.\n",
    "    metrics_folder : str\n",
    "        Folder containing your automatic metric CSV files.\n",
    "    metrics_files : dict\n",
    "        A dict that maps a CSV filename (e.g., 'bartscore_de_cnn.csv')\n",
    "        to a list of columns in that file to correlate. Example:\n",
    "            {\n",
    "                \"bartscore_de_cnn.csv\": [\"BARTScore_paper_avg\", \"BARTScore_paper_harm\"],\n",
    "                \"bleu_evaluation_en.csv\": [\"BLEU\"],\n",
    "                ...\n",
    "            }\n",
    "        Filenames should contain \"_de\" or \"_en\" to indicate which subset \n",
    "        of human data to merge with.\n",
    "    question_id_human : str, optional\n",
    "        The column name in the human eval CSV used to identify the question ID\n",
    "        (default \"qid\").\n",
    "    question_id_metric : str, optional\n",
    "        The column name in the metric CSV used to identify the question ID\n",
    "        (default \"question_id_q\").\n",
    "    human_eval_cols : list of str, optional\n",
    "        Which human-eval columns to compare against each metric. If None, \n",
    "        defaults to the columns in your example.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : pd.DataFrame\n",
    "        A DataFrame with columns:\n",
    "        [\n",
    "          \"metric_file\", \"metric_column\", \"human_column\", \"language\",\n",
    "          \"spearman_corr\",  \"spearman_pval\",\n",
    "          \"pearson_corr\",   \"pearson_pval\",\n",
    "          \"kendall_corr\",   \"kendall_pval\"\n",
    "        ]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function automatically splits the human-eval data into two subsets:\n",
    "      one for 'langq' == 'de' and one for 'langq' == 'en'.\n",
    "    - It detects whether a metric file is meant for German or English by\n",
    "      checking if \"_de\" or \"_en\" is present in the filename.\n",
    "    - Merges are done on question ID columns (inner join). Only rows present \n",
    "      in both data sets are used in the correlation.\n",
    "    - If either 'x' or 'y' is constant (zero variance), a warning is printed \n",
    "      and the correlations will come out as NaN.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Read the human evaluation CSV\n",
    "    human_eval_df = pd.read_csv(human_eval_path)\n",
    "\n",
    "    # 2) Split the human-eval data into DE and EN subsets\n",
    "    human_eval_de = human_eval_df[human_eval_df[\"langq\"] == \"de\"].copy()\n",
    "    human_eval_en = human_eval_df[human_eval_df[\"langq\"] == \"en\"].copy()\n",
    "\n",
    "    # Default columns if none are specified\n",
    "    if human_eval_cols is None:\n",
    "        human_eval_cols = [\n",
    "            \"avg_hallucination\",\n",
    "            \"avg_answer_acc\",\n",
    "            \"avg_user_sat\",\n",
    "            \"avg_coherence\",\n",
    "            \"avg_context_qual\",\n",
    "            \"avg_overall\"\n",
    "        ]\n",
    "    \n",
    "    # Prepare a list to accumulate correlation records\n",
    "    correlation_records = []\n",
    "\n",
    "    # 3) Loop over each metric file and correlate with the matching language subset\n",
    "    for metric_filename, metric_cols in metrics_files.items():\n",
    "        metric_path = os.path.join(metrics_folder, metric_filename)\n",
    "        \n",
    "        # Detect DE or EN from the filename\n",
    "        if \"_de\" in metric_filename.lower():\n",
    "            relevant_human_eval = human_eval_de\n",
    "            language = \"de\"\n",
    "        elif \"_en\" in metric_filename.lower():\n",
    "            relevant_human_eval = human_eval_en\n",
    "            language = \"en\"\n",
    "        else:\n",
    "            print(f\"Warning: '{metric_filename}' does not indicate 'de' or 'en'; skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Check file existence\n",
    "        if not os.path.exists(metric_path):\n",
    "            print(f\"File not found: {metric_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Read metric CSV\n",
    "        metric_df = pd.read_csv(metric_path)\n",
    "\n",
    "        # Merge with the appropriate subset\n",
    "        merged = pd.merge(\n",
    "            relevant_human_eval,\n",
    "            metric_df,\n",
    "            how=\"inner\",\n",
    "            left_on=question_id_human,\n",
    "            right_on=question_id_metric\n",
    "        )\n",
    "\n",
    "        # 4) For each metric column, compute correlations with each human_eval_col\n",
    "        for metric_col in metric_cols:\n",
    "            if metric_col not in merged.columns:\n",
    "                print(f\"Column '{metric_col}' not found in '{metric_filename}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Drop NaN rows\n",
    "            valid_data = merged.dropna(subset=human_eval_cols + [metric_col])\n",
    "\n",
    "            for human_col in human_eval_cols:\n",
    "                x = valid_data[human_col]\n",
    "                y = valid_data[metric_col]\n",
    "\n",
    "                # Check if x or y is constant (zero variance)\n",
    "                if x.nunique() <= 1 or y.nunique() <= 1:\n",
    "                    print(\n",
    "                        f\"Warning: Constant input array encountered in file '{metric_filename}' \"\n",
    "                        f\"(metric_col='{metric_col}', human_col='{human_col}', lang='{language}').\"\n",
    "                        \" All correlations will be NaN.\"\n",
    "                    )\n",
    "                    # We'll store NaN for all correlation types in this case\n",
    "                    r_spearman, pval_spearman = float('nan'), float('nan')\n",
    "                    r_pearson,  pval_pearson  = float('nan'), float('nan')\n",
    "                    r_kendall,  pval_kendall  = float('nan'), float('nan')\n",
    "                else:\n",
    "                    # -- SPEARMAN --\n",
    "                    r_spearman, pval_spearman = spearmanr(x, y)\n",
    "                    # -- PEARSON --\n",
    "                    r_pearson, pval_pearson = pearsonr(x, y)\n",
    "                    # -- KENDALL --\n",
    "                    r_kendall, pval_kendall = kendalltau(x, y)\n",
    "\n",
    "                correlation_records.append({\n",
    "                    \"metric_file\": metric_filename,\n",
    "                    \"metric_column\": metric_col,\n",
    "                    \"human_column\": human_col,\n",
    "                    \"language\": language,\n",
    "                    \"spearman_corr\":  r_spearman,\n",
    "                    \"spearman_pval\":  pval_spearman,\n",
    "                    \"pearson_corr\":   r_pearson,\n",
    "                    \"pearson_pval\":   pval_pearson,\n",
    "                    \"kendall_corr\":   r_kendall,\n",
    "                    \"kendall_pval\":   pval_kendall\n",
    "                })\n",
    "\n",
    "    # 5) Convert list of records to a DataFrame\n",
    "    results_df = pd.DataFrame(correlation_records)\n",
    "    results_df = results_df.sort_values(\"spearman_corr\", ascending=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_hallucination', lang='en'). All correlations will be NaN.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_answer_acc', lang='en'). All correlations will be NaN.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_user_sat', lang='en'). All correlations will be NaN.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_coherence', lang='en'). All correlations will be NaN.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_context_qual', lang='en'). All correlations will be NaN.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_overall', lang='en'). All correlations will be NaN.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_file</th>\n",
       "      <th>metric_column</th>\n",
       "      <th>human_column</th>\n",
       "      <th>language</th>\n",
       "      <th>spearman_corr</th>\n",
       "      <th>spearman_pval</th>\n",
       "      <th>pearson_corr</th>\n",
       "      <th>pearson_pval</th>\n",
       "      <th>kendall_corr</th>\n",
       "      <th>kendall_pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>context_quality_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.687466</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.686956</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.589667</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>llm_judge_together_with_ref_de.csv</td>\n",
       "      <td>weighted_overall_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.642921</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.627938</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.511327</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>llm_judge_together_with_ref_de.csv</td>\n",
       "      <td>context_quality_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.635773</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.542320</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.537594</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>weighted_overall_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.614935</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.629960</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.484637</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>llm_judge_seperate_with_ref_en.csv</td>\n",
       "      <td>coherence_clarity_fluency_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>en</td>\n",
       "      <td>0.610229</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.511205</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.512771</td>\n",
       "      <td>0.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>answer_accuracy_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.609065</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.598206</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.507115</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>llm_judge_seperate_no_ref_de.csv</td>\n",
       "      <td>weighted_overall_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.604513</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.426236</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>0.468992</td>\n",
       "      <td>0.000591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>user_satisfaction_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.604144</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.609755</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.507115</td>\n",
       "      <td>0.000353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>llm_judge_together_with_ref_de.csv</td>\n",
       "      <td>answer_accuracy_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.599864</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.601339</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.500129</td>\n",
       "      <td>0.000401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>llm_judge_together_with_ref_de.csv</td>\n",
       "      <td>user_satisfaction_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.599864</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.601339</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.500129</td>\n",
       "      <td>0.000401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            metric_file                    metric_column  \\\n",
       "410  llm_judge_seperate_with_ref_de.csv            context_quality_score   \n",
       "272  llm_judge_together_with_ref_de.csv           weighted_overall_score   \n",
       "266  llm_judge_together_with_ref_de.csv            context_quality_score   \n",
       "416  llm_judge_seperate_with_ref_de.csv           weighted_overall_score   \n",
       "440  llm_judge_seperate_with_ref_en.csv  coherence_clarity_fluency_score   \n",
       "392  llm_judge_seperate_with_ref_de.csv            answer_accuracy_score   \n",
       "344    llm_judge_seperate_no_ref_de.csv           weighted_overall_score   \n",
       "398  llm_judge_seperate_with_ref_de.csv          user_satisfaction_score   \n",
       "248  llm_judge_together_with_ref_de.csv            answer_accuracy_score   \n",
       "254  llm_judge_together_with_ref_de.csv          user_satisfaction_score   \n",
       "\n",
       "     human_column language  spearman_corr  spearman_pval  pearson_corr  \\\n",
       "410  avg_user_sat       de       0.687466       0.000010      0.686956   \n",
       "272  avg_user_sat       de       0.642921       0.000055      0.627938   \n",
       "266  avg_user_sat       de       0.635773       0.000070      0.542320   \n",
       "416  avg_user_sat       de       0.614935       0.000140      0.629960   \n",
       "440  avg_user_sat       en       0.610229       0.000163      0.511205   \n",
       "392  avg_user_sat       de       0.609065       0.000169      0.598206   \n",
       "344  avg_user_sat       de       0.604513       0.000195      0.426236   \n",
       "398  avg_user_sat       de       0.604144       0.000197      0.609755   \n",
       "248  avg_user_sat       de       0.599864       0.000224      0.601339   \n",
       "254  avg_user_sat       de       0.599864       0.000224      0.601339   \n",
       "\n",
       "     pearson_pval  kendall_corr  kendall_pval  \n",
       "410      0.000010      0.589667      0.000045  \n",
       "272      0.000091      0.511327      0.000137  \n",
       "266      0.001113      0.537594      0.000222  \n",
       "416      0.000085      0.484637      0.000264  \n",
       "440      0.002363      0.512771      0.000459  \n",
       "392      0.000236      0.507115      0.000336  \n",
       "344      0.013384      0.468992      0.000591  \n",
       "398      0.000165      0.507115      0.000353  \n",
       "248      0.000215      0.500129      0.000401  \n",
       "254      0.000215      0.500129      0.000401  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage (adapt to your paths and config):\n",
    "human_eval_csv = \"../../testing/human_eval_aggregated.csv\"\n",
    "metrics_dir = \"../../data/eval\"\n",
    "\n",
    "# Each CSV file plus the columns you want to correlate\n",
    "metrics_config = {\n",
    "    # ROUGE\n",
    "    \"rouge_evaluation_de.csv\": [\n",
    "        \"ROUGE-1_f\",\n",
    "        \"ROUGE-2_f\",\n",
    "        \"ROUGE-3_f\",\n",
    "        \"ROUGE-4_f\",\n",
    "        \"ROUGE-L_f\",\n",
    "        \"ROUGE-SU4_f\",\n",
    "        \"ROUGE-W-1.2_f\"],\n",
    "    \"rouge_evaluation_en.csv\": [\n",
    "        \"ROUGE-1_f\",\n",
    "        \"ROUGE-2_f\",\n",
    "        \"ROUGE-3_f\",\n",
    "        \"ROUGE-4_f\",\n",
    "        \"ROUGE-L_f\",\n",
    "        \"ROUGE-SU4_f\",\n",
    "        \"ROUGE-W-1.2_f\"],\n",
    "    #BLEU\n",
    "    \"bleu_evaluation_de.csv\": [\n",
    "        \"BLEU\"\n",
    "    ],\n",
    "    \"bleu_evaluation_en.csv\": [\n",
    "        \"BLEU\"\n",
    "    ],\n",
    "    # BERTScore\n",
    "    \"bertscore_evaluation_de.csv\": [\n",
    "        \"BERTScore_F1\",\n",
    "    ],\n",
    "    \"bertscore_evaluation_en.csv\": [\n",
    "        \"BERTScore_F1\",\n",
    "    ],\n",
    "    # BARTScore\n",
    "    \"bartscore_cnn_de.csv\": [\n",
    "        \"BARTScore_paper_avg\", \n",
    "        \"BARTScore_paper_harm\"\n",
    "    ],\n",
    "    \"bartscore_cnn_en.csv\": [\n",
    "        \"BARTScore_paper_avg\", \n",
    "        \"BARTScore_paper_harm\"\n",
    "    ],\n",
    "    \"bartscore_multi_de.csv\": [\n",
    "        \"BARTScore_multilang_avg\", \n",
    "        \"BARTScore_multilang_harm\"\n",
    "    ],\n",
    "    \"bartscore_multi_en.csv\": [\n",
    "        \"BARTScore_multilang_avg\", \n",
    "        \"BARTScore_multilang_harm\"\n",
    "    ],\n",
    "    # BLEURT\n",
    "    \"bleurt_evaluation_de.csv\": [\n",
    "        \"BLEURT\"\n",
    "    ],\n",
    "    \"bleurt_evaluation_en.csv\": [\n",
    "        \"BLEURT\"\n",
    "    ],\n",
    "    # LLM Judge\n",
    "    # together no ref\n",
    "    \"llm_judge_together_no_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"weighted_overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_together_no_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"weighted_overall_score\"\n",
    "    ],\n",
    "    # together with ref\n",
    "    \"llm_judge_together_with_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"weighted_overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_together_with_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"weighted_overall_score\"\n",
    "    ],\n",
    "    # seperate no ref\n",
    "    \"llm_judge_seperate_no_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"weighted_overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_seperate_no_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"weighted_overall_score\"\n",
    "    ],\n",
    "    # seperate with ref\n",
    "    \"llm_judge_seperate_with_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"weighted_overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_seperate_with_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"weighted_overall_score\"\n",
    "    ],\n",
    "    \n",
    "}\n",
    "\n",
    "# Compute correlations\n",
    "df_results = compute_correlations_by_language_2(\n",
    "    human_eval_path=human_eval_csv,\n",
    "    metrics_folder=metrics_dir,\n",
    "    metrics_files=metrics_config,\n",
    "    question_id_human=\"qid\",          \n",
    "    question_id_metric=\"question_id_q\"\n",
    ")\n",
    "\n",
    "df_results.head(10)\n",
    "# Optionally save to CSV\n",
    "# df_results.to_csv(\"correlation_splits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric_file</th>\n",
       "      <th>metric_column</th>\n",
       "      <th>human_column</th>\n",
       "      <th>language</th>\n",
       "      <th>spearman_corr</th>\n",
       "      <th>spearman_pval</th>\n",
       "      <th>pearson_corr</th>\n",
       "      <th>pearson_pval</th>\n",
       "      <th>kendall_corr</th>\n",
       "      <th>kendall_pval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>context_quality_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.687466</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.686956</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.589667</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>llm_judge_together_with_ref_de.csv</td>\n",
       "      <td>weighted_overall_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.642921</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.627938</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.511327</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>llm_judge_together_with_ref_de.csv</td>\n",
       "      <td>context_quality_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.635773</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.542320</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.537594</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>weighted_overall_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>de</td>\n",
       "      <td>0.614935</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.629960</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.484637</td>\n",
       "      <td>0.000264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>llm_judge_seperate_with_ref_en.csv</td>\n",
       "      <td>coherence_clarity_fluency_score</td>\n",
       "      <td>avg_user_sat</td>\n",
       "      <td>en</td>\n",
       "      <td>0.610229</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.511205</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.512771</td>\n",
       "      <td>0.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>coherence_clarity_fluency_score</td>\n",
       "      <td>avg_context_qual</td>\n",
       "      <td>de</td>\n",
       "      <td>0.363394</td>\n",
       "      <td>0.037644</td>\n",
       "      <td>0.251768</td>\n",
       "      <td>0.157533</td>\n",
       "      <td>0.277132</td>\n",
       "      <td>0.056248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>llm_judge_seperate_with_ref_en.csv</td>\n",
       "      <td>hallucination_score</td>\n",
       "      <td>avg_overall</td>\n",
       "      <td>en</td>\n",
       "      <td>0.358557</td>\n",
       "      <td>0.040459</td>\n",
       "      <td>0.375290</td>\n",
       "      <td>0.031389</td>\n",
       "      <td>0.297102</td>\n",
       "      <td>0.042409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>bartscore_multi_de.csv</td>\n",
       "      <td>BARTScore_multilang_harm</td>\n",
       "      <td>avg_context_qual</td>\n",
       "      <td>de</td>\n",
       "      <td>0.352304</td>\n",
       "      <td>0.044348</td>\n",
       "      <td>0.328928</td>\n",
       "      <td>0.061612</td>\n",
       "      <td>0.232464</td>\n",
       "      <td>0.065548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>llm_judge_together_no_ref_de.csv</td>\n",
       "      <td>context_quality_score</td>\n",
       "      <td>avg_overall</td>\n",
       "      <td>de</td>\n",
       "      <td>0.351275</td>\n",
       "      <td>0.045016</td>\n",
       "      <td>0.388344</td>\n",
       "      <td>0.025528</td>\n",
       "      <td>0.300960</td>\n",
       "      <td>0.045525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>llm_judge_seperate_with_ref_de.csv</td>\n",
       "      <td>coherence_clarity_fluency_score</td>\n",
       "      <td>avg_overall</td>\n",
       "      <td>de</td>\n",
       "      <td>0.347401</td>\n",
       "      <td>0.047602</td>\n",
       "      <td>0.255979</td>\n",
       "      <td>0.150471</td>\n",
       "      <td>0.281492</td>\n",
       "      <td>0.055248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            metric_file                    metric_column  \\\n",
       "410  llm_judge_seperate_with_ref_de.csv            context_quality_score   \n",
       "272  llm_judge_together_with_ref_de.csv           weighted_overall_score   \n",
       "266  llm_judge_together_with_ref_de.csv            context_quality_score   \n",
       "416  llm_judge_seperate_with_ref_de.csv           weighted_overall_score   \n",
       "440  llm_judge_seperate_with_ref_en.csv  coherence_clarity_fluency_score   \n",
       "..                                  ...                              ...   \n",
       "406  llm_judge_seperate_with_ref_de.csv  coherence_clarity_fluency_score   \n",
       "425  llm_judge_seperate_with_ref_en.csv              hallucination_score   \n",
       "142              bartscore_multi_de.csv         BARTScore_multilang_harm   \n",
       "197    llm_judge_together_no_ref_de.csv            context_quality_score   \n",
       "407  llm_judge_seperate_with_ref_de.csv  coherence_clarity_fluency_score   \n",
       "\n",
       "         human_column language  spearman_corr  spearman_pval  pearson_corr  \\\n",
       "410      avg_user_sat       de       0.687466       0.000010      0.686956   \n",
       "272      avg_user_sat       de       0.642921       0.000055      0.627938   \n",
       "266      avg_user_sat       de       0.635773       0.000070      0.542320   \n",
       "416      avg_user_sat       de       0.614935       0.000140      0.629960   \n",
       "440      avg_user_sat       en       0.610229       0.000163      0.511205   \n",
       "..                ...      ...            ...            ...           ...   \n",
       "406  avg_context_qual       de       0.363394       0.037644      0.251768   \n",
       "425       avg_overall       en       0.358557       0.040459      0.375290   \n",
       "142  avg_context_qual       de       0.352304       0.044348      0.328928   \n",
       "197       avg_overall       de       0.351275       0.045016      0.388344   \n",
       "407       avg_overall       de       0.347401       0.047602      0.255979   \n",
       "\n",
       "     pearson_pval  kendall_corr  kendall_pval  \n",
       "410      0.000010      0.589667      0.000045  \n",
       "272      0.000091      0.511327      0.000137  \n",
       "266      0.001113      0.537594      0.000222  \n",
       "416      0.000085      0.484637      0.000264  \n",
       "440      0.002363      0.512771      0.000459  \n",
       "..            ...           ...           ...  \n",
       "406      0.157533      0.277132      0.056248  \n",
       "425      0.031389      0.297102      0.042409  \n",
       "142      0.061612      0.232464      0.065548  \n",
       "197      0.025528      0.300960      0.045525  \n",
       "407      0.150471      0.281492      0.055248  \n",
       "\n",
       "[70 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for p-value < 0.05\n",
    "df_results_filtered = df_results[df_results[\"spearman_pval\"] < 0.05]\n",
    "df_results_filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
