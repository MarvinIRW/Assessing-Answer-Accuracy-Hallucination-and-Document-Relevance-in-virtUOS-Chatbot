{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_spearman_correlations_by_language(\n",
    "    human_eval_path,\n",
    "    metrics_folder,\n",
    "    metrics_files,\n",
    "    question_id_human=\"qid\",\n",
    "    question_id_metric=\"question_id_q\",\n",
    "    human_eval_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes Spearman correlations between human evaluation columns and \n",
    "    multiple automatic metric CSVs, separately for German (langq='de') and \n",
    "    English (langq='en') question-answer pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    human_eval_path : str\n",
    "        Path to the 'human_eval.csv' file.\n",
    "    metrics_folder : str\n",
    "        Folder containing your automatic metric CSV files.\n",
    "    metrics_files : dict\n",
    "        A dict that maps a CSV filename (e.g., 'bartscore_de_cnn.csv') \n",
    "        to a list of columns in that file to correlate.\n",
    "        Example:\n",
    "            {\n",
    "                \"bartscore_de_cnn.csv\": [\"BARTScore_paper_avg\", \"BARTScore_paper_harm\"],\n",
    "                \"bleu_evaluation_en.csv\": [\"BLEU\"],\n",
    "                ...\n",
    "            }\n",
    "    question_id_human : str, optional\n",
    "        The column name in the human eval CSV used to identify the question ID\n",
    "        (default \"qid\").\n",
    "    question_id_metric : str, optional\n",
    "        The column name in the metric CSV used to identify the question ID\n",
    "        (default \"question_id_q\").\n",
    "    human_eval_cols : list of str, optional\n",
    "        Which human-eval columns to compare against each metric.\n",
    "        Defaults to the columns in your provided CSV example.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : pd.DataFrame\n",
    "        A DataFrame with columns:\n",
    "        [\"metric_file\", \"metric_column\", \"human_column\", \"language\", \n",
    "         \"spearman_corr\", \"p_value\"]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function automatically splits the human-eval data into two subsets:\n",
    "      one for 'langq' == 'de' and one for 'langq' == 'en'.\n",
    "    - It detects whether a metric file is meant for German or English by\n",
    "      checking if the filename contains '_de' or '_en'.\n",
    "    - Merges are done on question ID columns. Only rows present in both\n",
    "      data sets are used in the correlation.\n",
    "    \"\"\"\n",
    "    # 1) Read the human evaluation CSV\n",
    "    human_eval_df = pd.read_csv(human_eval_path)\n",
    "\n",
    "    # 2) Split the human-eval data into DE and EN subsets\n",
    "    human_eval_de = human_eval_df[human_eval_df[\"langq\"] == \"de\"].copy()\n",
    "    human_eval_en = human_eval_df[human_eval_df[\"langq\"] == \"en\"].copy()\n",
    "\n",
    "    # Default columns if none are specified\n",
    "    if human_eval_cols is None:\n",
    "        human_eval_cols = [\n",
    "            \"avg_hallucination\",\n",
    "            \"avg_answer_acc\",\n",
    "            \"avg_user_sat\",\n",
    "            \"avg_coherence\",\n",
    "            \"avg_context_qual\",\n",
    "            \"avg_overall\",\n",
    "            \"overall_mean\"\n",
    "        ]\n",
    "    \n",
    "    # Prepare a list to accumulate correlation records\n",
    "    correlation_records = []\n",
    "\n",
    "    # 3) Loop over each metric file and correlate with the matching language subset\n",
    "    for metric_filename, metric_cols in metrics_files.items():\n",
    "        metric_path = os.path.join(metrics_folder, metric_filename)\n",
    "        \n",
    "        # Check if it's a DE file or an EN file\n",
    "        if \"_de\" in metric_filename.lower():\n",
    "            relevant_human_eval = human_eval_de\n",
    "            language = \"de\"\n",
    "        elif \"_en\" in metric_filename.lower():\n",
    "            relevant_human_eval = human_eval_en\n",
    "            language = \"en\"\n",
    "        else:\n",
    "            # If neither _de nor _en is found, skip or raise a warning\n",
    "            print(f\"Warning: '{metric_filename}' does not indicate 'de' or 'en'; skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Read the metric CSV\n",
    "        if not os.path.exists(metric_path):\n",
    "            print(f\"File not found: {metric_path}\")\n",
    "            continue\n",
    "        \n",
    "        metric_df = pd.read_csv(metric_path)\n",
    "        \n",
    "        # Merge on the question IDs\n",
    "        merged = pd.merge(\n",
    "            relevant_human_eval,\n",
    "            metric_df,\n",
    "            how=\"inner\",\n",
    "            left_on=question_id_human,\n",
    "            right_on=question_id_metric\n",
    "        )\n",
    "\n",
    "        # 4) For each metric column, compute correlation with each human_eval_col\n",
    "        for metric_col in metric_cols:\n",
    "            if metric_col not in merged.columns:\n",
    "                print(f\"Column '{metric_col}' not found in '{metric_filename}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Drop rows with missing data to avoid correlation errors\n",
    "            \n",
    "            valid_data = merged.dropna(subset=human_eval_cols + [metric_col])\n",
    "\n",
    "            for human_col in human_eval_cols:\n",
    "                x = valid_data[human_col]\n",
    "                y = valid_data[metric_col]\n",
    "\n",
    "                # Check if x or y is constant (zero variance)\n",
    "                if x.nunique() <= 1 or y.nunique() <= 1:\n",
    "                    print(\n",
    "                        f\"Warning: Constant input array encountered in file '{metric_filename}' \"\n",
    "                        f\"(metric_col='{metric_col}', human_col='{human_col}', lang='{language}'). \"\n",
    "                        \"Spearman's r is not defined for constant data.\"\n",
    "                    )\n",
    "                    # still compute it for completeness (will be NaN),\n",
    "                    \n",
    "                    r, pval = spearmanr(x, y)\n",
    "                else:\n",
    "                    # Regular correlation\n",
    "                    r, pval = spearmanr(x, y)\n",
    "                \n",
    "                correlation_records.append({\n",
    "                    \"metric_file\": metric_filename,\n",
    "                    \"metric_column\": metric_col,\n",
    "                    \"human_column\": human_col,\n",
    "                    \"language\": language,\n",
    "                    \"spearman_corr\": r,\n",
    "                    \"p_value\": pval\n",
    "                })\n",
    "\n",
    "    # 5) Convert list of records to a DataFrame\n",
    "    results_df = pd.DataFrame(correlation_records)\n",
    "    #results_df = results_df.sort_values(\"spearman_corr\", ascending=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_hallucination', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_answer_acc', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_user_sat', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_coherence', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_context_qual', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='avg_overall', lang='en'). Spearman's r is not defined for constant data.\n",
      "Warning: Constant input array encountered in file 'llm_judge_together_no_ref_en.csv' (metric_col='coherence_clarity_fluency_score', human_col='overall_mean', lang='en'). Spearman's r is not defined for constant data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wurch\\AppData\\Local\\Temp\\ipykernel_51864\\743644229.py:136: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, pval = spearmanr(x, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532, 6)\n"
     ]
    }
   ],
   "source": [
    "human_eval_csv = \"../../../data/human_eval_avg.csv\"\n",
    "metrics_dir = \"../../../data/eval\"\n",
    "\n",
    "# Each CSV file plus the columns you want to correlate\n",
    "metrics_config = {\n",
    "    # ROUGE\n",
    "    \"rouge_evaluation_de.csv\": [\n",
    "        \"ROUGE-1_f\",\n",
    "        \"ROUGE-2_f\",\n",
    "        \"ROUGE-3_f\",\n",
    "        \"ROUGE-4_f\",\n",
    "        \"ROUGE-L_f\",\n",
    "        \"ROUGE-SU4_f\",\n",
    "        \"ROUGE-W-1.2_f\"],\n",
    "    \"rouge_evaluation_en.csv\": [\n",
    "        \"ROUGE-1_f\",\n",
    "        \"ROUGE-2_f\",\n",
    "        \"ROUGE-3_f\",\n",
    "        \"ROUGE-4_f\",\n",
    "        \"ROUGE-L_f\",\n",
    "        \"ROUGE-SU4_f\",\n",
    "        \"ROUGE-W-1.2_f\"],\n",
    "    #BLEU\n",
    "    \"bleu_evaluation_de.csv\": [\n",
    "        \"BLEU\"\n",
    "    ],\n",
    "    \"bleu_evaluation_en.csv\": [\n",
    "        \"BLEU\"\n",
    "    ],\n",
    "    # BERTScore\n",
    "    \"bertscore_evaluation_de.csv\": [\n",
    "        \"BERTScore_F1\",\n",
    "    ],\n",
    "    \"bertscore_evaluation_en.csv\": [\n",
    "        \"BERTScore_F1\",\n",
    "    ],\n",
    "    # BARTScore\n",
    "    \"bartscore_cnn_de.csv\": [\n",
    "        \"BARTScore_paper_avg\", \n",
    "        \"BARTScore_paper_harm\"\n",
    "    ],\n",
    "    \"bartscore_cnn_en.csv\": [\n",
    "        \"BARTScore_paper_avg\", \n",
    "        \"BARTScore_paper_harm\"\n",
    "    ],\n",
    "    \"bartscore_multi_de.csv\": [\n",
    "        \"BARTScore_multilang_avg\", \n",
    "        \"BARTScore_multilang_harm\"\n",
    "    ],\n",
    "    \"bartscore_multi_en.csv\": [\n",
    "        \"BARTScore_multilang_avg\", \n",
    "        \"BARTScore_multilang_harm\"\n",
    "    ],\n",
    "    # BLEURT\n",
    "    \"bleurt_evaluation_de.csv\": [\n",
    "        \"BLEURT\"\n",
    "    ],\n",
    "    \"bleurt_evaluation_en.csv\": [\n",
    "        \"BLEURT\"\n",
    "    ],\n",
    "    # LLM Judge\n",
    "    # together no ref\n",
    "    \"llm_judge_together_no_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_together_no_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    # together with ref\n",
    "    \"llm_judge_together_with_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_together_with_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    # seperate no ref\n",
    "    \"llm_judge_seperate_no_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_seperate_no_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    # seperate with ref\n",
    "    \"llm_judge_seperate_with_ref_de.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    \"llm_judge_seperate_with_ref_en.csv\": [\n",
    "        \"hallucination_score\",\n",
    "        \"answer_accuracy_score\",\n",
    "        \"user_satisfaction_score\",\n",
    "        \"coherence_clarity_fluency_score\",\n",
    "        \"context_quality_score\",\n",
    "        \"overall_score\"\n",
    "    ],\n",
    "    \n",
    "}\n",
    "\n",
    "# Compute correlations\n",
    "df_results = compute_spearman_correlations_by_language(\n",
    "    human_eval_path=human_eval_csv,\n",
    "    metrics_folder=metrics_dir,\n",
    "    metrics_files=metrics_config,\n",
    "    question_id_human=\"qid\",          \n",
    "    question_id_metric=\"question_id_q\"\n",
    ")\n",
    "\n",
    "print(df_results.shape)\n",
    "# save to CSV\n",
    "df_results.to_csv(\"../../../data/eval/correlation/correlation_splits_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 6)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###filters out the unmatching LLM-as-a-Judge scores\n",
    "# Update the mapping to allow multiple mappings per LLM metric\n",
    "llm_human_mapping = {\n",
    "    \"hallucination_score\": [\"avg_hallucination\"],\n",
    "    \"answer_accuracy_score\": [\"avg_answer_acc\"],\n",
    "    \"user_satisfaction_score\": [\"avg_user_sat\"],\n",
    "    \"coherence_clarity_fluency_score\": [\"avg_coherence\"],\n",
    "    \"context_quality_score\": [\"avg_context_qual\"],\n",
    "    \"overall_score\": [\"avg_overall\", \"overall_mean\"]  # Multiple mappings for overall_score\n",
    "}\n",
    "# Filter for LLM-as-a-judge metrics\n",
    "llm_files = [\n",
    "    'llm_judge_together_no_ref_de.csv',\n",
    "    'llm_judge_together_no_ref_en.csv',\n",
    "    'llm_judge_together_with_ref_de.csv',\n",
    "    'llm_judge_together_with_ref_en.csv',\n",
    "    'llm_judge_seperate_no_ref_de.csv',\n",
    "    'llm_judge_seperate_no_ref_en.csv',\n",
    "    'llm_judge_seperate_with_ref_de.csv',\n",
    "    'llm_judge_seperate_with_ref_en.csv'\n",
    "]\n",
    "# Filter DataFrame for LLM-as-a-judge rows\n",
    "filtered_llm_df = df_results[\n",
    "    df_results['metric_file'].isin(llm_files)\n",
    "]\n",
    "\n",
    "# Apply the updated mapping for LLM-as-a-judge metrics\n",
    "filtered_llm_df = filtered_llm_df[\n",
    "    filtered_llm_df.apply(\n",
    "        lambda row: row['human_column'] in llm_human_mapping.get(row['metric_column'], []),\n",
    "        axis=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Combine with the other metrics (no filtering needed for these)\n",
    "other_files = [file for file in df_results['metric_file'].unique() if file not in llm_files]\n",
    "\n",
    "filtered_other_df = df_results[\n",
    "    df_results['metric_file'].isin(other_files)\n",
    "]\n",
    "\n",
    "# Concatenate the filtered LLM-as-a-judge DataFrame with other metrics\n",
    "final_filtered_df = pd.concat([filtered_llm_df, filtered_other_df], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "final_filtered_df.to_csv(\"../../../data/eval/correlation/correlation_splits_filtered_llm.csv\", index=False)\n",
    "\n",
    "final_filtered_df.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
