{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def _compute_randomgroup_values(dataset_length: int, groups_in_total: int) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of length `dataset_length` of integers in the range [1..groups_in_total],\n",
    "    distributed as evenly as possible. For example, if dataset_length=10 and groups_in_total=3,\n",
    "    the result might look like [1,1,1,2,2,2,3,3,3,3] (order is by group, which is simplest).\n",
    "    \"\"\"\n",
    "    # How many items per group?\n",
    "    base_count = dataset_length // groups_in_total   # e.g. 10 // 3 = 3\n",
    "    leftover   = dataset_length % groups_in_total    # e.g. 10 % 3 = 1\n",
    "\n",
    "    # For each group, compute how many items it should have\n",
    "    counts_per_group = [base_count]*groups_in_total\n",
    "    for i in range(leftover):\n",
    "        counts_per_group[i] += 1\n",
    "\n",
    "    # Build the result list: group 1 repeated counts_per_group[0] times, group 2 repeated ...\n",
    "    group_values = []\n",
    "    for group_id in range(1, groups_in_total+1):\n",
    "        group_values += [group_id]*counts_per_group[group_id-1]\n",
    "\n",
    "    return group_values  # length == dataset_length\n",
    "\n",
    "def add_question_groups(\n",
    "    input_lss: str,\n",
    "    output_lss: str,\n",
    "    original_group_id: str,\n",
    "    dataset_length: int,       # total number of clones we want\n",
    "    gid_start: int,\n",
    "    qid_start: int,\n",
    "    groups_in_total: int       # how many distinct randomgroup values to distribute\n",
    "):\n",
    "    \"\"\"\n",
    "    Clones a group in a LimeSurvey .lss file exactly `dataset_length` times, producing\n",
    "    dataset_length total groups (including the original). Each cloned group's group_relevance\n",
    "    is set to randomgroup==X, where X is determined by evenly distributing X in the range\n",
    "    [1.. groups_in_total].\n",
    "\n",
    "    :param input_lss:         Path to your existing .lss file\n",
    "    :param output_lss:        Path for the new .lss file\n",
    "    :param original_group_id: The gid (string) of the group we want to clone\n",
    "    :param dataset_length:    Number of clones to create (equivalent to how many questionâ€“answer rows)\n",
    "                             Because the original group already exists, total groups will be\n",
    "                             (1 original + dataset_length - 1) in practice.\n",
    "    :param gid_start:         Starting offset for new GIDs\n",
    "    :param qid_start:         Starting offset for new QIDs\n",
    "    :param groups_in_total:   The total number of distinct randomgroup values; distributed evenly\n",
    "                             across all dataset_length clones.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Parse the .lss and find main sections\n",
    "    tree = ET.parse(input_lss)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    groups_elem       = root.find(\"./groups\")\n",
    "    questions_elem    = root.find(\"./questions\")\n",
    "    subquestions_elem = root.find(\"./subquestions\")\n",
    "    answers_elem      = root.find(\"./answers\")\n",
    "    qattributes_elem  = root.find(\"./question_attributes\")  # For copying random_order, etc.\n",
    "\n",
    "    if not (groups_elem and questions_elem and subquestions_elem and answers_elem):\n",
    "        raise ValueError(\"Could not locate <groups>, <questions>, <subquestions>, or <answers> in the LSS.\")\n",
    "\n",
    "    group_rows      = groups_elem.find(\"rows\")\n",
    "    question_rows   = questions_elem.find(\"rows\")\n",
    "    subquestion_rows= subquestions_elem.find(\"rows\")\n",
    "    answer_rows     = answers_elem.find(\"rows\")\n",
    "\n",
    "    if not (group_rows and question_rows and subquestion_rows and answer_rows):\n",
    "        raise ValueError(\"One of the <rows> sections is missing inside <groups>, <questions>, <subquestions>, or <answers>.\")\n",
    "\n",
    "    # We'll also gather the question_attributes rows if present\n",
    "    qattr_rows = qattributes_elem.find(\"rows\") if qattributes_elem is not None else None\n",
    "\n",
    "    # 2) Gather the original group entries (both languages) with gid=original_group_id\n",
    "    original_group_entries = [\n",
    "        row for row in group_rows.findall(\"row\")\n",
    "        if row.find(\"gid\").text == original_group_id\n",
    "    ]\n",
    "    if not original_group_entries:\n",
    "        raise ValueError(f\"Could not find group with gid={original_group_id}.\")\n",
    "\n",
    "    # 3) Identify all QIDs from that group (questions + subquestions)\n",
    "    original_qid_list = []\n",
    "    # A) Main questions in that group\n",
    "    these_question_rows = [\n",
    "        row for row in question_rows.findall(\"row\")\n",
    "        if row.find(\"gid\").text == original_group_id\n",
    "    ]\n",
    "    original_qid_list.extend(row.find(\"qid\").text for row in these_question_rows)\n",
    "\n",
    "    # B) Subquestions in that group\n",
    "    these_subquestion_rows = [\n",
    "        row for row in subquestion_rows.findall(\"row\")\n",
    "        if row.find(\"gid\").text == original_group_id\n",
    "    ]\n",
    "    original_qid_list.extend(row.find(\"qid\").text for row in these_subquestion_rows)\n",
    "\n",
    "    original_qid_list = list(set(original_qid_list))  # deduplicate\n",
    "\n",
    "    # 4) Create a mapping {old_qid -> new_qid}, so each old QID gets a new unique QID\n",
    "    qid_counter = qid_start\n",
    "    old_qid_to_new_qid = {}\n",
    "    for old_qid in sorted(original_qid_list):\n",
    "        old_qid_to_new_qid[old_qid] = str(qid_counter)\n",
    "        qid_counter += 1\n",
    "\n",
    "    # 5) We replicate the group for dataset_length times (which effectively yields\n",
    "    #    (1 original + dataset_length-1) new copies in total).\n",
    "    additional_copies = dataset_length - 1\n",
    "    gid_counter = gid_start\n",
    "\n",
    "    # Prepare all new <row> elements so we can append them later\n",
    "    all_new_group_rows       = []\n",
    "    all_new_question_rows    = []\n",
    "    all_new_subquestion_rows = []\n",
    "    all_new_answer_rows      = []\n",
    "    all_new_qattr_rows       = []\n",
    "\n",
    "    # Generate the randomgroup distribution\n",
    "    randomgroup_values = _compute_randomgroup_values(dataset_length, groups_in_total)\n",
    "    # randomgroup_values is a list of length dataset_length,\n",
    "    # e.g. [1,1,2,2,2,3,3,...] as evenly as possible.\n",
    "\n",
    "    for i in range(1, dataset_length):\n",
    "        # i goes from 1.. dataset_length-1\n",
    "        # We'll replicate the original group to produce \"copy #i\".\n",
    "        placeholder_number = i + 1              # For placeholders (ans2, qid2, etc.)\n",
    "        randomgroup_val    = randomgroup_values[i]  # e.g. 1..groups_in_total\n",
    "\n",
    "        #\n",
    "        # (A) Clone the group row(s) for each language\n",
    "        #\n",
    "        for orig_grp in original_group_entries:\n",
    "            new_grp = ET.fromstring(ET.tostring(orig_grp))\n",
    "\n",
    "            # Overwrite GID\n",
    "            new_grp.find(\"gid\").text = str(gid_counter)\n",
    "\n",
    "            # Optionally shift group_order\n",
    "            group_order_el = new_grp.find(\"group_order\")\n",
    "            if group_order_el is not None and group_order_el.text.isdigit():\n",
    "                old_val = int(group_order_el.text)\n",
    "                group_order_el.text = str(old_val + i)\n",
    "\n",
    "            # Update placeholders in <description> if needed\n",
    "            desc_el = new_grp.find(\"description\")\n",
    "            if desc_el is not None and desc_el.text:\n",
    "                desc_text = desc_el.text\n",
    "\n",
    "                # Replace placeholders in the group description\n",
    "                desc_text = desc_text.replace(\"PLACEHOLDER_QUESTION_1\",  f\"PLACEHOLDER_QUESTION_{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"PLACEHOLDER_ANSWER_1\",    f\"PLACEHOLDER_ANSWER_{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"PLACEHOLDER_CONTEXT_1\",   f\"PLACEHOLDER_CONTEXT_{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"PLACEHOLDER_ID_1\",        f\"PLACEHOLDER_ID_{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"PLACEHOLDER_LANG_1\",      f\"PLACEHOLDER_LANG_{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"ans1\",                    f\"ans{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"qid1\",                    f\"qid{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"langq1\",                  f\"langq{placeholder_number}\")\n",
    "                desc_text = desc_text.replace(\"comment1\",                f\"comment{placeholder_number}\")\n",
    "                desc_el.text = desc_text\n",
    "\n",
    "            # Update grelevance => e.g. \"randomgroup==3\"\n",
    "            grel_el = new_grp.find(\"grelevance\")\n",
    "            if grel_el is not None:\n",
    "                old_expr = grel_el.text or \"\"\n",
    "                if \"==1\" in old_expr:\n",
    "                    new_expr = old_expr.replace(\"==1\", f\"=={randomgroup_val}\")\n",
    "                elif not old_expr.strip():\n",
    "                    new_expr = f\"randomgroup=={randomgroup_val}\"\n",
    "                else:\n",
    "                    new_expr = old_expr\n",
    "                grel_el.text = new_expr\n",
    "\n",
    "            all_new_group_rows.append(new_grp)\n",
    "\n",
    "        #\n",
    "        # (B) Clone all main questions\n",
    "        #\n",
    "        for qrow in these_question_rows:\n",
    "            old_qid = qrow.find(\"qid\").text\n",
    "            new_qid = old_qid_to_new_qid[old_qid]  # e.g. '300000' + offset\n",
    "\n",
    "            new_q = ET.fromstring(ET.tostring(qrow))\n",
    "            # Overwrite GID + QID\n",
    "            new_q.find(\"gid\").text = str(gid_counter)\n",
    "            new_q.find(\"qid\").text = new_qid\n",
    "\n",
    "            # shift question_order\n",
    "            qo_el = new_q.find(\"question_order\")\n",
    "            if qo_el is not None and qo_el.text.isdigit():\n",
    "                old_val = int(qo_el.text)\n",
    "                qo_el.text = str(old_val + i)\n",
    "\n",
    "            # placeholders in <title> + <question>\n",
    "            title_el = new_q.find(\"title\")\n",
    "            if title_el is not None and title_el.text:\n",
    "                tmp = title_el.text\n",
    "                tmp = tmp.replace(\"ans1\",       f\"ans{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"qid1\",       f\"qid{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"langq1\",     f\"langq{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"comment1\",   f\"comment{placeholder_number}\")\n",
    "                title_el.text = tmp\n",
    "\n",
    "            question_el = new_q.find(\"question\")\n",
    "            if question_el is not None and question_el.text:\n",
    "                tmp = question_el.text\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_QUESTION_1\", f\"PLACEHOLDER_QUESTION_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_ANSWER_1\",   f\"PLACEHOLDER_ANSWER_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_CONTEXT_1\",  f\"PLACEHOLDER_CONTEXT_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_ID_1\",       f\"PLACEHOLDER_ID_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_LANG_1\",     f\"PLACEHOLDER_LANG_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"ans1\",                   f\"ans{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"qid1\",                   f\"qid{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"langq1\",                 f\"langq{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"comment1\",               f\"comment{placeholder_number}\")\n",
    "                question_el.text = tmp\n",
    "\n",
    "            all_new_question_rows.append(new_q)\n",
    "\n",
    "        #\n",
    "        # (C) Clone subquestions\n",
    "        #\n",
    "        for sqrow in these_subquestion_rows:\n",
    "            old_subq_qid   = sqrow.find(\"qid\").text\n",
    "            new_subq_qid   = old_qid_to_new_qid[old_subq_qid]\n",
    "            old_parent_qid = sqrow.find(\"parent_qid\").text\n",
    "            new_parent_qid = old_qid_to_new_qid[old_parent_qid]\n",
    "\n",
    "            new_sq = ET.fromstring(ET.tostring(sqrow))\n",
    "            new_sq.find(\"qid\").text        = new_subq_qid\n",
    "            new_sq.find(\"parent_qid\").text = new_parent_qid\n",
    "            new_sq.find(\"gid\").text        = str(gid_counter)\n",
    "\n",
    "            qo_el = new_sq.find(\"question_order\")\n",
    "            if qo_el is not None and qo_el.text.isdigit():\n",
    "                old_val = int(qo_el.text)\n",
    "                qo_el.text = str(old_val + i)\n",
    "\n",
    "            # placeholders\n",
    "            title_el = new_sq.find(\"title\")\n",
    "            if title_el is not None and title_el.text:\n",
    "                tmp = title_el.text\n",
    "                tmp = tmp.replace(\"ans1\",       f\"ans{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"qid1\",       f\"qid{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"langq1\",     f\"langq{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"comment1\",   f\"comment{placeholder_number}\")\n",
    "                title_el.text = tmp\n",
    "\n",
    "            question_el = new_sq.find(\"question\")\n",
    "            if question_el is not None and question_el.text:\n",
    "                tmp = question_el.text\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_QUESTION_1\", f\"PLACEHOLDER_QUESTION_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_ANSWER_1\",   f\"PLACEHOLDER_ANSWER_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_CONTEXT_1\",  f\"PLACEHOLDER_CONTEXT_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_ID_1\",       f\"PLACEHOLDER_ID_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"PLACEHOLDER_LANG_1\",     f\"PLACEHOLDER_LANG_{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"ans1\",                   f\"ans{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"qid1\",                   f\"qid{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"langq1\",                 f\"langq{placeholder_number}\")\n",
    "                tmp = tmp.replace(\"comment1\",               f\"comment{placeholder_number}\")\n",
    "                question_el.text = tmp\n",
    "\n",
    "            all_new_subquestion_rows.append(new_sq)\n",
    "\n",
    "        #\n",
    "        # (D) Clone answers\n",
    "        #\n",
    "        for old_qid in original_qid_list:\n",
    "            new_qid = old_qid_to_new_qid[old_qid]\n",
    "            relevant_answers = [\n",
    "                row for row in answer_rows.findall(\"row\")\n",
    "                if row.find(\"qid\").text == old_qid\n",
    "            ]\n",
    "            for ansrow in relevant_answers:\n",
    "                new_ans = ET.fromstring(ET.tostring(ansrow))\n",
    "                new_ans.find(\"qid\").text = new_qid\n",
    "\n",
    "                ans_text_el = new_ans.find(\"answer\")\n",
    "                if ans_text_el is not None and ans_text_el.text:\n",
    "                    tmp = ans_text_el.text\n",
    "                    tmp = tmp.replace(\"PLACEHOLDER_QUESTION_1\", f\"PLACEHOLDER_QUESTION_{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"PLACEHOLDER_ANSWER_1\",   f\"PLACEHOLDER_ANSWER_{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"PLACEHOLDER_CONTEXT_1\",  f\"PLACEHOLDER_CONTEXT_{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"PLACEHOLDER_ID_1\",       f\"PLACEHOLDER_ID_{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"PLACEHOLDER_LANG_1\",     f\"PLACEHOLDER_LANG_{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"ans1\",                   f\"ans{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"qid1\",                   f\"qid{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"langq1\",                 f\"langq{placeholder_number}\")\n",
    "                    tmp = tmp.replace(\"comment1\",               f\"comment{placeholder_number}\")\n",
    "                    ans_text_el.text = tmp\n",
    "\n",
    "                all_new_answer_rows.append(new_ans)\n",
    "\n",
    "        #\n",
    "        # (E) Clone question_attributes for these QIDs\n",
    "        #\n",
    "        if qattr_rows is not None:\n",
    "            for old_qid in original_qid_list:\n",
    "                new_qid = old_qid_to_new_qid[old_qid]\n",
    "                relevant_attrs = [\n",
    "                    row for row in qattr_rows.findall(\"row\")\n",
    "                    if row.find(\"qid\").text == old_qid\n",
    "                ]\n",
    "                for attrrow in relevant_attrs:\n",
    "                    new_attr = ET.fromstring(ET.tostring(attrrow))\n",
    "                    new_attr.find(\"qid\").text = new_qid\n",
    "                    all_new_qattr_rows.append(new_attr)\n",
    "\n",
    "        #\n",
    "        # (F) Bump GID for the next group,\n",
    "        #     then re-init the QID mapping for the next iteration.\n",
    "        #\n",
    "        gid_counter += 1\n",
    "\n",
    "        old_qid_to_new_qid = {}\n",
    "        tmp_qid_counter = qid_counter\n",
    "        for old_qid in sorted(original_qid_list):\n",
    "            old_qid_to_new_qid[old_qid] = str(tmp_qid_counter)\n",
    "            tmp_qid_counter += 1\n",
    "        qid_counter = tmp_qid_counter\n",
    "\n",
    "    #\n",
    "    # 6) Append all newly created rows\n",
    "    #\n",
    "    for elem in all_new_group_rows:\n",
    "        group_rows.append(elem)\n",
    "    for elem in all_new_question_rows:\n",
    "        question_rows.append(elem)\n",
    "    for elem in all_new_subquestion_rows:\n",
    "        subquestion_rows.append(elem)\n",
    "    for elem in all_new_answer_rows:\n",
    "        answer_rows.append(elem)\n",
    "\n",
    "    # If question_attributes exist, append new ones too\n",
    "    if qattr_rows is not None:\n",
    "        for elem in all_new_qattr_rows:\n",
    "            qattr_rows.append(elem)\n",
    "\n",
    "    #\n",
    "    # 7) Write the final .lss to disk\n",
    "    #\n",
    "    tree.write(output_lss, encoding=\"utf-8\", xml_declaration=True)\n",
    "    print(f\"Done! Created a survey with {dataset_length} copies of group {original_group_id}.\")\n",
    "    print(f\"Distinct randomgroup values used: {groups_in_total}. File saved to: {output_lss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from markdown import markdown\n",
    "import re\n",
    "\n",
    "def convert_markdown_to_html_with_target_blank(df, markdown_column, html_column):\n",
    "    \"\"\"\n",
    "    Converts Markdown content in a DataFrame column to HTML and ensures links open in a new tab.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing chatbot answers.\n",
    "        markdown_column (str): The name of the column with Markdown content.\n",
    "        html_column (str): The name of the column where the HTML output will be stored.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with the HTML content.\n",
    "    \"\"\"\n",
    "    # Check if the markdown_column exists in the DataFrame\n",
    "    if markdown_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{markdown_column}' not found in the DataFrame.\")\n",
    "    \n",
    "    def add_target_blank_to_links(html):\n",
    "        # Regex to find all <a> tags and add target=\"_blank\"\n",
    "        return re.sub(r'(<a href=\"[^\"]+\")', r'\\1 target=\"_blank\"', html)\n",
    "    \n",
    "    # Convert Markdown to HTML and add target=\"_blank\" to links\n",
    "    df[html_column] = df[markdown_column].apply(\n",
    "        lambda x: add_target_blank_to_links(markdown(x)) if isinstance(x, str) else x\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_context_links(df, context_column, output_column, no_context_text):\n",
    "    \"\"\"\n",
    "    Extracts all links after 'Information taken from:' and formats them as a simple HTML list.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing context data.\n",
    "        context_column (str): Column name containing the chatbot context.\n",
    "        output_column (str): Column name to store the formatted HTML list.\n",
    "        no_context_text (str): Text to display if no context\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with HTML-formatted links.\n",
    "    \"\"\"\n",
    "    def extract_links(context):\n",
    "        # Extract all links after \"Information taken from:\"\n",
    "        matches = re.findall(r'Information taken from: (https?://[^\\s]+)', context)\n",
    "        if matches:\n",
    "            # Format each link into an HTML list item\n",
    "            links_html = ''.join(f'<li><a href=\"{link}\" target=\"_blank\">{link}</a></li>' for link in matches)\n",
    "            return f\"<ul>{links_html}</ul>\"\n",
    "        return f\"<p>{no_context_text}</p>\"\n",
    "\n",
    "    # Apply the transformation\n",
    "    df[output_column] = df[context_column].apply(\n",
    "        lambda x: extract_links(x) if isinstance(x, str) else f\"<p>{no_context_text}</p>\"\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_shuffle_by_lang_and_context(\n",
    "    df: pd.DataFrame,\n",
    "    lang_col: str = \"language\",         # your column that says 'en' or 'de'\n",
    "    context_col: str = \"context_html\",  # the HTML context\n",
    "    shuffle_seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balances and shuffles df across two dimensions:\n",
    "      1) language: 'en' vs. 'de'\n",
    "      2) context:  has context vs. no context\n",
    "\n",
    "    Steps:\n",
    "      - Tag each row with 'en_withctx', 'en_nocontext', 'de_withctx', or 'de_nocontext'.\n",
    "      - Shuffle each category independently using the same seed.\n",
    "      - Round-robin to interleave them.\n",
    "\n",
    "    This version treats known placeholder strings such as\n",
    "    'No context was used by the chatbot.' or\n",
    "    'Der Chatbot hat keinen Kontext verwendet.'\n",
    "    as no-context.\n",
    "    \"\"\"\n",
    "    # shuffel\n",
    "    df = df.sample(frac=1, random_state=shuffle_seed).reset_index(drop=True)\n",
    "\n",
    "    # Known \"no context\" placeholders (case-insensitive).\n",
    "    NO_CONTEXT_PHRASES = {\n",
    "        \"no context was used by the chatbot.\",\n",
    "        \"der chatbot hat keinen kontext verwendet.\"\n",
    "    }\n",
    "\n",
    "    def has_context(val) -> bool:\n",
    "        \"\"\"\n",
    "        Returns True if val is a string that is neither empty nor one of the\n",
    "        known \"no context\" placeholders. Otherwise returns False.\n",
    "        \"\"\"\n",
    "        if not isinstance(val, str):\n",
    "            return False\n",
    "        lower_val = val.strip().lower()\n",
    "        # If it's empty or explicitly in our known no-context set => False\n",
    "        if not lower_val or lower_val in NO_CONTEXT_PHRASES:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # Tag each row as e.g. 'en_withctx', 'en_nocontext', ...\n",
    "    category_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        lang_str = row[lang_col].lower()  # 'en' or 'de'\n",
    "        # Adjust if your 'language' might be something else\n",
    "        if lang_str.startswith('de'):\n",
    "            lang_str = 'de'\n",
    "        else:\n",
    "            lang_str = 'en'\n",
    "\n",
    "        ctx_str = 'withctx' if has_context(row[context_col]) else 'nocontext'\n",
    "        category_list.append(f\"{lang_str}_{ctx_str}\")\n",
    "\n",
    "    df['_category'] = category_list\n",
    "\n",
    "    # Split into sub-DataFrames by category\n",
    "    subsets = {}\n",
    "    for cat in df['_category'].unique():\n",
    "        subset_cat = df[df['_category'] == cat].copy()\n",
    "        # Shuffle this subset with the same seed\n",
    "        subset_cat = subset_cat.sample(frac=1, random_state=shuffle_seed).reset_index(drop=True)\n",
    "        subsets[cat] = subset_cat\n",
    "\n",
    "    # Round-robin to interleave from each subset\n",
    "    cat_order = sorted(subsets.keys())  # e.g. [\"de_nocontext\", \"de_withctx\", \"en_nocontext\", \"en_withctx\"]\n",
    "    max_len   = max(len(s) for s in subsets.values())\n",
    "    output_rows = []\n",
    "\n",
    "    for i in range(max_len):\n",
    "        for cat in cat_order:\n",
    "            subdf = subsets[cat]\n",
    "            if i < len(subdf):\n",
    "                output_rows.append(subdf.iloc[i])\n",
    "\n",
    "    # Convert list-of-Series to a new DataFrame\n",
    "    df_balanced = pd.DataFrame(output_rows).reset_index(drop=True)\n",
    "    # Drop the helper column\n",
    "    df_balanced.drop(columns=['_category'], inplace=True)\n",
    "\n",
    "    return df_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from random import seed as py_seed\n",
    "\n",
    "def build_shuffled_questions_and_clone(\n",
    "    df_en: pd.DataFrame,\n",
    "    df_de: pd.DataFrame,\n",
    "    input_lss: str,\n",
    "    output_lss: str,\n",
    "    original_group_id: str,\n",
    "    gid_start: int,\n",
    "    qid_start: int,\n",
    "    groups_in_total: int,\n",
    "    shuffle_seed: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Takes two DataFrames (English + German) which already have columns like:\n",
    "         - english_question_text_q, chatbot_answer_en_html, formatted_context_en_html, question_id_q, question_language_q\n",
    "         - german_question_text_q,  chatbot_answer_de_html, formatted_context_de_html, question_id_q, question_language_q\n",
    "\n",
    "    2) Renames/standardizes these columns into:\n",
    "         [question_text, answer_html, context_html, question_id, language]\n",
    "\n",
    "    3) Stacks them into one DataFrame (df_together), each row = a question with:\n",
    "         question_text, answer_html, context_html, question_id, language\n",
    "\n",
    "    4) If shuffle_seed != None shuffles df_together using a fixed seed.\n",
    "\n",
    "    5) Calls the existing add_question_groups(...) function to replicate your .lss\n",
    "       so that the total # of groups = len(df_together). (One per row in df_together.)\n",
    "\n",
    "    6) Finally, parses that newly generated .lss and replaces the placeholders:\n",
    "         PLACEHOLDER_QUESTION_i  => df_together.iloc[i-1][\"question_text\"]\n",
    "         PLACEHOLDER_ANSWER_i    => df_together.iloc[i-1][\"answer_html\"]\n",
    "         PLACEHOLDER_CONTEXT_i   => df_together.iloc[i-1][\"context_html\"]\n",
    "         PLACEHOLDER_ID_i        => df_together.iloc[i-1][\"question_id\"]\n",
    "         PLACEHOLDER_LANG_i      => df_together.iloc[i-1][\"language\"]\n",
    "\n",
    "       for i in 1..len(df_together). The result is saved back to output_lss.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The final stacked & shuffled DataFrame (df_together).\n",
    "                      (For logging/inspection. The placeholders are replaced in the .lss on disk.)\n",
    "    \"\"\"\n",
    "    # --- 1) Standardize columns for df_en\n",
    "    df_en_copy = df_en.copy()\n",
    "    df_en_copy[\"question_text\"] = df_en_copy[\"english_question_text_q\"]\n",
    "    df_en_copy[\"answer_html\"]   = df_en_copy[\"chatbot_answer_en_html\"]\n",
    "    df_en_copy[\"context_html\"]  = df_en_copy[\"formatted_context_en_html\"]\n",
    "    df_en_copy[\"question_id\"]   = df_en_copy[\"question_id_q\"]\n",
    "    df_en_copy[\"language\"]      = df_en_copy[\"original_question_language\"]\n",
    "    df_en_copy[\"df_language\"]   = \"en\"\n",
    "\n",
    "    # Keep only the unified columns\n",
    "    df_en_final = df_en_copy[[\"question_text\", \"answer_html\", \"context_html\", \"question_id\", \"language\", \"df_language\"]]\n",
    "\n",
    "    # --- 2) Standardize columns for df_de\n",
    "    df_de_copy = df_de.copy()\n",
    "    df_de_copy[\"question_text\"] = df_de_copy[\"german_question_text_q\"]\n",
    "    df_de_copy[\"answer_html\"]   = df_de_copy[\"chatbot_answer_de_html\"]\n",
    "    df_de_copy[\"context_html\"]  = df_de_copy[\"formatted_context_de_html\"]\n",
    "    df_de_copy[\"question_id\"]   = df_de_copy[\"question_id_q\"]\n",
    "    df_de_copy[\"language\"]      = df_de_copy[\"original_question_language\"]\n",
    "    df_de_copy[\"df_language\"]   = \"de\"\n",
    "\n",
    "    df_de_final = df_de_copy[[\"question_text\", \"answer_html\", \"context_html\", \"question_id\", \"language\", \"df_language\"]]\n",
    "\n",
    "    # --- 3) Concatenate into one df_together\n",
    "    df_together = pd.concat([df_en_final, df_de_final], ignore_index=True)\n",
    "\n",
    "    # 4) Balanced shuffle across (en/de) + (with context/no context)\n",
    "    if shuffle_seed is not None:\n",
    "        df_together = balanced_shuffle_by_lang_and_context(\n",
    "            df_together,\n",
    "            lang_col=\"df_language\",\n",
    "            context_col=\"context_html\",\n",
    "            shuffle_seed=shuffle_seed\n",
    "        )\n",
    "\n",
    "    # --- 5) Clone the group in the .lss (using your existing add_question_groups function)\n",
    "    add_question_groups(\n",
    "        input_lss=input_lss,\n",
    "        output_lss=output_lss,\n",
    "        original_group_id=original_group_id,\n",
    "        dataset_length=len(df_together),\n",
    "        gid_start=gid_start,\n",
    "        qid_start=qid_start,\n",
    "        groups_in_total=groups_in_total\n",
    "    )\n",
    "\n",
    "    # --- 6) Now fill in the newly created placeholders with the actual data from df_together\n",
    "    _fill_placeholders_with_data(output_lss, df_together)\n",
    "\n",
    "    return df_together\n",
    "\n",
    "\n",
    "def _fill_placeholders_with_data(output_lss: str, df_together: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Internal helper to parse the newly generated LSS (output_lss) \n",
    "    and replace placeholder text with real data from df_together.\n",
    "\n",
    "    For i in [1..N], we replace:\n",
    "      PLACEHOLDER_QUESTION_{i} -> df_together.iloc[i-1][\"question_text\"]\n",
    "      PLACEHOLDER_ANSWER_{i}   -> df_together.iloc[i-1][\"answer_html\"]\n",
    "      PLACEHOLDER_CONTEXT_{i}  -> df_together.iloc[i-1][\"context_html\"]\n",
    "      PLACEHOLDER_ID_{i}       -> df_together.iloc[i-1][\"question_id\"]\n",
    "      PLACEHOLDER_LANG_{i}     -> df_together.iloc[i-1][\"language\"]\n",
    "\n",
    "    The function overwrites the same .lss on disk.\n",
    "    \"\"\"\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    # Load the .lss we just created\n",
    "    tree = ET.parse(output_lss)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # We define a small helper that does multi-line text replacement:\n",
    "    def replace_placeholders_in_text(original_text):\n",
    "        if not original_text:\n",
    "            return original_text\n",
    "        new_text = original_text\n",
    "\n",
    "        # **Key change**: we do replacements in descending order:\n",
    "        #   i = len(df_together), len(df_together)-1, ..., 2, 1\n",
    "        for i in range(len(df_together), 0, -1):\n",
    "            row = df_together.iloc[i - 1]\n",
    "            idx_str = str(i)\n",
    "\n",
    "            new_text = new_text.replace(f\"PLACEHOLDER_QUESTION_{idx_str}\", str(row[\"question_text\"]))\n",
    "            new_text = new_text.replace(f\"PLACEHOLDER_ANSWER_{idx_str}\",   str(row[\"answer_html\"]))\n",
    "            new_text = new_text.replace(f\"PLACEHOLDER_CONTEXT_{idx_str}\",  str(row[\"context_html\"]))\n",
    "            new_text = new_text.replace(f\"PLACEHOLDER_ID_{idx_str}\",       str(row[\"question_id\"]))\n",
    "            new_text = new_text.replace(f\"PLACEHOLDER_LANG_{idx_str}\",     str(row[\"df_language\"]))\n",
    "\n",
    "        return new_text\n",
    "\n",
    "    # Replace in <groups> -> <rows> -> <row> -> <description>\n",
    "    groups_elem = root.find(\"./groups\")\n",
    "    if groups_elem is not None:\n",
    "        rows_el = groups_elem.find(\"rows\")\n",
    "        if rows_el is not None:\n",
    "            for row_el in rows_el.findall(\"row\"):\n",
    "                desc_el = row_el.find(\"description\")\n",
    "                if desc_el is not None:\n",
    "                    desc_el.text = replace_placeholders_in_text(desc_el.text)\n",
    "\n",
    "    # Replace in <questions> -> <rows> -> <row> -> <question>/<title>\n",
    "    questions_elem = root.find(\"./questions\")\n",
    "    if questions_elem is not None:\n",
    "        qrows_el = questions_elem.find(\"rows\")\n",
    "        if qrows_el is not None:\n",
    "            for row_el in qrows_el.findall(\"row\"):\n",
    "                question_el = row_el.find(\"question\")\n",
    "                if question_el is not None:\n",
    "                    question_el.text = replace_placeholders_in_text(question_el.text)\n",
    "\n",
    "                title_el = row_el.find(\"title\")\n",
    "                if title_el is not None:\n",
    "                    title_el.text = replace_placeholders_in_text(title_el.text)\n",
    "\n",
    "    # Replace in <subquestions> -> <rows> -> <row> -> <question>/<title>\n",
    "    subquestions_elem = root.find(\"./subquestions\")\n",
    "    if subquestions_elem is not None:\n",
    "        sqrows_el = subquestions_elem.find(\"rows\")\n",
    "        if sqrows_el is not None:\n",
    "            for row_el in sqrows_el.findall(\"row\"):\n",
    "                question_el = row_el.find(\"question\")\n",
    "                if question_el is not None:\n",
    "                    question_el.text = replace_placeholders_in_text(question_el.text)\n",
    "\n",
    "                title_el = row_el.find(\"title\")\n",
    "                if title_el is not None:\n",
    "                    title_el.text = replace_placeholders_in_text(title_el.text)\n",
    "\n",
    "    # Replace in <answers> -> <rows> -> <row> -> <answer>\n",
    "    answers_elem = root.find(\"./answers\")\n",
    "    if answers_elem is not None:\n",
    "        arows_el = answers_elem.find(\"rows\")\n",
    "        if arows_el is not None:\n",
    "            for row_el in arows_el.findall(\"row\"):\n",
    "                ans_el = row_el.find(\"answer\")\n",
    "                if ans_el is not None:\n",
    "                    ans_el.text = replace_placeholders_in_text(ans_el.text)\n",
    "\n",
    "    # Finally, write back to the same .lss\n",
    "    tree.write(output_lss, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german length: 33 english length: 33\n",
      "Done! Created a survey with 66 copies of group 99600.\n",
      "Distinct randomgroup values used: 10. File saved to: ../../data/human_eval/survey_output.lss\n"
     ]
    }
   ],
   "source": [
    "# English dataset\n",
    "no_context_text_en = \"No context was used by the chatbot.\"\n",
    "no_context_text_de = \"Der Chatbot hat keinen Kontext verwendet.\"\n",
    "\n",
    "df_en = pd.read_csv(\"../../../data/short_dataset_en.csv\")\n",
    "df_en = convert_markdown_to_html_with_target_blank(\n",
    "    df_en, \n",
    "    markdown_column=\"chatbot_answer_en\", \n",
    "    html_column=\"chatbot_answer_en_html\")\n",
    "\n",
    "df_en = extract_context_links(\n",
    "    df_en, \n",
    "    context_column=\"chatbot_context_en\", \n",
    "    output_column=\"formatted_context_en_html\", \n",
    "    no_context_text=no_context_text_en)\n",
    "# German dataset\n",
    "df_de = pd.read_csv(\"../../../data/short_dataset_de.csv\")\n",
    "df_de = convert_markdown_to_html_with_target_blank(\n",
    "    df_de, \n",
    "    markdown_column=\"chatbot_answer_de\", \n",
    "    html_column=\"chatbot_answer_de_html\")\n",
    "df_de = extract_context_links(\n",
    "    df_de, \n",
    "    context_column=\"chatbot_context_de\", \n",
    "    output_column=\"formatted_context_de_html\", \n",
    "    no_context_text=no_context_text_de)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------\n",
    "input_lss = \"../../../data/human_eval/survey_blank.lss\"      # Path to your existing .lss file\n",
    "output_lss = \"../../../data/human_eval/survey_output.lss\"  # Path for the new .lss file\n",
    "\n",
    "original_group_id = \"99600\"    # The group we want to clone\n",
    "groups_in_total = 10       # How many distinct randomgroup values to distribute\n",
    "\n",
    "# Starting offset for new GIDs and QIDs:\n",
    "gid_start = 200000\n",
    "qid_start = 300000\n",
    "\n",
    "print(f\"german length: {len(df_de)}\", f\"english length: {len(df_en)}\")\n",
    "\n",
    "# 2) Call the function:\n",
    "df_final = build_shuffled_questions_and_clone(\n",
    "    df_en=df_en,\n",
    "    df_de=df_de,\n",
    "    input_lss=input_lss,\n",
    "    output_lss=output_lss,\n",
    "    original_group_id=original_group_id,\n",
    "    gid_start=gid_start,\n",
    "    qid_start=qid_start,\n",
    "    groups_in_total=groups_in_total,\n",
    "    shuffle_seed=42\n",
    ")\n",
    "\n",
    "# save the final dataframe\n",
    "df_final.to_csv(\"../../../data/human_eval/df_for_survey.csv\", index=False, quoting=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of draws to complete 3 collections of 10 items: 61.3661\n"
     ]
    }
   ],
   "source": [
    "# using equastion from https://mat.uab.cat/matmat_antiga/PDFv2014/v2014n02.pdf [(Ferrante & Saltalamacchia, 2006)]\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "from math import factorial, exp\n",
    "\n",
    "def Sm(t, m):\n",
    "    \"\"\"\n",
    "    Compute the sum S_m(t) = \\sum_{k=0}^{m-1} (t^k / k!)\n",
    "    \"\"\"\n",
    "    return sum((t**k) / factorial(k) for k in range(m))\n",
    "\n",
    "def integrand(t, N, m):\n",
    "    \"\"\"\n",
    "    The integrand for the expected number of draws.\n",
    "    \"\"\"\n",
    "    sm_t = Sm(t, m)\n",
    "    return 1 - (1 - sm_t * exp(-t))**N\n",
    "\n",
    "def expected_draws(N, m):\n",
    "    \"\"\"\n",
    "    Calculate the expected number of draws to complete m collections of N items.\n",
    "\n",
    "    Parameters:\n",
    "        N: int - Number of unique items (coupons).\n",
    "        m: int - Number of collections.\n",
    "\n",
    "    Returns:\n",
    "        float - Expected number of draws.\n",
    "    \"\"\"\n",
    "    result, _ = quad(lambda t: integrand(t, N, m), 0, np.inf)\n",
    "    return N * result\n",
    "\n",
    "# Example usage\n",
    "N =10  # Number of unique items\n",
    "m = 3  # Number of times each item is needed\n",
    "\n",
    "expected = expected_draws(N, m)\n",
    "print(f\"Expected number of draws to complete {m} collections of {N} items: {expected:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[X] for N=3, m=2 is approximately 9.63888888888889\n"
     ]
    }
   ],
   "source": [
    "import mpmath as mp\n",
    "\n",
    "def S_m(t, m):\n",
    "    \"\"\"\n",
    "    Compute S_m(t) = sum_{k=0}^{m-1} [t^k / k!].\n",
    "    \"\"\"\n",
    "    s = mp.mpf('0')\n",
    "    for k in range(m):\n",
    "        s += (t**k) / mp.factorial(k)\n",
    "    return s\n",
    "\n",
    "def expected_time_m_collections(N, m):\n",
    "    \"\"\"\n",
    "    Return the expected number of coupons needed to collect\n",
    "    m complete sets of N equally likely coupon types.\n",
    "\n",
    "    Formula:\n",
    "      E[X] = N * âˆ«_{0 to âˆž} [ 1 - (1 - S_m(t)*e^-t)^N ] dt\n",
    "    \"\"\"\n",
    "    # Define the integrand for the integral\n",
    "    def integrand(t):\n",
    "        val = S_m(t, m) * mp.e**(-t)\n",
    "        return 1 - (1 - val)**N\n",
    "\n",
    "    # Perform the integral from 0 to âˆž\n",
    "    result = N * mp.quad(integrand, [0, mp.inf])\n",
    "    return result\n",
    "\n",
    "# Example usage for small values of N, m:\n",
    "m_example = 2\n",
    "N_example = 3\n",
    "est = expected_time_m_collections(N_example, m_example)\n",
    "print(f\"E[X] for N={N_example}, m={m_example} is approximately {est}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
